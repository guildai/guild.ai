{
    "docs": [
        {
            "location": "/", 
            "text": "Guild projects\n\n    \n\n\n      Add \nguild.yml\n to your project to automate your\n      TensorFlow model operations and enable experiment tracking,\n      testing, distribution and remote training.\n\n      \nLearn more\n.\n\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nModel operations\n\n    \n\n\n      Train your model by running the command \nguild\n      run train\n to generate a unique experiment that preserves\n      training files and metadata.\n\n      \nLearn more\n.\n\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nExperiments\n\n    \n\n\n      Track experiments to save logs and trained models, compare\n      performance, diff files, visualize results in TensorBoard and\n      backup to the cloud.\n\n      \nLearn more\n.\n\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nEnd-to-end workflow\n\n    \n\n\n      Automate your model's complete life cycle, from data collection\n      and pre-processing to training and evaluation to optimization\n      and deployment.\n\n      \nLearn more\n.\n\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nModel tests\n\n    \n\n\n      Verify all stages of your TensorFlow automation workflow with\n      tests that exercise code and check results such as expected loss\n      and accuracy ranges.\n\n      \nLearn more\n.\n\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nPackages and code reuse\n\n    \n\n\n      Save time and reduce errors by reusing tested, proven models and\n      operations\ndeveloped by you as well as others in the\n      TensorFlow community.\n\n      \nLearn more\n.\n\n    \n\n    \n\n      \n\n    \n\n  \n\n\n\n\n\n\n  \n\n    \nGuild AI introduction \n\n  \n\n\n\n\n\n\n\nMore features\n\n\n\n  \n\n    \n\n  \n\n  \n\n    \nTrain remotely on Amazon EC2\n\n    \n\n\n      Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras\n      vel nisi ut neque vehicula pellentesque. Donec tellus nisi,\n      ultrices sit amet felis ac, vehicula hendrerit ipsum.\n\n      \n\n        \nLearn more\n\n      \n\n\n    \n\n  \n\n\n\n\n\n\n  \n\n    \n\n  \n\n  \n\n    \nBackup runs to S3\n\n    \n\n\n      Nulla vitae urna nec est convallis malesuada non sit amet\n      orci. Aenean ut mauris id velit pretium malesuada at scelerisque\n      ante. Vestibulum eu tellus ut lectus consectetur\n      faucibus. Phasellus sodales accumsan ex et sodales. Etiam vel mi\n      eu leo tincidunt vulputate. Aliquam quis auctor massa.\n\n    \n\n\n      \n\n        \nLearn more\n\n      \n\n  \n\n\n\n\n\n\n  \n\n    \n\n  \n\n  \n\n    \nVisualize runs with TensorBoard\n\n    \n\n\n      Nulla vitae urna nec est convallis malesuada non sit amet\n      orci. Aenean ut mauris id velit pretium malesuada at scelerisque\n      ante. Vestibulum eu tellus ut lectus consectetur\n      faucibus. Phasellus sodales accumsan ex et sodales. Etiam vel mi\n      eu leo tincidunt vulputate. Aliquam quis auctor massa.\n\n    \n\n\n      \n\n        \nLearn more\n\n      \n\n  \n\n\n\n\n\n\n  \n\n    \n\n  \n\n  \n\n    \nCompare run performance and diff changes\n\n    \n\n\n      Nulla vitae urna nec est convallis malesuada non sit amet\n      orci. Aenean ut mauris id velit pretium malesuada at scelerisque\n      ante. Vestibulum eu tellus ut lectus consectetur\n      faucibus. Phasellus sodales accumsan ex et sodales. Etiam vel mi\n      eu leo tincidunt vulputate. Aliquam quis auctor massa.\n\n    \n\n\n      \n\n        \nLearn more\n\n      \n\n  \n\n\n\n\n\n\n  \n\n    \n\n  \n\n  \n\n    \nPublish packaged models to PyPI\n\n    \n\n\n      Nulla vitae urna nec est convallis malesuada non sit amet\n      orci. Aenean ut mauris id velit pretium malesuada at scelerisque\n      ante. Vestibulum eu tellus ut lectus consectetur\n      faucibus. Phasellus sodales accumsan ex et sodales. Etiam vel mi\n      eu leo tincidunt vulputate. Aliquam quis auctor massa.\n\n    \n\n\n      \n\n        \nLearn more\n\n      \n\n  \n\n\n\n\n\n\n\nGet started\n\n\n\n  \n\n    \n\n      \nConvert a Jupyter Notebook to a Guild project\n\n      \n\n\n        If your TensorFlow training code is in a Jupyter Notebook, use\n        Guild to create a reusable project that supports experiment\n        tracking, remote training, and detailed run comparison.\n\n      \n\n      \nConvert a Jupyter Notebook \n\n    \n\n  \n\n\n  \n\n    \n\n      \nAdd Guild to your TensorFlow or Keras project\n\n      \n\n\n        If you have scripts that train your TensorFlow or Keras\n        models, add a Guild file (i.e. a file\n        named \nguild.yml\n) to your project to enable Guild\n        features.\n\n      \n\n      \nAdd Guild to a project \n\n    \n\n  \n\n\n  \n\n    \n\n      \nCreate an image classifier\n\n      \n\n\n        Guild supports a variety of feature-rich packages and project\n        templates. This guide steps you through the process of\n        creating an image classifier.\n\n      \n\n      \nCreate an image classifier \n\n    \n\n  \n\n\n  \n\n    \n\n      \nBrowse Guild AI documentation\n\n      \n\n\n        If you're interested in a complete picture of Guild AI, start by\n        browsing its comprehensives documentation.\n\n      \n\n      \nBrowse documentation", 
            "title": "Home"
        }, 
        {
            "location": "/#more-features", 
            "text": "", 
            "title": "More features"
        }, 
        {
            "location": "/#get-started", 
            "text": "", 
            "title": "Get started"
        }, 
        {
            "location": "/install/", 
            "text": "Install Guild AI\n\n\n\n\nRequirements\n\n\nInstall Guild AI\n\n\nUsing pip\n\n\nFrom source code\n\n\n\n\n\n\nInstall TensorFlow\n\n\nInstall optional libraries\n\n\nCUDA and cuDNN\n\n\nNVIDIA System Management Interface\n\n\n\n\n\n\nVerify your installation\n\n\nNext steps\n\n\n\n\nRequirements\n\n\nGuild AI has the following requirements:\n\n\n\n\nMax OS or Linux\n\n\nPython 2.7 or Python 3\n\n\npip\n\n\n\n\nGuild is installed from PyPI using \npip\n. Refer to \npip Installation\n to ensure you have it\ninstalled.\n\n\nInstall Guild AI\n\n\nUsing pip\n\n\nTo install Guild AI, run the following command:\n\n\npip install guildai\n\n\n\n\nIf you need to run \ninstall\n with administrative privileges, run:\n\n\nsudo pip install guildai\n\n\n\n\nIf you want the latest pre-release version of Guild AI, use the\n\n--pre\n option:\n\n\npip install --pre guildai\n\n\n\n\nFrom source code\n\n\n\n\nNote\n\n\nThis step is an alternative to installing Guild AI with pip\ndescribed above. Install Guild AI from source code if you want a\nspecific version from GitHub (e.g. an early release or development\nbranch) or if you want to contribute to the project.\n\n\n\n\nAdditional required tools for installing from source code:\n\n\n\n\ngit\n\n\nnpm\n v5.8.0 or later\n\n\nPython development library and headers for your system\n\n\n\n\nTo install Guild from source, clone the repository by running:\n\n\ngit clone https://github.com/guildai/guild.git\n\n\n\n\nChange to the \nguild\n directory and install the required pip packages:\n\n\ncd guild\npip install -r requirements.txt\n\n\n\n\nBuild Guild by running:\n\n\npython setup.py build\n\n\n\n\nVerify Guild by running:\n\n\nguild/scripts/guild check\n\n\n\n\nIf see the message \nNOT INSTALLED (No module named 'tensorflow')\n\nthat\ns okay - you\nll install TensorFlow in the steps below. If you see\nother errors, please \nopen an issue on GitHub\n and we\nll help!\n\n\nYou can run the \nGUILD_SOURCE_DIR/guild/scripts/guild\n executable\ndirectly (where \nGUILD_SOURCE_DIR\n is the location of your cloned\nGuild AI source repository) or modify your environment to make \nguild\n\navailable on your PATH using one of these methods:\n\n\n\n\nAdd \nGUILD_SOURCE_DIR/guild/scripts\n directory to your \nPATH\n environment\n  variable, OR\n\n\nCreate a symlink to \nGUILD_SOURCE_DIR/guild/scripts/guild\n that is\n  available on your PATH\n\n\n\n\nInstall TensorFlow\n\n\nGuild requires TensorFlow but does not install it for\nyou. \n1\n You can use \npip\n to install TensorFlow by running:\n\n\npip install tensorflow\n\n\n\n\nIf your system has a GPU, install the GPU enabled package by running:\n\n\npip install tensorflow-gpu\n\n\n\n\nFor alternative installation methods, refer to \nInstalling TensorFlow\n.\n\n\nInstall optional libraries\n\n\nIf you system has a GPU or other accelerator supported by TensorFlow,\nyou will need to install and configure support for your hardware.\n\n\nCUDA and cuDNN\n\n\nIf you have an NVIDIA GPU and and want to use the GPU enabled\nTensorFlow package, you must install the NVIDIA CUDA and cuDNN\nlibraries for your system. Refer to the links below for help\ninstalling the libraries.\n\n\n\n\nCUDA Toolkit Download\n\n\nNVIDIA cuDNN\n\n\n\n\nNVIDIA System Management Interface\n\n\nGuild uses NVIDIA System Management Interface (\nnvidia-smi\n) on GPU\naccelerated systems to collect GPU metrics. This tool is optional and\nGuild will run without it. However, to collect GPU stats on systems\nwith one or more GPUs, ensure that \nnvidia-smi\n is installed.\n\n\n\n\nNote\n\n\nNVIDIA System Management Interface is typically installed with NVIDIA\nGPU drivers. Refer to \nNVIDIA System Management Interface\n\nfor more information.\n\n\n\n\nVerify your installation\n\n\nVerify that Guild is installed properly by running the\n\ncheck\n command:\n\n\nguild check\n\n\n\n\nIf there are problems with your installation, Guild will display the\ndetails and exit with an error. Refer to\n\nTroubleshooting\n for assistance.\n\n\nNext steps\n\n\nCongratulations, you\nve installed Guild AI! We\nve outlined some next\nsteps for you below.\n\n\n\n\n\n\n\n\nTrain your first model\n\n\n\n\nDive in and train your first model using Guild AI. This introductory\ntutorial will walk you through the basics of Guild and cover most of\nits features.\n\n\n\n\nTrain your first model \n\n\n\n\n\n\n\n\n\n\n\nDiscover Guild models\n\n\n\n\nGuild AI provides a catalog of state-of-the-art TensorFlow models that\ncan be used to build deep learning applications. Start here to see\nwhat developers are building.\n\n\n\n\nDiscover Guild models \n\n\n\n\n\n\n\n\n\n\n\nBrowse the docs\n\n\n\n\nIf you're interested in a complete picture of Guild AI, start by\nbrowsing its comprehensives documentation.\n\n\n\n\nBrowse the docs \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow is a rapidly evolving software library and is provided as\nboth CPU and GPU supported packages. Guild leaves the specific package\nand version of TensorFlow up to the user.", 
            "title": "Install"
        }, 
        {
            "location": "/install/#install-guild-ai", 
            "text": "Requirements  Install Guild AI  Using pip  From source code    Install TensorFlow  Install optional libraries  CUDA and cuDNN  NVIDIA System Management Interface    Verify your installation  Next steps", 
            "title": "Install Guild AI"
        }, 
        {
            "location": "/install/#requirements", 
            "text": "Guild AI has the following requirements:   Max OS or Linux  Python 2.7 or Python 3  pip   Guild is installed from PyPI using  pip . Refer to  pip Installation  to ensure you have it\ninstalled.", 
            "title": "Requirements"
        }, 
        {
            "location": "/install/#install-guild-ai_1", 
            "text": "", 
            "title": "Install Guild AI"
        }, 
        {
            "location": "/install/#using-pip", 
            "text": "To install Guild AI, run the following command:  pip install guildai  If you need to run  install  with administrative privileges, run:  sudo pip install guildai  If you want the latest pre-release version of Guild AI, use the --pre  option:  pip install --pre guildai", 
            "title": "Using pip"
        }, 
        {
            "location": "/install/#from-source-code", 
            "text": "Note  This step is an alternative to installing Guild AI with pip\ndescribed above. Install Guild AI from source code if you want a\nspecific version from GitHub (e.g. an early release or development\nbranch) or if you want to contribute to the project.   Additional required tools for installing from source code:   git  npm  v5.8.0 or later  Python development library and headers for your system   To install Guild from source, clone the repository by running:  git clone https://github.com/guildai/guild.git  Change to the  guild  directory and install the required pip packages:  cd guild\npip install -r requirements.txt  Build Guild by running:  python setup.py build  Verify Guild by running:  guild/scripts/guild check  If see the message  NOT INSTALLED (No module named 'tensorflow') \nthat s okay - you ll install TensorFlow in the steps below. If you see\nother errors, please  open an issue on GitHub  and we ll help!  You can run the  GUILD_SOURCE_DIR/guild/scripts/guild  executable\ndirectly (where  GUILD_SOURCE_DIR  is the location of your cloned\nGuild AI source repository) or modify your environment to make  guild \navailable on your PATH using one of these methods:   Add  GUILD_SOURCE_DIR/guild/scripts  directory to your  PATH  environment\n  variable, OR  Create a symlink to  GUILD_SOURCE_DIR/guild/scripts/guild  that is\n  available on your PATH", 
            "title": "From source code"
        }, 
        {
            "location": "/install/#install-tensorflow", 
            "text": "Guild requires TensorFlow but does not install it for\nyou.  1  You can use  pip  to install TensorFlow by running:  pip install tensorflow  If your system has a GPU, install the GPU enabled package by running:  pip install tensorflow-gpu  For alternative installation methods, refer to  Installing TensorFlow .", 
            "title": "Install TensorFlow"
        }, 
        {
            "location": "/install/#install-optional-libraries", 
            "text": "If you system has a GPU or other accelerator supported by TensorFlow,\nyou will need to install and configure support for your hardware.", 
            "title": "Install optional libraries"
        }, 
        {
            "location": "/install/#cuda-and-cudnn", 
            "text": "If you have an NVIDIA GPU and and want to use the GPU enabled\nTensorFlow package, you must install the NVIDIA CUDA and cuDNN\nlibraries for your system. Refer to the links below for help\ninstalling the libraries.   CUDA Toolkit Download  NVIDIA cuDNN", 
            "title": "CUDA and cuDNN"
        }, 
        {
            "location": "/install/#nvidia-system-management-interface", 
            "text": "Guild uses NVIDIA System Management Interface ( nvidia-smi ) on GPU\naccelerated systems to collect GPU metrics. This tool is optional and\nGuild will run without it. However, to collect GPU stats on systems\nwith one or more GPUs, ensure that  nvidia-smi  is installed.   Note  NVIDIA System Management Interface is typically installed with NVIDIA\nGPU drivers. Refer to  NVIDIA System Management Interface \nfor more information.", 
            "title": "NVIDIA System Management Interface"
        }, 
        {
            "location": "/install/#verify-your-installation", 
            "text": "Verify that Guild is installed properly by running the check  command:  guild check  If there are problems with your installation, Guild will display the\ndetails and exit with an error. Refer to Troubleshooting  for assistance.", 
            "title": "Verify your installation"
        }, 
        {
            "location": "/install/#next-steps", 
            "text": "Congratulations, you ve installed Guild AI! We ve outlined some next\nsteps for you below.", 
            "title": "Next steps"
        }, 
        {
            "location": "/docs/", 
            "text": "Documentation\n\n\n\n\n\n\nConcepts\n\n\nIntroduction\nModels\nOperations\nRuns\nResources\nPackages\n\n\n\n\nVisual tools\n\n\nGuild View\nTensorBoard\n\n\n\n\nReference\n\n\nCommands\nGuild file reference\nGuild home reference\n\n\n\n\nStep by step\n\n\nGuides", 
            "title": "Documentation"
        }, 
        {
            "location": "/docs/#documentation", 
            "text": "", 
            "title": "Documentation"
        }, 
        {
            "location": "/docs/#concepts", 
            "text": "Introduction Models Operations Runs Resources Packages", 
            "title": "Concepts"
        }, 
        {
            "location": "/docs/#visual-tools", 
            "text": "Guild View TensorBoard", 
            "title": "Visual tools"
        }, 
        {
            "location": "/docs/#reference", 
            "text": "Commands Guild file reference Guild home reference", 
            "title": "Reference"
        }, 
        {
            "location": "/docs/#step-by-step", 
            "text": "Guides", 
            "title": "Step by step"
        }, 
        {
            "location": "/docs/intro/", 
            "text": "Guild AI introduction\n\n\n\n\nGuild projects\n\n\nExample\n\n\n\n\n\n\nModel operations\n\n\nExample\n\n\n\n\n\n\nExperiments\n\n\nExample\n\n\n\n\n\n\n\n\nThis introduction provides an overview of Guild AI core\nfunctionality. Feel free to follow the steps under each \nExample\n\nsection below to get hands-on experience with Guild. The examples\nimplement the Keras image classifier described in TensorFlow\ns \nTrain\nyour first neural network: basic classification\n. Refer\nto the \ncompleted\nproject\n for\nthe full example.\n\n\n\n\nFashion-MNIST images used in examples below\n\n\nBefore running any of the steps below, first \ninstall Guild AI\n.\n\n\nGuild projects\n\n\nA Guild project is a standard TensorFlow or Keras source code project\nthat contains a file named \nguild.yml\n in the root directory. We refer\nto \nguild.yml\n as a project \nGuild file\n.\n\n\nGuild files supplement TensorFlow and Keras projects. You often don\nt\nneed to modify your project source files to take advantage of Guild\nfeatures.\n\n\nGuild files are YAML formatted files that define \nmodels\n,\nmodel \noperations\n, \nresources\n, and\n\npackages\n.\n\n\nExample\n\n\nIn this example, we create a Guild project skeleton to illustrate the\nbasics of model definition and discovery.\n\n\nStart by creating a project directory:\n\n\nmkdir sample-project\n\n\n\n\nCreate a file \nsample-project/guild.yml\n and modify it to be:\n\n\n- model: fashion\n  description: Basic Fashion-MNIST image classifier.\n\n\n\n\nSave your changes to \nguild.yml\n. Confirm that the project structure\nis:\n\n\n\n\n\n\nsample-project\n \n\n \nguild.yml\n\n \n\n\n\n\n\n\n\n\n\nChange to the project directory and list the project models:\n\n\ncd sample-project\nguild models\n\n\n\n\nGuild displays the project models:\n\n\n./fashion  Basic Fashion-MNIST image classifier.\n\n\n\n\nGuild models represent the TensorFlow or Keras models in your\nproject. In the examples that follow, we fill in details to\ncreate a fully functional classifier for the Fashion-MNIST dataset.\n\n\nModel operations\n\n\nModel operations automate model related tasks. A common operation is\n\ntrain\n, which trains a model from scratch. As we\nll see later in this\nintroduction, operations can be any task that you want to\nautomate. These may include:\n\n\n\n\nprepare-data\n\n\nPrepare a dataset for training.\n\n\ntransfer-learn\n\n\nTrain a model using transfer learning.\n\n\nfinetune\n\n\nFine tune a trained model.\n\n\nevaluate\n\n\nEvaluate a trained model on a hold-out dataset.\n\n\nquantize\n\n\nQuantize a trained model to use 8 bit integers.\n\n\npredict\n\n\nUse a trained model to make predictions.\n\n\nserve\n\n\nRun a trained model as an inference server.\n\n\n\n\nYou\nre free to define the operations that suit your model\nGuild does\nnot prescribe the operations a model may support.\n\n\nOnce an operation is defined for a model, you can run it using the\n\nrun\n command. As we\nll see in the next section, each operation\nrun is tracked as a separate experiment.\n\n\nExample\n\n\nIf you are following the examples, in this section we add a \ntrain\n\noperation to our model and run it as an experiment.\n\n\nModify \nguild.yml\n to be:\n\n\n- model: fashion\n  description: Basic Fashion-MNIST image classifier.\n  operations:\n    train:\n      description: Train classifier from scratch.\n      main: train\n\n\n\n\nHere we add a \ntrain\n operation with a description and a main module.\n\n\nSave your changes to \nguild.yml\n.\n\n\nUse the \noperations\n command to list available operations:\n\n\nguild operations\n\n\n\n\n\n\nTip\n\n\nYou can use \nops\n as a short cut to the \noperations\n command. We use\n\nops\n through the remained of this introduction.\n\n\n\n\nGuild displays the newly added \ntrain\n operation:\n\n\n./fashion:train  Train classifier from scratch.\n\n\n\n\nYou can view help for the project using the \nhelp\n command:\n\n\nguild help\n\n\n\n\nGuild automatically generates help from the Guild file. This\ninformation is useful for working with the project, especially as it\nbecomes more complex over time.\n\n\nNext, run the operation:\n\n\nguild run train\n\n\n\n\nPress \nEnter\n to confirm the operation.\n\n\n\n\nTip\n\n\nYou can use \n-y\n with the \nrun\n command to bypass the\nprompt. E.g. \nguild run -y train\n runs the operation\nwithout prompting.\n\n\n\n\nThe command fails with the message:\n\n\nguild: No module named train\n\n\n\n\nThat\ns okay! We expect this error because our project is indeed\nmissing the \ntrain\n module.\n\n\nGuild operations are implemented using Python main\nmodules\ni.e. Python modules that can be executed as a program (see\n\nPython help\n for\ndetails). This means that any Python script can be used for a Guild\nmodel operation.\n\n\nLet\ns create the missing module.\n\n\nDownload \ntrain.py\n\nand save it to the project directory. If you have \nwget\n installed,\nyou can download the file directly to the project directory by\nrunning:\n\n\nwget https://raw.githubusercontent.com/guildai/examples/master/fashion/train.py\n\n\n\n\nConfirm that the project structure is now:\n\n\n\n\n\n\nsample-project\n \n\n \nguild.yml\n\n \ntrain.py\n\n \n\n\n\n\n\n\n\n\n\nThe \ntrain\n module uses Keras to train a simple image classifer. The\nmodule is derived from the \nTensorFlow getting started\ntutorial\n\nand requires a few additional Python packages:\n\n\n\n\nmatplotlib\n\n\nh5py\n\n\nnumpy\n\n\n\n\nInstall those packages now:\n\n\npip install matplotlib h5py numpy\n\n\n\n\nNow that \ntrain.py\n is available, along with its required Python\npackages, run the operation again:\n\n\nguild run train\n\n\n\n\nPress \nEnter\n to confirm the operation.\n\n\nGuild runs the train operation by executing \ntrain\n (the module\ndefined in \nsample-project/train.py\n) as a program\nthe operation is\nimplemented entirely by that module.\n\n\nExperiments\n\n\nGuild tracks each operation run as an isolated experiment using \nrun\ndirectories\n. Files generated by a run are written to a\nunique directory and can be accessed as normal files.\n\n\nGuild provides extensive support for managing and using runs:\n\n\n\n\nShow run information, including metadata, files, and output\n\n\nList by operation, status, and label\n\n\nDelete, restore, and purge\n\n\nExport and import\n\n\nTag with custom labels\n\n\nPush to and pull from remote environments\n\n\nCompare run performance\n\n\nDiff run metadata, files, and output\n\n\n\n\nFor more information on managing runs, see \nRuns\n.\n\n\nExample\n\n\nIf you are following the examples, in this section we use Guild\ns run\nmanagement facility to examine the runs generated in the previous\nexample.\n\n\nFirst, fist available runs:\n\n\nguild runs\n\n\n\n\nGuild displays the runs so far. You should see two (dates and IDs will\ndiffer):\n\n\n[1:19c67a72]  ./fashion:train  2018-10-16 15:57:32  completed\n[2:1451e20c]  ./fashion:train  2018-10-16 15:57:23  error\n\n\n\n\nThe latest run (listed first) should have a status of \ncompleted\n,\nindicating that the operation exited without an error. The prior run\n(listed second) should have a status of \nerror\n because the \ntrain\n\nmodule was not originally available (see previous section).\n\n\nNext, show output for the failed run:\n\n\nguild run info --output 2\n\n\n\n\nThe option \n--output\n tells Guild to include run output. The value\n\n2\n tells Guild to show information for the run with index \n2\n (see\nlisting above).\n\n\nYou should see information for the failed run (dates and IDs will\ndiffer):\n\n\nid: 1451e20cd18611e88f52d017c2ab916f\noperation: ./fashion:train\nstatus: error\nstarted: 2018-10-16 15:57:23\nstopped: 2018-10-16 15:57:23\nrun_dir: ~/.guild/runs/1451e20cd18611e88f52d017c2ab916f\ncommand: /usr/bin/python -um guild.op_main train\nexit_status: 1\npid:\noutput:\n  guild: No module named train\n\n\n\n\nNote the error message in the output. This information is retained as\na part of the tracked experiment.\n\n\nNext, delete the failed run\nwe don\nt need it:\n\n\nguild runs rm 2\n\n\n\n\nAs with the \nruns info\n command, the value \n2\n is a reference to the\nsecond run in the list.\n\n\nGuild prompts you before deleting the run. Press \nEnter\n to\nconfirm. After deleting the run, you can verify that it was deleted by\nrunning \nguild runs\n.\n\n\nIf you make a mistake and delete a run by accident, you can restore it\nusing \nruns restore\n.\n\n\nNext, view the files generated by the successful run:\n\n\nguild ls\n\n\n\n\nWe can omit a reference to the run in this case\nGuild assumes you\nwant to show files for the latest run.\n\n\nGuild show these file (IDs and timestamps will differ):\n\n\n~/.guild/runs/7d230c98d20811e88f52d017c2ab916f:\n  events.out.tfevents.1539779455.local\n  weights-0001-13.233.hdf5\n  weights-0002-13.080.hdf5\n  weights-0003-13.031.hdf5\n  weights-0004-12.996.hdf5\n  weights-0005-13.007.hdf5\n\n\n\n\nThese are the log file and trained model weights. Guild tracks these\nfiles along with all other run related metadata.\n\n\n\n\nTip\n\n\nBy default \nls\n shows relative file paths. You can show full\npaths by including the \n-f\n option. This is useful if you need to\naccess a particular file on the file system.\n\n\n\n\nGuild provides a number of commands to help you view and understand\nruns. Try running any of these from the \nsample-project\n directory:\n\n\n\n\nguild view\n\n\nOpen \nGuild View\n.\n\n\nguild tensorboard\n\n\nView TensorFlow event logs in \nTensorBoard\n.\n\n\nguild runs info\n\n\nShow basic run information (try \nguild runs info --help\n for a\n  list of additional options).\n\n\nguild open\n\n\nOpen the run directory in your system file browser.", 
            "title": "Introduction"
        }, 
        {
            "location": "/docs/intro/#guild-ai-introduction", 
            "text": "Guild projects  Example    Model operations  Example    Experiments  Example     This introduction provides an overview of Guild AI core\nfunctionality. Feel free to follow the steps under each  Example \nsection below to get hands-on experience with Guild. The examples\nimplement the Keras image classifier described in TensorFlow s  Train\nyour first neural network: basic classification . Refer\nto the  completed\nproject  for\nthe full example.   Fashion-MNIST images used in examples below  Before running any of the steps below, first  install Guild AI .", 
            "title": "Guild AI introduction"
        }, 
        {
            "location": "/docs/intro/#guild-projects", 
            "text": "A Guild project is a standard TensorFlow or Keras source code project\nthat contains a file named  guild.yml  in the root directory. We refer\nto  guild.yml  as a project  Guild file .  Guild files supplement TensorFlow and Keras projects. You often don t\nneed to modify your project source files to take advantage of Guild\nfeatures.  Guild files are YAML formatted files that define  models ,\nmodel  operations ,  resources , and packages .", 
            "title": "Guild projects"
        }, 
        {
            "location": "/docs/intro/#example", 
            "text": "In this example, we create a Guild project skeleton to illustrate the\nbasics of model definition and discovery.  Start by creating a project directory:  mkdir sample-project  Create a file  sample-project/guild.yml  and modify it to be:  - model: fashion\n  description: Basic Fashion-MNIST image classifier.  Save your changes to  guild.yml . Confirm that the project structure\nis:    sample-project\n  \n  guild.yml \n      Change to the project directory and list the project models:  cd sample-project\nguild models  Guild displays the project models:  ./fashion  Basic Fashion-MNIST image classifier.  Guild models represent the TensorFlow or Keras models in your\nproject. In the examples that follow, we fill in details to\ncreate a fully functional classifier for the Fashion-MNIST dataset.", 
            "title": "Example"
        }, 
        {
            "location": "/docs/intro/#model-operations", 
            "text": "Model operations automate model related tasks. A common operation is train , which trains a model from scratch. As we ll see later in this\nintroduction, operations can be any task that you want to\nautomate. These may include:   prepare-data  Prepare a dataset for training.  transfer-learn  Train a model using transfer learning.  finetune  Fine tune a trained model.  evaluate  Evaluate a trained model on a hold-out dataset.  quantize  Quantize a trained model to use 8 bit integers.  predict  Use a trained model to make predictions.  serve  Run a trained model as an inference server.   You re free to define the operations that suit your model Guild does\nnot prescribe the operations a model may support.  Once an operation is defined for a model, you can run it using the run  command. As we ll see in the next section, each operation\nrun is tracked as a separate experiment.", 
            "title": "Model operations"
        }, 
        {
            "location": "/docs/intro/#example_1", 
            "text": "If you are following the examples, in this section we add a  train \noperation to our model and run it as an experiment.  Modify  guild.yml  to be:  - model: fashion\n  description: Basic Fashion-MNIST image classifier.\n  operations:\n    train:\n      description: Train classifier from scratch.\n      main: train  Here we add a  train  operation with a description and a main module.  Save your changes to  guild.yml .  Use the  operations  command to list available operations:  guild operations   Tip  You can use  ops  as a short cut to the  operations  command. We use ops  through the remained of this introduction.   Guild displays the newly added  train  operation:  ./fashion:train  Train classifier from scratch.  You can view help for the project using the  help  command:  guild help  Guild automatically generates help from the Guild file. This\ninformation is useful for working with the project, especially as it\nbecomes more complex over time.  Next, run the operation:  guild run train  Press  Enter  to confirm the operation.   Tip  You can use  -y  with the  run  command to bypass the\nprompt. E.g.  guild run -y train  runs the operation\nwithout prompting.   The command fails with the message:  guild: No module named train  That s okay! We expect this error because our project is indeed\nmissing the  train  module.  Guild operations are implemented using Python main\nmodules i.e. Python modules that can be executed as a program (see Python help  for\ndetails). This means that any Python script can be used for a Guild\nmodel operation.  Let s create the missing module.  Download  train.py \nand save it to the project directory. If you have  wget  installed,\nyou can download the file directly to the project directory by\nrunning:  wget https://raw.githubusercontent.com/guildai/examples/master/fashion/train.py  Confirm that the project structure is now:    sample-project\n  \n  guild.yml \n  train.py \n      The  train  module uses Keras to train a simple image classifer. The\nmodule is derived from the  TensorFlow getting started\ntutorial \nand requires a few additional Python packages:   matplotlib  h5py  numpy   Install those packages now:  pip install matplotlib h5py numpy  Now that  train.py  is available, along with its required Python\npackages, run the operation again:  guild run train  Press  Enter  to confirm the operation.  Guild runs the train operation by executing  train  (the module\ndefined in  sample-project/train.py ) as a program the operation is\nimplemented entirely by that module.", 
            "title": "Example"
        }, 
        {
            "location": "/docs/intro/#experiments", 
            "text": "Guild tracks each operation run as an isolated experiment using  run\ndirectories . Files generated by a run are written to a\nunique directory and can be accessed as normal files.  Guild provides extensive support for managing and using runs:   Show run information, including metadata, files, and output  List by operation, status, and label  Delete, restore, and purge  Export and import  Tag with custom labels  Push to and pull from remote environments  Compare run performance  Diff run metadata, files, and output   For more information on managing runs, see  Runs .", 
            "title": "Experiments"
        }, 
        {
            "location": "/docs/intro/#example_2", 
            "text": "If you are following the examples, in this section we use Guild s run\nmanagement facility to examine the runs generated in the previous\nexample.  First, fist available runs:  guild runs  Guild displays the runs so far. You should see two (dates and IDs will\ndiffer):  [1:19c67a72]  ./fashion:train  2018-10-16 15:57:32  completed\n[2:1451e20c]  ./fashion:train  2018-10-16 15:57:23  error  The latest run (listed first) should have a status of  completed ,\nindicating that the operation exited without an error. The prior run\n(listed second) should have a status of  error  because the  train \nmodule was not originally available (see previous section).  Next, show output for the failed run:  guild run info --output 2  The option  --output  tells Guild to include run output. The value 2  tells Guild to show information for the run with index  2  (see\nlisting above).  You should see information for the failed run (dates and IDs will\ndiffer):  id: 1451e20cd18611e88f52d017c2ab916f\noperation: ./fashion:train\nstatus: error\nstarted: 2018-10-16 15:57:23\nstopped: 2018-10-16 15:57:23\nrun_dir: ~/.guild/runs/1451e20cd18611e88f52d017c2ab916f\ncommand: /usr/bin/python -um guild.op_main train\nexit_status: 1\npid:\noutput:\n  guild: No module named train  Note the error message in the output. This information is retained as\na part of the tracked experiment.  Next, delete the failed run we don t need it:  guild runs rm 2  As with the  runs info  command, the value  2  is a reference to the\nsecond run in the list.  Guild prompts you before deleting the run. Press  Enter  to\nconfirm. After deleting the run, you can verify that it was deleted by\nrunning  guild runs .  If you make a mistake and delete a run by accident, you can restore it\nusing  runs restore .  Next, view the files generated by the successful run:  guild ls  We can omit a reference to the run in this case Guild assumes you\nwant to show files for the latest run.  Guild show these file (IDs and timestamps will differ):  ~/.guild/runs/7d230c98d20811e88f52d017c2ab916f:\n  events.out.tfevents.1539779455.local\n  weights-0001-13.233.hdf5\n  weights-0002-13.080.hdf5\n  weights-0003-13.031.hdf5\n  weights-0004-12.996.hdf5\n  weights-0005-13.007.hdf5  These are the log file and trained model weights. Guild tracks these\nfiles along with all other run related metadata.   Tip  By default  ls  shows relative file paths. You can show full\npaths by including the  -f  option. This is useful if you need to\naccess a particular file on the file system.   Guild provides a number of commands to help you view and understand\nruns. Try running any of these from the  sample-project  directory:   guild view  Open  Guild View .  guild tensorboard  View TensorFlow event logs in  TensorBoard .  guild runs info  Show basic run information (try  guild runs info --help  for a\n  list of additional options).  guild open  Open the run directory in your system file browser.", 
            "title": "Example"
        }, 
        {
            "location": "/docs/models/", 
            "text": "Models\n\n\n\n\nCreate a model\n\n\nInstall models\n\n\nGet model help\n\n\nList models\n\n\nUninstall models\n\n\n\n\nA model in Guild AI is a representation of a machine learning model. A\nmodel may be as simple, like a linear regression, or complex, like a\ndeep neural network.\n\n\nModels in Guild are characterized by their support of\n\noperations\n, which are actions that can be performed\non the model. One of the most common model operations is \ntrain\n,\nwhich iteratively modifies model parameters to optimize the model\ns\npredictive or generative capability. Other common operations include\n\nevaluate\n, \npredict\n, \ndeploy\n, and \ngenerate\n.\n\n\nGuild does not implement model operations. That task is the\nresponsibility of the model developer. Guild instead manages the work\nflow associated with running operations and tracking results.\n\n\nConsider the following Guild command:\n\n\nguild train mnist\n\n\n\n\nWhen this command is run, Guild will look for a model named \nmnist\n\nand then for an operation associated with that model named\n\ntrain\n. With this information, Guild will start a \nrun\n,\nwhich is a Python session used to train the model. Guild will track\nsave run metadata and output so that you can access it later.\n\n\nGuild looks for models in \nGuild files\n, which are\nfiles named \nguild.yml\n that contain model definitions.\n\n\nHere\ns what a \nguild.yml\n that defines an \nmnist\n model might look\nlike:\n\n\nmodel: mnist\noperations:\n  train:\n    cmd: train_mnist\n\n\n\n\nThis file tells Guild that the model \nmnist\n has an operation \ntrain\n\nthat is performed by running a Python module \ntrain_mnist\n.\n\n\nHere\ns what \ntrain_mnist.py\nthe training module source file\n\nmight look like:\n\n\nimport mnist_data\nimport mnist_model\n\ndef train():\n    data = mnist_data.load()\n    model = mnist_model.init()\n    model.train(data)\n\nif __name__ == \n__main__\n:\n    train()\n\n\n\n\nModels in Guild are very flexible and can be used to manage any type\nof operation associated with the model.\n\n\nCreate a model\n\n\nTo create a model, create a file named \nguild.yml\n in your model\nproject directory.\n\n\n\n\nTip\n\n\nIf you\nre starting from scratch, use the \ninit\n command\nto initialize a new project, including a sample training script.\n\n\n\n\nUsing a text editor, paste the following code sample into \nguild.yml\n:\n\n\n- model: my-model\n  description: My model\n  operations:\n    train:\n      cmd: train_my_model\n\n\n\n\n\n\n\n\nReplace the values for \nmodel\n and \ndescription\n to reflect the\n  attributes of your model.\n\n\n\n\n\n\nReplace \ntrain_my_model\n with the Python module used to train your\n  model.\n\n\n\n\n\n\nSave \nguild.yml\n.\n\n\n\n\n\n\nFor more information on defining models and operations, see \nGuile\nfile reference\n.\n\n\nFor a detailed step-by-step example of creating a Guild model, see\n\nDevelop a model from\nscratch\n.\n\n\nYou may experiment with training your model by running:\n\n\nguild train\n\n\n\n\nFor information on working with runs generated by operations, see\n\nRuns\n.\n\n\nInstall models\n\n\nGuild AI is a model packaging and distribution system that can be used\nto install models from the deep learning ecosystem.\n\n\nYou install models by installing Guild packages that contain the\nmodels.\n\n\nTo install a package, run:\n\n\nguild install PACKAGE\n\n\n\n\nYou can lookup packages by running:\n\n\nguild search TERM\n\n\n\n\nFor example, to install a package that contains ResNet models, search\nfor \nresnet\n by running:\n\n\nguild search resnet\n\n\n\n\nYou can also browse \nGuild AI models\n.\n\n\nAfter reviewing the list of available packages, if you decide to\ninstall \ntensorflow.reset\n, you can install it by running:\n\n\nguild install tensorflow.resnet\n\n\n\n\nFor more information, see \nInstall\npackages\n and the \ninstall\n\ncommand.\n\n\nGet model help\n\n\nTo view help for models defined in the current directory (i.e. models\ndefined in \n./guild.yml\n) run:\n\n\nguild help\n\n\n\n\nTo view help for models defined in another directory, run:\n\n\nguild help DIRECTORY\n\n\n\n\n\n\nNote\n\n\nDIRECTORY\n must contain a Guild file (i.e. \nguild.yml\n).\n\n\n\n\nTo view help for models defined in a package, run:\n\n\nguild help PACKAGE\n\n\n\n\nFor example, to view help for the models provided by\n\ntensorflow.resnet\n, run:\n\n\nguild help tensorflow.resnet\n\n\n\n\nFor more information, see the \nhelp\n command.\n\n\nList models\n\n\nList available models by running:\n\n\nguild models\n\n\n\n\nYou can filter the results using a term. For example, to list\navailable models that contain \ncnn\n, run:\n\n\nguild models cnn\n\n\n\n\nFor more information, see the \nmodels\n command.\n\n\nUninstall models\n\n\nYou can remove models defined in packages by uninstalling the\npackages.\n\n\nguild uninstall PACKAGE\n\n\n\n\nThe package that a model is associated with is displayed when you run\n\nguild models\n in the form \nPACKAGE/MODEL\n.\n\n\nFor more information, see \nUninstall\npackages\n and the\n\nuninstall\n command.", 
            "title": "Models"
        }, 
        {
            "location": "/docs/models/#models", 
            "text": "Create a model  Install models  Get model help  List models  Uninstall models   A model in Guild AI is a representation of a machine learning model. A\nmodel may be as simple, like a linear regression, or complex, like a\ndeep neural network.  Models in Guild are characterized by their support of operations , which are actions that can be performed\non the model. One of the most common model operations is  train ,\nwhich iteratively modifies model parameters to optimize the model s\npredictive or generative capability. Other common operations include evaluate ,  predict ,  deploy , and  generate .  Guild does not implement model operations. That task is the\nresponsibility of the model developer. Guild instead manages the work\nflow associated with running operations and tracking results.  Consider the following Guild command:  guild train mnist  When this command is run, Guild will look for a model named  mnist \nand then for an operation associated with that model named train . With this information, Guild will start a  run ,\nwhich is a Python session used to train the model. Guild will track\nsave run metadata and output so that you can access it later.  Guild looks for models in  Guild files , which are\nfiles named  guild.yml  that contain model definitions.  Here s what a  guild.yml  that defines an  mnist  model might look\nlike:  model: mnist\noperations:\n  train:\n    cmd: train_mnist  This file tells Guild that the model  mnist  has an operation  train \nthat is performed by running a Python module  train_mnist .  Here s what  train_mnist.py the training module source file \nmight look like:  import mnist_data\nimport mnist_model\n\ndef train():\n    data = mnist_data.load()\n    model = mnist_model.init()\n    model.train(data)\n\nif __name__ ==  __main__ :\n    train()  Models in Guild are very flexible and can be used to manage any type\nof operation associated with the model.", 
            "title": "Models"
        }, 
        {
            "location": "/docs/models/#create-a-model", 
            "text": "To create a model, create a file named  guild.yml  in your model\nproject directory.   Tip  If you re starting from scratch, use the  init  command\nto initialize a new project, including a sample training script.   Using a text editor, paste the following code sample into  guild.yml :  - model: my-model\n  description: My model\n  operations:\n    train:\n      cmd: train_my_model    Replace the values for  model  and  description  to reflect the\n  attributes of your model.    Replace  train_my_model  with the Python module used to train your\n  model.    Save  guild.yml .    For more information on defining models and operations, see  Guile\nfile reference .  For a detailed step-by-step example of creating a Guild model, see Develop a model from\nscratch .  You may experiment with training your model by running:  guild train  For information on working with runs generated by operations, see Runs .", 
            "title": "Create a model"
        }, 
        {
            "location": "/docs/models/#install-models", 
            "text": "Guild AI is a model packaging and distribution system that can be used\nto install models from the deep learning ecosystem.  You install models by installing Guild packages that contain the\nmodels.  To install a package, run:  guild install PACKAGE  You can lookup packages by running:  guild search TERM  For example, to install a package that contains ResNet models, search\nfor  resnet  by running:  guild search resnet  You can also browse  Guild AI models .  After reviewing the list of available packages, if you decide to\ninstall  tensorflow.reset , you can install it by running:  guild install tensorflow.resnet  For more information, see  Install\npackages  and the  install \ncommand.", 
            "title": "Install models"
        }, 
        {
            "location": "/docs/models/#get-model-help", 
            "text": "To view help for models defined in the current directory (i.e. models\ndefined in  ./guild.yml ) run:  guild help  To view help for models defined in another directory, run:  guild help DIRECTORY   Note  DIRECTORY  must contain a Guild file (i.e.  guild.yml ).   To view help for models defined in a package, run:  guild help PACKAGE  For example, to view help for the models provided by tensorflow.resnet , run:  guild help tensorflow.resnet  For more information, see the  help  command.", 
            "title": "Get model help"
        }, 
        {
            "location": "/docs/models/#list-models", 
            "text": "List available models by running:  guild models  You can filter the results using a term. For example, to list\navailable models that contain  cnn , run:  guild models cnn  For more information, see the  models  command.", 
            "title": "List models"
        }, 
        {
            "location": "/docs/models/#uninstall-models", 
            "text": "You can remove models defined in packages by uninstalling the\npackages.  guild uninstall PACKAGE  The package that a model is associated with is displayed when you run guild models  in the form  PACKAGE/MODEL .  For more information, see  Uninstall\npackages  and the uninstall  command.", 
            "title": "Uninstall models"
        }, 
        {
            "location": "/docs/operations/", 
            "text": "Operations\n\n\n\n\nRun an operation\n\n\nOperation aliases\n\n\n\n\n\n\nGet operation help\n\n\nList operations\n\n\nFlags\n\n\nRequired resources\n\n\nImplementing an operation\n\n\n\n\nAn operation is an action performed on a \nmodel\n. When run, an\noperation generates a \nrun\n, which is persistent record of the\noperation.\n\n\nExamples of model operations include:\n\n\n\n\ntrain\n\n\nTrain a model\n\n\nevaluate\n\n\nEvaluate a trained model\n\n\nfinetune\n\n\nFine tune a pretrained model\n\n\nprepare\n\n\nPrepare a dataset for use in training\n\n\ngenerate\n\n\nUse a model to generate content\n\n\n\n\nWhile these operations are commonly used, model developers are free to\ndefine different operations as needed. For example, if a model\nsupports compression (e.g. by using quantization), it might define a\n\ncompress\n operation.\n\n\nRun an operation\n\n\nTo run an operation, use the \nrun\n command:\n\n\nguild run OPERATION [ARG...]\n\n\n\n\nOPERATION\n must include the complete operation name and may also\ninclude package and model information to disambiguate the operation.\n\n\nTo specify the model along with the operation name, use\n\nMODEL:OPERATION\n. For example, to run the \nprepare\n operation on a\nmodel named \niris-dataset\n, you would run:\n\n\nguild run iris-dataset:prepare\n\n\n\n\nFor more information, see the \nrun\n command.\n\n\nOperation aliases\n\n\nSome operations are so common that Guild provides\n\naliases\n for them. Aliases let you run commands\nthis way:\n\n\nguild OPERATION_ALIAS [MODEL] [ARG...]\n\n\n\n\nThe following operation aliases are supported:\n\n\n\n\ntrain\n\n\n\n\nFor example, to run the \ntrain\n operation on a model, use:\n\n\nguild train MODEL\n\n\n\n\nThis command is equivalent to running:\n\n\nguild run MODEL:train\n\n\n\n\nGet operation help\n\n\nOperation help is displayed for model help when run \nguild\nhelp\n. See \nGet model help\n and the\n\nhelp\n command for more information on general model help.\n\n\nYou can get help for a specific operation using the \n--help-op\n\noption with the \nrun\n command (or an operation alias).\n\n\nOperation help includes the list of flags you can specify for an\noperation. This is useful when you have started to type a run command\nand want help on available or required flags.\n\n\nFor example, to view operation help for the \ntrain\n operation, run:\n\n\nguild train --help-op\n\n\n\n\nList operations\n\n\nTo list available operations, run:\n\n\nguild operations\n\n\n\n\nFor more information, see the \noperations\n command.\n\n\nFlags\n\n\nFlags are operation parameters and are used to specify the behavior of\nan operation for a run.\n\n\nFlags are defined by the model operation. For more information on flag\ndefinitions, see \nFlags\n in the\nGuild file reference.\n\n\nFlag values are specified using \nNAME=VALUE\n arguments to the\n\nrun\n command (or operation alias).\n\n\nFor example, consider the operation help for\n\nkeras.mnist/mnist-mlp:train\n, which we can show by running:\n\n\nguild run mlp:train --help-op\n\n\n\n\nUsage: guild run [OPTIONS] mnist-mlp:train [FLAG]...\n\nTrain the MLP\n\nUse 'guild run --help' for a list of options.\n\nFlags:\n  batch-size  Training batch size (default is 128)\n  epochs      Number of epochs to train (default is 20)\n\n\n\n\nAs described in the operation help, \nmnist-mlp:train\n supports two\nflags: \nbatch-size\n and \nepochs\n. If we wanted to train the model over\n\n10\n epochs using a batch size of \n64\n, we would use:\n\n\nguild train batch-size=64 epochs=10\n\n\n\n\nRequired resources\n\n\nOperations may require \nresources\n. Required resources\nare listed in the operation\ns \nrequires\n attribute.\n\n\nWhen Guild starts an operation, it first resolves each required\nresource. If a resource cannot be resolved, the operation fails with\nan error message.\n\n\nResources are resolved by acquiring them (e.g. download a file from\nthe Internet), verifying them, and finally creating links to resources\nfiles in the run directory. In this way, operations can easily express\n\nI need these files to run\n and ensure that the correct files are\navailable for each run.\n\n\nIn most cases resources are automatically resolved, but in some cases\nan operation may require that the user specify a resource. Resources\ncan be specified the same way flag values are specified\nusing\n\nNAME=VALUE\n. In the case of a resource, \nVALUE\n is the name of the\nrequired resource.\n\n\nRequired resources are described in operation help, if applicable.\n\n\nImplementing an operation\n\n\nOperations are implemented in Python modules. If \nmain\n is specified,\nthe module must execute when loaded, and should use this pattern:\n\n\ndef main():\n    \nOperation code here.\n\n\nif __name__ == \n__main__\n:\n    main()\n\n\n\n\nOperations are executed in the context of the current run directory.\n\n\n\n\n\nOperations have access to a number of environment variables.\n\n\n\n\nCMD_DIR\n\n\nPath where the operation was run. This is the original working\n  directory that was changed to \nRUN_DIR\n for the operation.\n\n\nGUILD_HOME\n\n\nGuild install location.\n\n\nGUILD_OP\n\n\nName of the operation including the model.\n\n\nGUILD_PLUGINS\n\n\nComma separated list of active Guild plugins.\n\n\nLOG_LEVEL\n\n\nPython log level active for the run.\n\n\nMODEL_DIR\n\n\nThe directory containing the operation model definition. This is\n  where the Guild file is located and can be used to reference\n  relative files.\n\n\nRUN_DIR\n\n\nActive run directory path. This is the working directory during an\n  operation. See \nCMD_DIR\n for the original working directory -\n  i.e. where the operation was run from.\n\n\nRUN_ID\n\n\nActive run ID.", 
            "title": "Operations"
        }, 
        {
            "location": "/docs/operations/#operations", 
            "text": "Run an operation  Operation aliases    Get operation help  List operations  Flags  Required resources  Implementing an operation   An operation is an action performed on a  model . When run, an\noperation generates a  run , which is persistent record of the\noperation.  Examples of model operations include:   train  Train a model  evaluate  Evaluate a trained model  finetune  Fine tune a pretrained model  prepare  Prepare a dataset for use in training  generate  Use a model to generate content   While these operations are commonly used, model developers are free to\ndefine different operations as needed. For example, if a model\nsupports compression (e.g. by using quantization), it might define a compress  operation.", 
            "title": "Operations"
        }, 
        {
            "location": "/docs/operations/#run-an-operation", 
            "text": "To run an operation, use the  run  command:  guild run OPERATION [ARG...]  OPERATION  must include the complete operation name and may also\ninclude package and model information to disambiguate the operation.  To specify the model along with the operation name, use MODEL:OPERATION . For example, to run the  prepare  operation on a\nmodel named  iris-dataset , you would run:  guild run iris-dataset:prepare  For more information, see the  run  command.", 
            "title": "Run an operation"
        }, 
        {
            "location": "/docs/operations/#operation-aliases", 
            "text": "Some operations are so common that Guild provides aliases  for them. Aliases let you run commands\nthis way:  guild OPERATION_ALIAS [MODEL] [ARG...]  The following operation aliases are supported:   train   For example, to run the  train  operation on a model, use:  guild train MODEL  This command is equivalent to running:  guild run MODEL:train", 
            "title": "Operation aliases"
        }, 
        {
            "location": "/docs/operations/#get-operation-help", 
            "text": "Operation help is displayed for model help when run  guild\nhelp . See  Get model help  and the help  command for more information on general model help.  You can get help for a specific operation using the  --help-op \noption with the  run  command (or an operation alias).  Operation help includes the list of flags you can specify for an\noperation. This is useful when you have started to type a run command\nand want help on available or required flags.  For example, to view operation help for the  train  operation, run:  guild train --help-op", 
            "title": "Get operation help"
        }, 
        {
            "location": "/docs/operations/#list-operations", 
            "text": "To list available operations, run:  guild operations  For more information, see the  operations  command.", 
            "title": "List operations"
        }, 
        {
            "location": "/docs/operations/#flags", 
            "text": "Flags are operation parameters and are used to specify the behavior of\nan operation for a run.  Flags are defined by the model operation. For more information on flag\ndefinitions, see  Flags  in the\nGuild file reference.  Flag values are specified using  NAME=VALUE  arguments to the run  command (or operation alias).  For example, consider the operation help for keras.mnist/mnist-mlp:train , which we can show by running:  guild run mlp:train --help-op  Usage: guild run [OPTIONS] mnist-mlp:train [FLAG]...\n\nTrain the MLP\n\nUse 'guild run --help' for a list of options.\n\nFlags:\n  batch-size  Training batch size (default is 128)\n  epochs      Number of epochs to train (default is 20)  As described in the operation help,  mnist-mlp:train  supports two\nflags:  batch-size  and  epochs . If we wanted to train the model over 10  epochs using a batch size of  64 , we would use:  guild train batch-size=64 epochs=10", 
            "title": "Flags"
        }, 
        {
            "location": "/docs/operations/#required-resources", 
            "text": "Operations may require  resources . Required resources\nare listed in the operation s  requires  attribute.  When Guild starts an operation, it first resolves each required\nresource. If a resource cannot be resolved, the operation fails with\nan error message.  Resources are resolved by acquiring them (e.g. download a file from\nthe Internet), verifying them, and finally creating links to resources\nfiles in the run directory. In this way, operations can easily express I need these files to run  and ensure that the correct files are\navailable for each run.  In most cases resources are automatically resolved, but in some cases\nan operation may require that the user specify a resource. Resources\ncan be specified the same way flag values are specified using NAME=VALUE . In the case of a resource,  VALUE  is the name of the\nrequired resource.  Required resources are described in operation help, if applicable.", 
            "title": "Required resources"
        }, 
        {
            "location": "/docs/operations/#implementing-an-operation", 
            "text": "Operations are implemented in Python modules. If  main  is specified,\nthe module must execute when loaded, and should use this pattern:  def main():\n     Operation code here. \n\nif __name__ ==  __main__ :\n    main()  Operations are executed in the context of the current run directory.   Operations have access to a number of environment variables.   CMD_DIR  Path where the operation was run. This is the original working\n  directory that was changed to  RUN_DIR  for the operation.  GUILD_HOME  Guild install location.  GUILD_OP  Name of the operation including the model.  GUILD_PLUGINS  Comma separated list of active Guild plugins.  LOG_LEVEL  Python log level active for the run.  MODEL_DIR  The directory containing the operation model definition. This is\n  where the Guild file is located and can be used to reference\n  relative files.  RUN_DIR  Active run directory path. This is the working directory during an\n  operation. See  CMD_DIR  for the original working directory -\n  i.e. where the operation was run from.  RUN_ID  Active run ID.", 
            "title": "Implementing an operation"
        }, 
        {
            "location": "/docs/runs/", 
            "text": "Runs\n\n\n\n\nOverview\n\n\nConcepts\n\n\nRun directory\n\n\nLimiting runs\n\n\nRun scope\n\n\nRun filtering\n\n\n\n\n\n\nSelecting runs\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nStart a run\n\n\nOperation aliases\n\n\n\n\n\n\nFlag values\n\n\nList runs\n\n\nGet run information\n\n\nCompare runs\n\n\nLabel runs\n\n\nDelete runs\n\n\nFrequently used delete commands\n\n\n\n\n\n\nRestore deleted runs\n\n\nPurge deleted runs\n\n\n\n\nOverview\n\n\nRuns are generated in Guild AI by running an \noperation\n.\n\n\nWhen you train a model, you generate a \nrun\n, which contains the\ntrained model as well as training logs and other artifacts associated\nwith the operation.\n\n\nSimilarly, when you fine tune a model, you generate a run. When you\ntest a model, you generate a run. In fact, any operation that you run\ngenerates a distinct run. This is how Guild manages your work.\n\n\nHere is a common work flow:\n\n\n\n\nFind and install a model\n\n\nRun an operation on that model (e.g. \ntrain\n)\n\n\nMonitor the progress of the operation (e.g. \nview\n)\n\n\nRun another operation with different hyper-parameters (flags)\n\n\nCompare runs\n\n\nDelete runs that you\nre no longer interested in\n\n\nSelect successful runs for deployment or use in other operations\n\n\n\n\nThe work centers on \nruns\ncreating, comparing, and selecting.\n\n\nConcepts\n\n\nAs you work with runs in Guild it\ns important to understand some core\nconcepts. If you\nd prefer to skip this conceptual material, jump to\n\nStart a run\n below.\n\n\nRun directory\n\n\nA \nrun directory\n is a file system directory (or folder) that contains\nartifacts associated with a run. Guild creates a unique run directory\nfor every run. This directory contains a variety of important data:\n\n\n\n\nRun metadata\n\n\nRun sources such as datasets\n\n\nRun output such as event logs and saved models\n\n\n\n\nRun directories are located in \nGUILD_HOME/runs\n. For more information\nsee \nGuild home\n.\n\n\nRun related operations interact with run directories in various ways:\n\n\n\n\nguild run\n creates a new run directory\n\n\nguild runs info\n prints information read from a run directory\n\n\nguild runs list\n enumerates run directories\n\n\nguild runs delete\n deletes run directories\n\n\n\n\nLimiting runs\n\n\nOver time you\nll generate a large number of runs. This list can become\nunwieldy, especially when you\nre interested in a small subset\n\ne.g. runs associated with a particular model you\nre working with. For\nthis reason, Guild provides two ways of limiting the runs that apply\nto run related commands:\n\n\n\n\nLimit to runs associated with a model defined in the current\n  directory\n\n\nLimit to runs that match a filter\n\n\n\n\nRun scope\n\n\nThe first limit is known as \nrun scope\n. Scope can be either \nlocal\n\nor \nglobal\n. By default, scope is local when the current directory\ncontains a \nmodel definition\n, otherwise scope is\nglobal. Local scope limits runs to those associated with models\ndefined in the current directory. Global scope displays all runs.\n\n\nGlobal scope can be applied using the \n--all\n (or \n-a\n) option.\n\n\nRun scope is applied based on the directory that Guild commands are\nrun in. Consider the following directory structure:\n\n\n\n\n\n\nHome\nDoes not contain a model definition \n global scope applies\n\n\nModels\n \n\n \nmnist \nContains a model definition \n local scope applies\n\n  \n\n  \nMODELS \nModel definition\n\n  \n\n \n\n \n\n\n\n\n\n\n\n\n\nCommands run the from \n/Home\n have \nglobal\n run scope because \n/Home\n\ndoesn\nt contain a model definition. Commands run from \n/Models/mnist\n\nhowever have \nlocal\n scope because that directory contains a model\ndefinition (\n/Models/mnist/MODELS\n).\n\n\nRun scope defaults to \nlocal\n when a model definition is exists\nbecause Guild assumes that the user is working on models defined at\nthat location and is not interested in other runs, at least by\ndefault. This follows the pattern of command line tools such as \ngit\n\nthat apply operations locally when they find a project, repository,\netc. in the current directory.\n\n\nWhen a command is run in local scope, Guild prints a message to\nindicate that results are limited:\n\n\nLimiting runs to the current directory (use --all to include all)\n\n\n\nRun filtering\n\n\nThe other limit is \nrun filtering\n. Filters are applied with command\nline options that specify run attributes, which may include:\n\n\n\n\nOperation\n\n\nRun status\n\n\nDeleted status\n\n\n\n\nRun filtering is applied \nafter\n run scope (see above).\n\n\nFor example, to view runs that are associated with the \ntrain\n\noperation, use the \n--op\n (or \n-o\n) option:\n\n\nguild runs --op train\n\n\n\n\nIf the command is in local scope, Guild will limit runs to those\nassociated with models in the current directory otherwise it will use\nall runs. It will then filter those runs, limiting the result to those\nassociated with operations containing the string \ntrain\n.\n\n\nSelecting runs\n\n\nSome run related commands let you select one or more runs:\n\n\n\n\nruns delete\nruns info\nruns purge\nruns restore\n\n\n\n\nFor these commands, runs can be specified in various ways:\n\n\n\n\nIndex as returned by \nguild runs\n or \nguild runs list\n\n\nRun ID (full or partial if unique)\n\n\n\n\nAdditionally, a range may be specified using run indexes in the form:\n\n\n[START]:[STOP]\n\n\n\nSTOP\n and \nSTART\n are inclusive\nruns are selected beginning with\nthe \nSTOP\n index up to and including those with the \nSTART\n index.\n\n\nBoth \nSTOP\n and \nSTART\n are optional. If \nSTART\n is omitted it is\nassumed to be \n0\n (i.e. the first run in the list). If \nSTOP\n is\nomitted it is assumed to be the index of the last run.\n\n\n\n\nImportant\n\n\nRun indexes are relative to the list of runs returned by \nguild\nruns\n or \nguild runs list\n for a given scope and filter (see\n\nLimiting runs\n above). The run associated with index\n\n0\n for one listing may not be the same run for another\nlisting. Always verify the selected runs before proceeding with a\ncommand.\n\n\nWhen in doubt, use a run ID to select a run.\n\n\n\n\nExamples\n\n\nConsider this output from \nguild runs\n:\n\n\nLimiting runs to the current directory (use --all to include all)\n[0:9734f85e]   ./slim-resnet-101:train        2017-12-14 07:56:32  terminated\n[1:d8cde0fc]   ./slim-resnet-50:export        2017-12-13 13:14:31  completed\n[2:0df943ac]   ./slim-resnet-50:predict       2017-12-06 11:51:15  completed\n[3:e150e44a]   ./slim-resnet-50:predict       2017-12-06 11:50:00  completed\n\n\n\n\n\n\nNote\n\n\nThe \nrun scope\n in the above command is \nlocal\n. If\nthe user had run \nguild runs --all\n the scope would be \nglobal\n\nthe list and run indexes would likely be different.\n\n\n\n\nBelow are various operations with run selectors applied to this\nlist.\n\n\n\n\nguild runs rm 0\n\n\nDelete run \n9734f85e\n (you can always use index \n0\n to select the\n  most recently started run in the list)\n\n\nguild runs rm 1:2\n\n\nDelete runs \nd8cde0fc\n and \n0df943ac\n\n\nguild runs rm :\n\n\nDelete all runs\n\n\nguild runs rm 0df943ac e150e44a\n\n\nDelete runs \n0df943ac\n and \ne150e44a\n\n\n\n\n\n\nNote\n\n\nThe following assumptions must hold for the above examples that use\nrun indexes:\n\n\n\n\n\n\nCommands must be executed in the same directory as the command\n  that generated the list and without scope modifiers or filters\n\n\n\n\n\n\nThe runs themselves must not change\ni.e. runs cannot be deleted\n  or started\n\n\n\n\n\n\n\n\nStart a run\n\n\nTo start a run, use the \nrun\n command. The basic format of a\n\nrun\n command looks like this:\n\n\nguild run OPERATION\nguild run MODEL:OPERATION\nguild run PACKAGE/MODEL:OPERATION\n\n\n\n\nYou can list available operations using the \noperations\n\ncommand.\n\n\nIn general, you can omit information about an operation name as long\nas Guild can uniquely identify the operation.\n\n\nFor example, if the output of \noperations\n looks like this:\n\n\niris/iris-cnn:train\niris/iris-cnn:finetune\niris/iris-cnn:test\n\n\n\n\nYou can start the \nfinetune\n operation by running:\n\n\nguild run finetune\n\n\n\n\nYou can always provide the model or package. For example, this form\nwill also start \nfinetune\n:\n\n\nguild run iris-cnn:finetune\n\n\n\n\nYou use part of the operation specification as long as Guild can\nuniquely identify the operation. For example, you can run the \ntest\n\non \niris-cnn\n using:\n\n\nguild run cnn:train\n\n\n\n\nOperation aliases\n\n\nSome operations are so common that Guild provides \nalias\n\ncommands. Aliases currently include:\n\n\n\n\ntrain\n\n\n\n\nAliases are used to start operation using these forms:\n\n\nguild ALIAS_CMD\nguild ALIAS_CMD MODEL\nguild ALIAS_CMD PACKAGE/MODEL\n\n\n\n\nThe \ntrain\n alias is used to run the \ntrain\n operation. In the example\nabove, the following commands can be used to train the iris model:\n\n\nguild train\nguild train iris-cnn\nguild train cnn\n\n\n\n\nFlag values\n\n\nSpecify operation flag values as \nNAME=VALUE\n arguments to \nrun\n.\n\n\nTo get help on available and required flags for an operation, run:\n\n\nguild run OPERATION --help-op\n\n\n\n\nYou can also view help for models defined in the current directory by\nrunning:\n\n\nguild help\n\n\n\n\nTo get help for a packaged model, run:\n\n\nguild help PACKAGE\n\n\n\n\nIf you omit a required flag, the \nrun\n command (or applicable alias)\nwill exit with an error message.\n\n\nList runs\n\n\nTo list Guild runs, use the \nruns\n or \nruns list\n\ncommand.\n\n\nguild runs\n is shorthand for \nguild runs list\n.\n\n\nWhen listing runs, be aware of \nrun scope\n and \nrun\nfiltering\nthese effect the runs that are\ndisplayed.\n\n\n\n\nguild runs\n\n\nList all runs with the run scope. If the current directory contain a\n  \nmodel definition\n the list is limited to runs\n  associated with the locally defined models, otherwise the list will\n  contain all runs.\n\n\nfoo\n\n\nAnother thing yo.\n\n\n\n\nThe command:\n\n\nguild runs\n\n\n\n\nwill display different lists depending on the directory it\ns run\nin. If the directory contains a model definition, runs will be limited\nto those associated with the locally defined models. If the directory\ndoes not contain a model definition, all runs are displayed.\n\n\nGet run information\n\n\nUse \nruns info\n to show information about a run.\n\n\nBy default, Guild shows information about the latest run:\n\n\nguild runs info\n\n\n\n\nYou can select a specific run by providing a run ID or index.\n\n\nRun indexes are displayed in run lists (see \nList runs\n\nabove).\n\n\nCompare runs\n\n\nCompare runs by running:\n\n\nguild compare\n\n\n\n\nGuild Compare is spreadsheet-like application that displays runs,\ntheir status, and metrics such as validation accuracy and training\nloss.\n\n\nTo display compare results as a table, use:\n\n\nguild compare --table\n\n\n\n\nTo display compare results in CSV format (e.g. for use in Excel), use:\n\n\nguild compare --csv\n\n\n\n\nFor more help, see the \ncompare\n command.\n\n\nLabel runs\n\n\nRuns can have \nlabels\n, which provide additional information about the\nrun. A label can used for filtering in the \nruns list\n\ncommand.\n\n\nUse \nruns label\n to set or clear a label for a run.\n\n\nUse \nguild runs list LABEL\n to list runs with the specified label.\n\n\nDelete runs\n\n\nDelete runs using \nguild runs delete\n or \nguild runs rm\n. See\n\nruns delete\n for command details.\n\n\nGuild will display the list of runs to be deleted and ask you to\nconfirm the operation. You must type \ny\n and then press \nEnter\n to\nconfirm.\n\n\nDeleted runs can be restored using the \nruns\nrestore\n command. Refer to \nRestoring deleted\nruns\n below for details.\n\n\nFrequently used delete commands\n\n\nTo delete all failed runs, use:\n\n\nguild runs rm -E\n\n\n\n\nTo permanently delete all failed runs, use:\n\n\nguild runs rm -Ep\n\n\n\n\n\n\nImportant\n\n\nPermanently deleted runs cannot be recovered!\n\n\n\n\nTo delete all failed and terminated runs, use:\n\n\nguild runs -ET\n\n\n\n\nRestore deleted runs\n\n\nDeleted runs can be recovered by running:\n\n\nguild runs restore [RUN...]\n\n\n\n\nFor more help, see the \nruns restore\n command.\n\n\nPurge deleted runs\n\n\nThe disk space used by deleted runs can be recovered by permanently\ndeleting them using \nruns purge\n.\n\n\n\n\nTip\n\n\nYou can show the list deleted runs using \nguild runs --deleted\n.\n\n\n\n\nFor example, to permanently delete all deleted runs, use:\n\n\nguild runs purge\n\n\n\n\nGuild will prompt you before proceeding.\n\n\n\n\nImportant\n\n\nPurging deleted runs will permanently delete them! Be certain that\nyou don\nt need a run before permanently deleting it.\n\n\n\n\nFor more help, see the \nruns purge\n command.", 
            "title": "Runs"
        }, 
        {
            "location": "/docs/runs/#runs", 
            "text": "Overview  Concepts  Run directory  Limiting runs  Run scope  Run filtering    Selecting runs  Examples      Start a run  Operation aliases    Flag values  List runs  Get run information  Compare runs  Label runs  Delete runs  Frequently used delete commands    Restore deleted runs  Purge deleted runs", 
            "title": "Runs"
        }, 
        {
            "location": "/docs/runs/#overview", 
            "text": "Runs are generated in Guild AI by running an  operation .  When you train a model, you generate a  run , which contains the\ntrained model as well as training logs and other artifacts associated\nwith the operation.  Similarly, when you fine tune a model, you generate a run. When you\ntest a model, you generate a run. In fact, any operation that you run\ngenerates a distinct run. This is how Guild manages your work.  Here is a common work flow:   Find and install a model  Run an operation on that model (e.g.  train )  Monitor the progress of the operation (e.g.  view )  Run another operation with different hyper-parameters (flags)  Compare runs  Delete runs that you re no longer interested in  Select successful runs for deployment or use in other operations   The work centers on  runs creating, comparing, and selecting.", 
            "title": "Overview"
        }, 
        {
            "location": "/docs/runs/#concepts", 
            "text": "As you work with runs in Guild it s important to understand some core\nconcepts. If you d prefer to skip this conceptual material, jump to Start a run  below.", 
            "title": "Concepts"
        }, 
        {
            "location": "/docs/runs/#run-directory", 
            "text": "A  run directory  is a file system directory (or folder) that contains\nartifacts associated with a run. Guild creates a unique run directory\nfor every run. This directory contains a variety of important data:   Run metadata  Run sources such as datasets  Run output such as event logs and saved models   Run directories are located in  GUILD_HOME/runs . For more information\nsee  Guild home .  Run related operations interact with run directories in various ways:   guild run  creates a new run directory  guild runs info  prints information read from a run directory  guild runs list  enumerates run directories  guild runs delete  deletes run directories", 
            "title": "Run directory"
        }, 
        {
            "location": "/docs/runs/#limiting-runs", 
            "text": "Over time you ll generate a large number of runs. This list can become\nunwieldy, especially when you re interested in a small subset \ne.g. runs associated with a particular model you re working with. For\nthis reason, Guild provides two ways of limiting the runs that apply\nto run related commands:   Limit to runs associated with a model defined in the current\n  directory  Limit to runs that match a filter", 
            "title": "Limiting runs"
        }, 
        {
            "location": "/docs/runs/#run-scope", 
            "text": "The first limit is known as  run scope . Scope can be either  local \nor  global . By default, scope is local when the current directory\ncontains a  model definition , otherwise scope is\nglobal. Local scope limits runs to those associated with models\ndefined in the current directory. Global scope displays all runs.  Global scope can be applied using the  --all  (or  -a ) option.  Run scope is applied based on the directory that Guild commands are\nrun in. Consider the following directory structure:    Home Does not contain a model definition   global scope applies  Models\n  \n  mnist  Contains a model definition   local scope applies \n   \n   MODELS  Model definition \n   \n  \n      Commands run the from  /Home  have  global  run scope because  /Home \ndoesn t contain a model definition. Commands run from  /Models/mnist \nhowever have  local  scope because that directory contains a model\ndefinition ( /Models/mnist/MODELS ).  Run scope defaults to  local  when a model definition is exists\nbecause Guild assumes that the user is working on models defined at\nthat location and is not interested in other runs, at least by\ndefault. This follows the pattern of command line tools such as  git \nthat apply operations locally when they find a project, repository,\netc. in the current directory.  When a command is run in local scope, Guild prints a message to\nindicate that results are limited:  Limiting runs to the current directory (use --all to include all)", 
            "title": "Run scope"
        }, 
        {
            "location": "/docs/runs/#run-filtering", 
            "text": "The other limit is  run filtering . Filters are applied with command\nline options that specify run attributes, which may include:   Operation  Run status  Deleted status   Run filtering is applied  after  run scope (see above).  For example, to view runs that are associated with the  train \noperation, use the  --op  (or  -o ) option:  guild runs --op train  If the command is in local scope, Guild will limit runs to those\nassociated with models in the current directory otherwise it will use\nall runs. It will then filter those runs, limiting the result to those\nassociated with operations containing the string  train .", 
            "title": "Run filtering"
        }, 
        {
            "location": "/docs/runs/#selecting-runs", 
            "text": "Some run related commands let you select one or more runs:   runs delete runs info runs purge runs restore   For these commands, runs can be specified in various ways:   Index as returned by  guild runs  or  guild runs list  Run ID (full or partial if unique)   Additionally, a range may be specified using run indexes in the form:  [START]:[STOP]  STOP  and  START  are inclusive runs are selected beginning with\nthe  STOP  index up to and including those with the  START  index.  Both  STOP  and  START  are optional. If  START  is omitted it is\nassumed to be  0  (i.e. the first run in the list). If  STOP  is\nomitted it is assumed to be the index of the last run.   Important  Run indexes are relative to the list of runs returned by  guild\nruns  or  guild runs list  for a given scope and filter (see Limiting runs  above). The run associated with index 0  for one listing may not be the same run for another\nlisting. Always verify the selected runs before proceeding with a\ncommand.  When in doubt, use a run ID to select a run.", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/runs/#examples", 
            "text": "Consider this output from  guild runs :  Limiting runs to the current directory (use --all to include all)\n[0:9734f85e]   ./slim-resnet-101:train        2017-12-14 07:56:32  terminated\n[1:d8cde0fc]   ./slim-resnet-50:export        2017-12-13 13:14:31  completed\n[2:0df943ac]   ./slim-resnet-50:predict       2017-12-06 11:51:15  completed\n[3:e150e44a]   ./slim-resnet-50:predict       2017-12-06 11:50:00  completed   Note  The  run scope  in the above command is  local . If\nthe user had run  guild runs --all  the scope would be  global \nthe list and run indexes would likely be different.   Below are various operations with run selectors applied to this\nlist.   guild runs rm 0  Delete run  9734f85e  (you can always use index  0  to select the\n  most recently started run in the list)  guild runs rm 1:2  Delete runs  d8cde0fc  and  0df943ac  guild runs rm :  Delete all runs  guild runs rm 0df943ac e150e44a  Delete runs  0df943ac  and  e150e44a    Note  The following assumptions must hold for the above examples that use\nrun indexes:    Commands must be executed in the same directory as the command\n  that generated the list and without scope modifiers or filters    The runs themselves must not change i.e. runs cannot be deleted\n  or started", 
            "title": "Examples"
        }, 
        {
            "location": "/docs/runs/#start-a-run", 
            "text": "To start a run, use the  run  command. The basic format of a run  command looks like this:  guild run OPERATION\nguild run MODEL:OPERATION\nguild run PACKAGE/MODEL:OPERATION  You can list available operations using the  operations \ncommand.  In general, you can omit information about an operation name as long\nas Guild can uniquely identify the operation.  For example, if the output of  operations  looks like this:  iris/iris-cnn:train\niris/iris-cnn:finetune\niris/iris-cnn:test  You can start the  finetune  operation by running:  guild run finetune  You can always provide the model or package. For example, this form\nwill also start  finetune :  guild run iris-cnn:finetune  You use part of the operation specification as long as Guild can\nuniquely identify the operation. For example, you can run the  test \non  iris-cnn  using:  guild run cnn:train", 
            "title": "Start a run"
        }, 
        {
            "location": "/docs/runs/#operation-aliases", 
            "text": "Some operations are so common that Guild provides  alias \ncommands. Aliases currently include:   train   Aliases are used to start operation using these forms:  guild ALIAS_CMD\nguild ALIAS_CMD MODEL\nguild ALIAS_CMD PACKAGE/MODEL  The  train  alias is used to run the  train  operation. In the example\nabove, the following commands can be used to train the iris model:  guild train\nguild train iris-cnn\nguild train cnn", 
            "title": "Operation aliases"
        }, 
        {
            "location": "/docs/runs/#flag-values", 
            "text": "Specify operation flag values as  NAME=VALUE  arguments to  run .  To get help on available and required flags for an operation, run:  guild run OPERATION --help-op  You can also view help for models defined in the current directory by\nrunning:  guild help  To get help for a packaged model, run:  guild help PACKAGE  If you omit a required flag, the  run  command (or applicable alias)\nwill exit with an error message.", 
            "title": "Flag values"
        }, 
        {
            "location": "/docs/runs/#list-runs", 
            "text": "To list Guild runs, use the  runs  or  runs list \ncommand.  guild runs  is shorthand for  guild runs list .  When listing runs, be aware of  run scope  and  run\nfiltering these effect the runs that are\ndisplayed.   guild runs  List all runs with the run scope. If the current directory contain a\n   model definition  the list is limited to runs\n  associated with the locally defined models, otherwise the list will\n  contain all runs.  foo  Another thing yo.   The command:  guild runs  will display different lists depending on the directory it s run\nin. If the directory contains a model definition, runs will be limited\nto those associated with the locally defined models. If the directory\ndoes not contain a model definition, all runs are displayed.", 
            "title": "List runs"
        }, 
        {
            "location": "/docs/runs/#get-run-information", 
            "text": "Use  runs info  to show information about a run.  By default, Guild shows information about the latest run:  guild runs info  You can select a specific run by providing a run ID or index.  Run indexes are displayed in run lists (see  List runs \nabove).", 
            "title": "Get run information"
        }, 
        {
            "location": "/docs/runs/#compare-runs", 
            "text": "Compare runs by running:  guild compare  Guild Compare is spreadsheet-like application that displays runs,\ntheir status, and metrics such as validation accuracy and training\nloss.  To display compare results as a table, use:  guild compare --table  To display compare results in CSV format (e.g. for use in Excel), use:  guild compare --csv  For more help, see the  compare  command.", 
            "title": "Compare runs"
        }, 
        {
            "location": "/docs/runs/#label-runs", 
            "text": "Runs can have  labels , which provide additional information about the\nrun. A label can used for filtering in the  runs list \ncommand.  Use  runs label  to set or clear a label for a run.  Use  guild runs list LABEL  to list runs with the specified label.", 
            "title": "Label runs"
        }, 
        {
            "location": "/docs/runs/#delete-runs", 
            "text": "Delete runs using  guild runs delete  or  guild runs rm . See runs delete  for command details.  Guild will display the list of runs to be deleted and ask you to\nconfirm the operation. You must type  y  and then press  Enter  to\nconfirm.  Deleted runs can be restored using the  runs\nrestore  command. Refer to  Restoring deleted\nruns  below for details.", 
            "title": "Delete runs"
        }, 
        {
            "location": "/docs/runs/#frequently-used-delete-commands", 
            "text": "To delete all failed runs, use:  guild runs rm -E  To permanently delete all failed runs, use:  guild runs rm -Ep   Important  Permanently deleted runs cannot be recovered!   To delete all failed and terminated runs, use:  guild runs -ET", 
            "title": "Frequently used delete commands"
        }, 
        {
            "location": "/docs/runs/#restore-deleted-runs", 
            "text": "Deleted runs can be recovered by running:  guild runs restore [RUN...]  For more help, see the  runs restore  command.", 
            "title": "Restore deleted runs"
        }, 
        {
            "location": "/docs/runs/#purge-deleted-runs", 
            "text": "The disk space used by deleted runs can be recovered by permanently\ndeleting them using  runs purge .   Tip  You can show the list deleted runs using  guild runs --deleted .   For example, to permanently delete all deleted runs, use:  guild runs purge  Guild will prompt you before proceeding.   Important  Purging deleted runs will permanently delete them! Be certain that\nyou don t need a run before permanently deleting it.   For more help, see the  runs purge  command.", 
            "title": "Purge deleted runs"
        }, 
        {
            "location": "/docs/resources/", 
            "text": "Resources\n\n\n\n\nResource sources\n\n\nResolving resources\n\n\nUnpacking sources\n\n\nSelecting source files\n\n\nOperation output\n\n\n\n\nA resource is named set of files that are used by model\n\noperations\n.\n\n\nResources may be defined at two levels:\n\n\n\n\nModel resource\n\n\nPackage resource\n\n\n\n\nModel resources are defined by a model. Here\ns an example:\n\n\n# MODEL\nname: simple-model\noperations:\n  train: train\n  requires: data\nresources:\n  data: data.csv\n\n\n\n\nResource sources\n\n\nResources are comprised of one or more \nsources\n. A source may be a\nstring or an object.\n\n\nA source string is equivalent to a source object with the value used\nfor the \nfile\n attribute (see below).\n\n\nA source object may define these attributes:\n\n\n\n\nurl\n\n\nSource is located on a remote server and is accessible via a\n  URL. The protocols \nhttp\n and \nhttps\n are supported. \nurl\n cannot be\n  used with either \nfile\n or \noperation\nthey are mutually\n  exclusive.\n\n\nfile\n\n\nSource is a file or directory located relative to the model or\n  package file. \nfile\n cannot be used with \nurl\n or \noperation\n\n  they are mutually exclusive.\n\n\noperation\n\n\nSource is a file generated by a model operation. Operations must be\n  specified as \n[[PACKAGE/]MODEL:]OPERATION\n. Multiple operations may\n  be specified by separating them with a comma. For more information\n  see \nOperation output\n below. \noperation\n cannot\n  be used with \nurl\n or \nfile\nthey are mutually exclusive.\n\n\nselect\n\n\nA regular expression used to select files from a local directory,\n  archive, or a run directory if the source is an operation. For more\n  information see \nSelecting source files\n\n  below. \nselect\n is required if \noperation\n is used.\n\n\nsha256\n\n\nA \nSHA-256\n hash of the\n  resource source. If specified, the source SHA-256 hash must match\n  this value for the resource to resolve.\n\n\nunpack\n\n\nA boolean flag (\ntrue\n or \nfalse\n) indicating whether or not to\n  unpack the source. If unspecified, sources are unpacked if they are\n  archives. For more information see \nUnpacking\n  sources\n below.\n\n\n\n\nThe attributes \nurl\n, \nfile\n, and \noperation\n collectively represent\nthe source \ntype\n. One and only one of these attributes must be\nspecified for each source object.\n\n\nResolving resources\n\n\nBefore an operation is started, each required resource must be\n\nresolved\n. Resource resolution consists of these steps for each\nresource source:\n\n\n\n\nAcquire the source\n\n\nIf a SHA-256 hash is available, verify the source\n\n\nIf the source is an archive and \nunpack\n is not \nno\n, unpack the\n  archive\n\n\nCreate link to the source within the operation run directory or, if\n  \nselect\n is specified, create a link for each matching path within\n  the source\n\n\n\n\nWhen all required resources are resolved, Guild will start the\noperation.\n\n\nURL sources are stored in Guild\ns \nresource\ncache\n. If a source\n\n\nUnpacking sources\n\n\nSource archives may be unpacked to access their constituent files. A\nfile is considered an archive if it has one of the following\nextensions: \n.zip\n, \n.tar\n, \n.tgz\n, \n.tar.*\n.\n\n\nBy default, archives are unpacked. You can explicitly disable\nunpacking by setting the \nunpack\n source attribute to \nno\n.\n\n\nSelecting source files\n\n\nA source may indicate that files within a directory or archive should\nbe \nselected\n for use by specifying the source \nselect\n attribute. The\nvalue of \nselect\n must be a value regular expression. When specified,\nGuild will create links to each matching path within the source\ndirectory or archive.\n\n\nArchives must be unpacked to select source files.\n\n\nLinks use the basename of each matching file and do not contain parent\npaths. To illustrate, consider this structure, which may apply to\neither a file system directory or the contents of an archive:\n\n\n\n\n\n\nmodels-master\n \n\n \nsrc\n  \n\n  \nmnist\n  \n\n \n\n \n\n\n\n\n\n\n\n\n\nTo create a link to \nmnist\n in the operation run directory, use a\n\nselect\n value of \nmodels-master/src/mnist\n.\n\n\nHere\ns a model definition that illustrates this scheme.\n\n\nname: example\noperations:\n  train:\n    cmd: train\n    requires: mnist\nresources:\n  mnist:\n    sources:\n    - url: https://github.com/acme/models/archive/master.zip\n      select: models-master/src/mnist\n\n\n\n\nOperation output\n\n\nIt\ns common for an operation to require the output of another\noperation. Examples include:\n\n\n\n\nModel training requires a prepared dataset\n\n\nModel compression requires a trained model\n\n\nModel deployment requires a compressed model\n\n\n\n\nBy using required resources with \noperation\n sources, model developers\ncan effectively link operations together in a pipeline.\n\n\nGuild resolves operations using these steps:\n\n\n\n\n\n\nIf the user specifies a run ID as an argument to the \nrun\n\n  command in the form \nRESOURCE_NAME=RUN_ID\n Guild will resolve the\n  operation source using the target run directory.\n\n\n\n\n\n\nIf the user does not specify a run ID, Guild uses the latest\n  non-error run for any of the specified operations. Multiple\n  operations may be specified by separating the operation specs with a\n  comma.\n\n\n\n\n\n\nConsider the following model definition:\n\n\nname: example\noperations:\n  prepare:\n    cmd: prepare\n  train:\n    cmd: train\n    requires: data\nresources:\n  data:\n    sources:\n    - operation: prepare\n      select: data.csv\n\n\n\n\nIn this example, the \ntrain\n operation requires output from the\n\nprepare\n operation. This requirement is expressed using the\n\nrequires\n operation attribute, which references the named resource\n\ndata\n. The \ndata\n resource consists of a single source:\n\ndata.csv\n, which is generated by the \nprepare\n operation.", 
            "title": "Resources"
        }, 
        {
            "location": "/docs/resources/#resources", 
            "text": "Resource sources  Resolving resources  Unpacking sources  Selecting source files  Operation output   A resource is named set of files that are used by model operations .  Resources may be defined at two levels:   Model resource  Package resource   Model resources are defined by a model. Here s an example:  # MODEL\nname: simple-model\noperations:\n  train: train\n  requires: data\nresources:\n  data: data.csv", 
            "title": "Resources"
        }, 
        {
            "location": "/docs/resources/#resource-sources", 
            "text": "Resources are comprised of one or more  sources . A source may be a\nstring or an object.  A source string is equivalent to a source object with the value used\nfor the  file  attribute (see below).  A source object may define these attributes:   url  Source is located on a remote server and is accessible via a\n  URL. The protocols  http  and  https  are supported.  url  cannot be\n  used with either  file  or  operation they are mutually\n  exclusive.  file  Source is a file or directory located relative to the model or\n  package file.  file  cannot be used with  url  or  operation \n  they are mutually exclusive.  operation  Source is a file generated by a model operation. Operations must be\n  specified as  [[PACKAGE/]MODEL:]OPERATION . Multiple operations may\n  be specified by separating them with a comma. For more information\n  see  Operation output  below.  operation  cannot\n  be used with  url  or  file they are mutually exclusive.  select  A regular expression used to select files from a local directory,\n  archive, or a run directory if the source is an operation. For more\n  information see  Selecting source files \n  below.  select  is required if  operation  is used.  sha256  A  SHA-256  hash of the\n  resource source. If specified, the source SHA-256 hash must match\n  this value for the resource to resolve.  unpack  A boolean flag ( true  or  false ) indicating whether or not to\n  unpack the source. If unspecified, sources are unpacked if they are\n  archives. For more information see  Unpacking\n  sources  below.   The attributes  url ,  file , and  operation  collectively represent\nthe source  type . One and only one of these attributes must be\nspecified for each source object.", 
            "title": "Resource sources"
        }, 
        {
            "location": "/docs/resources/#resolving-resources", 
            "text": "Before an operation is started, each required resource must be resolved . Resource resolution consists of these steps for each\nresource source:   Acquire the source  If a SHA-256 hash is available, verify the source  If the source is an archive and  unpack  is not  no , unpack the\n  archive  Create link to the source within the operation run directory or, if\n   select  is specified, create a link for each matching path within\n  the source   When all required resources are resolved, Guild will start the\noperation.  URL sources are stored in Guild s  resource\ncache . If a source", 
            "title": "Resolving resources"
        }, 
        {
            "location": "/docs/resources/#unpacking-sources", 
            "text": "Source archives may be unpacked to access their constituent files. A\nfile is considered an archive if it has one of the following\nextensions:  .zip ,  .tar ,  .tgz ,  .tar.* .  By default, archives are unpacked. You can explicitly disable\nunpacking by setting the  unpack  source attribute to  no .", 
            "title": "Unpacking sources"
        }, 
        {
            "location": "/docs/resources/#selecting-source-files", 
            "text": "A source may indicate that files within a directory or archive should\nbe  selected  for use by specifying the source  select  attribute. The\nvalue of  select  must be a value regular expression. When specified,\nGuild will create links to each matching path within the source\ndirectory or archive.  Archives must be unpacked to select source files.  Links use the basename of each matching file and do not contain parent\npaths. To illustrate, consider this structure, which may apply to\neither a file system directory or the contents of an archive:    models-master\n  \n  src\n   \n   mnist\n   \n  \n      To create a link to  mnist  in the operation run directory, use a select  value of  models-master/src/mnist .  Here s a model definition that illustrates this scheme.  name: example\noperations:\n  train:\n    cmd: train\n    requires: mnist\nresources:\n  mnist:\n    sources:\n    - url: https://github.com/acme/models/archive/master.zip\n      select: models-master/src/mnist", 
            "title": "Selecting source files"
        }, 
        {
            "location": "/docs/resources/#operation-output", 
            "text": "It s common for an operation to require the output of another\noperation. Examples include:   Model training requires a prepared dataset  Model compression requires a trained model  Model deployment requires a compressed model   By using required resources with  operation  sources, model developers\ncan effectively link operations together in a pipeline.  Guild resolves operations using these steps:    If the user specifies a run ID as an argument to the  run \n  command in the form  RESOURCE_NAME=RUN_ID  Guild will resolve the\n  operation source using the target run directory.    If the user does not specify a run ID, Guild uses the latest\n  non-error run for any of the specified operations. Multiple\n  operations may be specified by separating the operation specs with a\n  comma.    Consider the following model definition:  name: example\noperations:\n  prepare:\n    cmd: prepare\n  train:\n    cmd: train\n    requires: data\nresources:\n  data:\n    sources:\n    - operation: prepare\n      select: data.csv  In this example, the  train  operation requires output from the prepare  operation. This requirement is expressed using the requires  operation attribute, which references the named resource data . The  data  resource consists of a single source: data.csv , which is generated by the  prepare  operation.", 
            "title": "Operation output"
        }, 
        {
            "location": "/docs/packages/", 
            "text": "Packages\n\n\n\n\nFind packages\n\n\nInstall packages\n\n\nList installed packages\n\n\nUninstall packages\n\n\nCreate package\n\n\n\n\nA Guild AI \npackage\n is a container for \nmodels\n and\n\nresources\n. Packages let developers easily publish\ntheir work for users to discover. They let users easily find, install,\nand use models. Packages are the central feature of Guild\ns support\nfor model collaboration, sharing and reuse.\n\n\nFind packages\n\n\nYou can find Guild packages in various ways:\n\n\n\n\nVisit Guild\ns \nmodel repository\n\n\nSearch for a model using the \nsearch\n command\n\n\n\n\nFor example, to find models that support the ImageNet dataset, simply run:\n\n\nguild search imagenet\n\n\n\n\nNew models are being published all the time so if you don\nt find what\nyou\nre looking for, let the community know by \nsubmitting a request on\nGuild\ns issue tracker\n.\n\n\nIt\ns also easy to create and publish your own models.\n\n\nInstall packages\n\n\nInstall a package by running:\n\n\nguild install PACKAGE\n\n\n\n\nYou can find package names using \nsearch\n (see \nFind\npackages\n above).\n\n\nYou can also browse \nGuild AI models\n.\n\n\nList installed packages\n\n\nTo list installed Guild AI packages, run:\n\n\nguild packages\n\n\n\n\nYou can list specific packages using \npackages\nlist\n:\n\n\nguild packages list FILTER\n\n\n\n\nFor example, to list installed packages containing \nmagenta\n, run:\n\n\nguild packages list magenta\n\n\n\n\nUninstall packages\n\n\nUninstall a package by running:\n\n\nguild uninstall PACKAGE\n\n\n\n\nGuild will prompt you before deleting package files.\n\n\nIf you\nd prefer to skip the prompt, use the \n-y\n option:\n\n\nguild uninstall PACKAGE -y\n\n\n\n\nCreate package\n\n\nCreating packages is an advanced topic that is not currently covered\nin this documentation.\n\n\nYou may however review the package definitions at\n\nhttps://github.com/guildai/packages\n for examples of packages.\n\n\nIf you need help creating a package, drop us a line at\n\nniceperson@guild.ai\n and we\nll be happy\nto help!", 
            "title": "Packages"
        }, 
        {
            "location": "/docs/packages/#packages", 
            "text": "Find packages  Install packages  List installed packages  Uninstall packages  Create package   A Guild AI  package  is a container for  models  and resources . Packages let developers easily publish\ntheir work for users to discover. They let users easily find, install,\nand use models. Packages are the central feature of Guild s support\nfor model collaboration, sharing and reuse.", 
            "title": "Packages"
        }, 
        {
            "location": "/docs/packages/#find-packages", 
            "text": "You can find Guild packages in various ways:   Visit Guild s  model repository  Search for a model using the  search  command   For example, to find models that support the ImageNet dataset, simply run:  guild search imagenet  New models are being published all the time so if you don t find what\nyou re looking for, let the community know by  submitting a request on\nGuild s issue tracker .  It s also easy to create and publish your own models.", 
            "title": "Find packages"
        }, 
        {
            "location": "/docs/packages/#install-packages", 
            "text": "Install a package by running:  guild install PACKAGE  You can find package names using  search  (see  Find\npackages  above).  You can also browse  Guild AI models .", 
            "title": "Install packages"
        }, 
        {
            "location": "/docs/packages/#list-installed-packages", 
            "text": "To list installed Guild AI packages, run:  guild packages  You can list specific packages using  packages\nlist :  guild packages list FILTER  For example, to list installed packages containing  magenta , run:  guild packages list magenta", 
            "title": "List installed packages"
        }, 
        {
            "location": "/docs/packages/#uninstall-packages", 
            "text": "Uninstall a package by running:  guild uninstall PACKAGE  Guild will prompt you before deleting package files.  If you d prefer to skip the prompt, use the  -y  option:  guild uninstall PACKAGE -y", 
            "title": "Uninstall packages"
        }, 
        {
            "location": "/docs/packages/#create-package", 
            "text": "Creating packages is an advanced topic that is not currently covered\nin this documentation.  You may however review the package definitions at https://github.com/guildai/packages  for examples of packages.  If you need help creating a package, drop us a line at niceperson@guild.ai  and we ll be happy\nto help!", 
            "title": "Create package"
        }, 
        {
            "location": "/docs/visual/guild-view/", 
            "text": "Guild View\n\n\nGuild View is a visual application provided with the Guild AI\ntoolkit. It can be used to view run results and browse run\nartifacts. It also integrates with TensorBoard to let you view\nTensorFlow event logs generated for runs.\n\n\n\n\nGuild View\n\n\n\n\nStarting Guild View\n\n\nViewing specific runs\n\n\nFiltering runs within Guild View\n\n\nViewing run files\n\n\nView runs in TensorBoard\n\n\nAutomatic updates\n\n\nStopping Guild View\n\n\n\n\nStarting Guild View\n\n\nStart Guild View by running:\n\n\nguild view\n\n\n\n\nThis will start Guild View an a randomly selected port and open it in\nyour browser.\n\n\n\n\nImportant\n\n\nIf you\nre running Guild View on a remote server, Guild will not be\nable to open it on your workstation. Instead, manually open the link\ndisplayed by the \nguild view\n command.\n\n\n\n\nIf you\nd like to run Guild View on a specific port, use:\n\n\nguild view --port PORT\n\n\n\n\nIf you\nd like to start Guild View without opening a new browser\nwindow, use:\n\n\nguild view --no-open\n\n\n\n\nFor more help, see the \nview\n command.\n\n\nViewing specific runs\n\n\nYou can limit the runs that you view using various Guild View options.\n\n\nFor example, to view only runs for operations associated with a\nparticular model, use the \n-o\n (or \n--operation\n) option and specify\nthe model:\n\n\nguild view -o MODEL\n\n\n\n\nFor example, if you\nre working with the \nresnet-50\n model, you can\ntell Guild View to only show runs for that model by running:\n\n\nguild view -o resnet-50\n\n\n\n\nFor a complete list of options, see the \nview\n command.\n\n\nFiltering runs within Guild View\n\n\nYou can further filter runs in Guild View using the \nFilter\n input\nin the upper left of the window.\n\n\nRuns can be filtered using:\n\n\n\n\nPackage\n\n\nModel\n\n\nOperation name\n\n\nRun ID\n\n\n\n\nViewing run files\n\n\nYou can browse run files\nboth those used as run input and those\ngenerated as output\nby clicking on the \nFILES\n tab.\n\n\n\n\nGuild View - \nFILES\n tab\n\n\nUse the \nFilter\n input at the top of the files list to view only\nfiles you\nre interested in.\n\n\nGuild View provides a built-in file viewer for some file types:\n\n\n\n\nImages\n\n\nMusic\n\n\n\n\nIf a file can be opened in Guild View, its name will appear as a light\ngrey button. When you click the button, Guild will open a file viewer.\n\n\n\n\nMedia file names have have grey buttons, which can be clicked to\n  view the file.\n\n\nWhen you\nre viewing files in Guild View, you can navigate through them\nusing \nNEXT\n and \nPREV\n buttons.\n\n\n\n\nUse Guild View to view generated artifacts like images and music\n\n\nView runs in TensorBoard\n\n\nGuild View provides integration with TensorBoard. To view TensorFlow\nevent logs, click \n in the upper left of\nthe window.\n\n\n\n\nImportant\n\n\nIf you\nre running Guild View on a remote server, the \nView in\nTensorBoard\n feature will work. This is a known issue and will be\nfixed in upcoming releases of Guild AI. To view runs in\nTensorBoard on a remote server, use the\n\ntensorboard\n command from the remote server\ninstead of Guild View.\n\n\n\n\nAutomatic updates\n\n\nGuild View and TensorBoard will automatically update to show the\nlatest runs and run files.\n\n\nStopping Guild View\n\n\nGuild View will run until it\ns stopped. To stop Guild View, type\n\nCTRL-C\n in the console window where Guild View is running.\n\n\n\n\nNote\n\n\nWhen you\nve stopped Guild View, the associated browser window will\nno longer update or respond.", 
            "title": "Guild View"
        }, 
        {
            "location": "/docs/visual/guild-view/#guild-view", 
            "text": "Guild View is a visual application provided with the Guild AI\ntoolkit. It can be used to view run results and browse run\nartifacts. It also integrates with TensorBoard to let you view\nTensorFlow event logs generated for runs.   Guild View   Starting Guild View  Viewing specific runs  Filtering runs within Guild View  Viewing run files  View runs in TensorBoard  Automatic updates  Stopping Guild View", 
            "title": "Guild View"
        }, 
        {
            "location": "/docs/visual/guild-view/#starting-guild-view", 
            "text": "Start Guild View by running:  guild view  This will start Guild View an a randomly selected port and open it in\nyour browser.   Important  If you re running Guild View on a remote server, Guild will not be\nable to open it on your workstation. Instead, manually open the link\ndisplayed by the  guild view  command.   If you d like to run Guild View on a specific port, use:  guild view --port PORT  If you d like to start Guild View without opening a new browser\nwindow, use:  guild view --no-open  For more help, see the  view  command.", 
            "title": "Starting Guild View"
        }, 
        {
            "location": "/docs/visual/guild-view/#viewing-specific-runs", 
            "text": "You can limit the runs that you view using various Guild View options.  For example, to view only runs for operations associated with a\nparticular model, use the  -o  (or  --operation ) option and specify\nthe model:  guild view -o MODEL  For example, if you re working with the  resnet-50  model, you can\ntell Guild View to only show runs for that model by running:  guild view -o resnet-50  For a complete list of options, see the  view  command.", 
            "title": "Viewing specific runs"
        }, 
        {
            "location": "/docs/visual/guild-view/#filtering-runs-within-guild-view", 
            "text": "You can further filter runs in Guild View using the  Filter  input\nin the upper left of the window.  Runs can be filtered using:   Package  Model  Operation name  Run ID", 
            "title": "Filtering runs within Guild View"
        }, 
        {
            "location": "/docs/visual/guild-view/#viewing-run-files", 
            "text": "You can browse run files both those used as run input and those\ngenerated as output by clicking on the  FILES  tab.   Guild View -  FILES  tab  Use the  Filter  input at the top of the files list to view only\nfiles you re interested in.  Guild View provides a built-in file viewer for some file types:   Images  Music   If a file can be opened in Guild View, its name will appear as a light\ngrey button. When you click the button, Guild will open a file viewer.   Media file names have have grey buttons, which can be clicked to\n  view the file.  When you re viewing files in Guild View, you can navigate through them\nusing  NEXT  and  PREV  buttons.   Use Guild View to view generated artifacts like images and music", 
            "title": "Viewing run files"
        }, 
        {
            "location": "/docs/visual/guild-view/#view-runs-in-tensorboard", 
            "text": "Guild View provides integration with TensorBoard. To view TensorFlow\nevent logs, click   in the upper left of\nthe window.   Important  If you re running Guild View on a remote server, the  View in\nTensorBoard  feature will work. This is a known issue and will be\nfixed in upcoming releases of Guild AI. To view runs in\nTensorBoard on a remote server, use the tensorboard  command from the remote server\ninstead of Guild View.", 
            "title": "View runs in TensorBoard"
        }, 
        {
            "location": "/docs/visual/guild-view/#automatic-updates", 
            "text": "Guild View and TensorBoard will automatically update to show the\nlatest runs and run files.", 
            "title": "Automatic updates"
        }, 
        {
            "location": "/docs/visual/guild-view/#stopping-guild-view", 
            "text": "Guild View will run until it s stopped. To stop Guild View, type CTRL-C  in the console window where Guild View is running.   Note  When you ve stopped Guild View, the associated browser window will\nno longer update or respond.", 
            "title": "Stopping Guild View"
        }, 
        {
            "location": "/docs/visual/tensorboard/", 
            "text": "TensorBoard\n\n\n\n\nTensorBoard\n\n\nTensorBoard is a visualization tool from the TensorFlow team. It\ns a\nweb based application that lets you view TensorFlow event logs, which\ncontain a variety of useful information associated with a run:\n\n\n\n\nMetrics (scalars)\n\n\nImages, audio, and text generated during training\n\n\nModel graph\n\n\nModel statistics\n\n\n\n\nFor more information, see \nTensorBoard: Visualizing\nLearning\n.\n\n\nTensorBoard and Guild\n\n\nGuild integrates TensorBoard to make it easy to visualize TensorFlow\nevent logs. To visualize events for a set of runs, you can launch\nTensorBoard by running:\n\n\nguild tensorboard\n\n\n\n\nFor more information, see \ntensorboard\ncommand\n.\n\n\nTensorBoard is also integrated with \nGuild\nView\n. You can launch TensorBoard from Guild\nView by clicking \n which is located in\nthe upper left of the screen.\n\n\nRun synchronization\n\n\nWhen you run TensorBoard from Guild, by either the \ntensorboard\n\ncommand or from Guild View, the list of runs is automatically\nsynchronized with the current \nrun view\n.", 
            "title": "TensorBoard"
        }, 
        {
            "location": "/docs/visual/tensorboard/#tensorboard", 
            "text": "TensorBoard  TensorBoard is a visualization tool from the TensorFlow team. It s a\nweb based application that lets you view TensorFlow event logs, which\ncontain a variety of useful information associated with a run:   Metrics (scalars)  Images, audio, and text generated during training  Model graph  Model statistics   For more information, see  TensorBoard: Visualizing\nLearning .", 
            "title": "TensorBoard"
        }, 
        {
            "location": "/docs/visual/tensorboard/#tensorboard-and-guild", 
            "text": "Guild integrates TensorBoard to make it easy to visualize TensorFlow\nevent logs. To visualize events for a set of runs, you can launch\nTensorBoard by running:  guild tensorboard  For more information, see  tensorboard\ncommand .  TensorBoard is also integrated with  Guild\nView . You can launch TensorBoard from Guild\nView by clicking   which is located in\nthe upper left of the screen.", 
            "title": "TensorBoard and Guild"
        }, 
        {
            "location": "/docs/visual/tensorboard/#run-synchronization", 
            "text": "When you run TensorBoard from Guild, by either the  tensorboard \ncommand or from Guild View, the list of runs is automatically\nsynchronized with the current  run view .", 
            "title": "Run synchronization"
        }, 
        {
            "location": "/docs/commands/", 
            "text": "Commands\n\n\n\n\nGeneral\n\n\nModel support\n\n\nRun support\n\n\nPackage support\n\n\nVisualization\n\n\nUtilities\n\n\nRunning commands\n\n\nCommand options\n\n\nRunning commands in a separate console\n\n\n\n\n\n\nCommand help\n\n\n\n\n\n\n\n\nGeneral\n\n\ncheck\nhelp\ninit\n\n\n\n\nModel support\n\n\nhelp\nmodels\noperations\n\n\n\n\nRun support\n\n\ncompare\nlabel\nrun\nruns\nruns delete\nruns info\nruns label\nruns list\nruns purge\nruns restore\nruns stop\nstop\nsync\n\n\n\n\nPackage support\n\n\ninstall\npackage\npackages\npackages delete\npackages info\npackages list\nresources\nsearch\nuninstall\n\n\n\n\nVisualization\n\n\ntensorboard\nview\n\n\n\n\nUtilities\n\n\nindex\ntensorflow inspect\n\n\n\n\nRunning commands\n\n\nGuild commands must be executed on a command line. If you\nre\nunfamiliar with running commands on your system, refer to \nGetting to\nKnow the Command Line\n for a\nprimer.\n\n\nAt a command line, run a Guild command using this convention:\n\n\nguild COMMAND\n\n\n\n\nwhere \nCOMMAND\n is one of the commands above. For a list of commands,\nrefer to this page or run \nguild --help\n on the command line. For\nmore information on getting see \nCommand help\n below.\n\n\nAll Guild commands run in the foreground and terminate when the\ncommand succeeds or an error occurs. You can stop a command at any\ntime by typing \nCTRL-c\n (i.e. hold down the \ncontrol\nkey\n for your system and press \nc\n).\n\n\nCommand options\n\n\nCommands accept \noptions\n, which may be provided as command line\narguments in the format \n--OPTION [VALUE]\n or \n-OPTION_CHAR\n[VALUE]\n where \nOPTION\n is the full name of the option and\n\nOPTION_CHAR\n is the single character option shortcut. \nVALUE\n may be\nrequired, optional, or considered invalid depending on the specific\ncommand option.\n\n\nOptions for each command are printed when you run \nguild COMMAND\n--help\n. They are also listed and described in more details in this\nguide.\n\n\nRunning commands in a separate console\n\n\nThere are some commands that will not terminate until you explicitly\nstop them:\n\n\n\n\ncompare\n\n\nview\n\n\ntensorboard\n\n\n\n\nTo run another command while one of these is still running, run the\nnew command in a separate console. There are various strategies for\nmanaging separate consoles:\n\n\n\n\nOpen another console/terminal\n\n\nUse a console/terminal application that supports multiple tabs\n\n\nUse an integrated developer environment (IDE) that supports running\n  commands in different terminals\n\n\nUse a multiplexer like \ntmux\n\n  (advanced)\n\n\n\n\nCommand help\n\n\nThe Guild CLI provides two levels of help:\n\n\n\n\nGeneral help\n\n\nCommand specific help\n\n\n\n\nGeneral help is available by running:\n\n\nguild --help\n\n\n\n\nThis will print Guild\ns global options as well as available commands.\n\n\nGlobal options may be specified for any command but must be specified\nbefore the command.\n\n\nCommand help is available by running:\n\n\nguild COMMAND --help\n\n\n\n\nThis will print details about what the command does and how it can be\nconfigured including details about its options.", 
            "title": "Commands"
        }, 
        {
            "location": "/docs/commands/#commands", 
            "text": "General  Model support  Run support  Package support  Visualization  Utilities  Running commands  Command options  Running commands in a separate console    Command help", 
            "title": "Commands"
        }, 
        {
            "location": "/docs/commands/#general", 
            "text": "check help init", 
            "title": "General"
        }, 
        {
            "location": "/docs/commands/#model-support", 
            "text": "help models operations", 
            "title": "Model support"
        }, 
        {
            "location": "/docs/commands/#run-support", 
            "text": "compare label run runs runs delete runs info runs label runs list runs purge runs restore runs stop stop sync", 
            "title": "Run support"
        }, 
        {
            "location": "/docs/commands/#package-support", 
            "text": "install package packages packages delete packages info packages list resources search uninstall", 
            "title": "Package support"
        }, 
        {
            "location": "/docs/commands/#visualization", 
            "text": "tensorboard view", 
            "title": "Visualization"
        }, 
        {
            "location": "/docs/commands/#utilities", 
            "text": "index tensorflow inspect", 
            "title": "Utilities"
        }, 
        {
            "location": "/docs/commands/#running-commands", 
            "text": "Guild commands must be executed on a command line. If you re\nunfamiliar with running commands on your system, refer to  Getting to\nKnow the Command Line  for a\nprimer.  At a command line, run a Guild command using this convention:  guild COMMAND  where  COMMAND  is one of the commands above. For a list of commands,\nrefer to this page or run  guild --help  on the command line. For\nmore information on getting see  Command help  below.  All Guild commands run in the foreground and terminate when the\ncommand succeeds or an error occurs. You can stop a command at any\ntime by typing  CTRL-c  (i.e. hold down the  control\nkey  for your system and press  c ).", 
            "title": "Running commands"
        }, 
        {
            "location": "/docs/commands/#command-options", 
            "text": "Commands accept  options , which may be provided as command line\narguments in the format  --OPTION [VALUE]  or  -OPTION_CHAR\n[VALUE]  where  OPTION  is the full name of the option and OPTION_CHAR  is the single character option shortcut.  VALUE  may be\nrequired, optional, or considered invalid depending on the specific\ncommand option.  Options for each command are printed when you run  guild COMMAND\n--help . They are also listed and described in more details in this\nguide.", 
            "title": "Command options"
        }, 
        {
            "location": "/docs/commands/#running-commands-in-a-separate-console", 
            "text": "There are some commands that will not terminate until you explicitly\nstop them:   compare  view  tensorboard   To run another command while one of these is still running, run the\nnew command in a separate console. There are various strategies for\nmanaging separate consoles:   Open another console/terminal  Use a console/terminal application that supports multiple tabs  Use an integrated developer environment (IDE) that supports running\n  commands in different terminals  Use a multiplexer like  tmux \n  (advanced)", 
            "title": "Running commands in a separate console"
        }, 
        {
            "location": "/docs/commands/#command-help", 
            "text": "The Guild CLI provides two levels of help:   General help  Command specific help   General help is available by running:  guild --help  This will print Guild s global options as well as available commands.  Global options may be specified for any command but must be specified\nbefore the command.  Command help is available by running:  guild COMMAND --help  This will print details about what the command does and how it can be\nconfigured including details about its options.", 
            "title": "Command help"
        }, 
        {
            "location": "/docs/commands/check-cmd/", 
            "text": "check command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild check [OPTIONS]\n\n  \n\n    \nCheck the Guild setup.\n\n\nThis command performs a number of checks and prints information\nabout the Guild setup.\n\n\nYou can also run the Guild test suite by specifying the \n--tests\n\noption.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-T, --tests\n\n        \nRun Guild test suite.\n\n      \n\n      \n      \n\n        \n-t, --test TEST\n\n        \nRun \nTEST\n (may be used multiple times).\n\n      \n\n      \n      \n\n        \n-n, --no-info\n\n        \nDon't print info (useful when just running tests).\n\n      \n\n      \n      \n\n        \n-s, --skip TEST\n\n        \nSkip \nTEST\n when running Guild test suite. Ignored otherwise.\n\n      \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow more information.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nCheck remote environment.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "check command"
        }, 
        {
            "location": "/docs/commands/check-cmd/#check-command", 
            "text": "Usage  Options", 
            "title": "check command"
        }, 
        {
            "location": "/docs/commands/check-cmd/#usage", 
            "text": "guild check [OPTIONS] \n   \n     Check the Guild setup.  This command performs a number of checks and prints information\nabout the Guild setup.  You can also run the Guild test suite by specifying the  --tests \noption.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/check-cmd/#options", 
            "text": "-T, --tests \n         Run Guild test suite. \n       \n      \n       \n         -t, --test TEST \n         Run  TEST  (may be used multiple times). \n       \n      \n       \n         -n, --no-info \n         Don't print info (useful when just running tests). \n       \n      \n       \n         -s, --skip TEST \n         Skip  TEST  when running Guild test suite. Ignored otherwise. \n       \n      \n       \n         -v, --verbose \n         Show more information. \n       \n      \n       \n         -r, --remote REMOTE \n         Check remote environment. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/compare-cmd/", 
            "text": "compare command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild compare [OPTIONS] [RUN...]\n\n  \n\n    \nCompare run results.\n\n\nGuild Compare is a console based application that displays a table\nof runs with their current accuracy and loss. The application will\ncontinue to run until you exit it by pressing \nq\n (for quit).\n\n\nGuild Compare supports a number of commands. Commands are\nactivated by pressing a letter. To view the list of commands,\npress \n?\n.\n\n\nGuild Compare does not automatically update to display the latest\navailable data. If you want to update the list of runs and their\nstatus, press \nr\n (for refresh).\n\n\nYou may alternative use this command to generate CSV output for\nrun. Use the \n--csv\n option to print data to standard output\ninstead of running as an application. You can redirect this output\nto a file using:\n\n\nguild compare --csv \n RUNS.csv\n\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-t, --table\n\n        \nGenerate comparison data as a table.\n\n      \n\n      \n      \n\n        \n-c, --csv\n\n        \nGenerate comparison data as a CSV file.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "compare command"
        }, 
        {
            "location": "/docs/commands/compare-cmd/#compare-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status    Options", 
            "title": "compare command"
        }, 
        {
            "location": "/docs/commands/compare-cmd/#usage", 
            "text": "guild compare [OPTIONS] [RUN...] \n   \n     Compare run results.  Guild Compare is a console based application that displays a table\nof runs with their current accuracy and loss. The application will\ncontinue to run until you exit it by pressing  q  (for quit).  Guild Compare supports a number of commands. Commands are\nactivated by pressing a letter. To view the list of commands,\npress  ? .  Guild Compare does not automatically update to display the latest\navailable data. If you want to update the list of runs and their\nstatus, press  r  (for refresh).  You may alternative use this command to generate CSV output for\nrun. Use the  --csv  option to print data to standard output\ninstead of running as an application. You can redirect this output\nto a file using:  guild compare --csv   RUNS.csv", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/compare-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/compare-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/compare-cmd/#filtering-by-run-status", 
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.", 
            "title": "Filtering by run status"
        }, 
        {
            "location": "/docs/commands/compare-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -t, --table \n         Generate comparison data as a table. \n       \n      \n       \n         -c, --csv \n         Generate comparison data as a CSV file. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/help-cmd/", 
            "text": "help command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild help [OPTIONS] [PATH_OR_PACKAGE]\n\n  \n\n    \nShow help for a path or package.\n\n\nBy default shows information about the models defined in the\nproject.\n\n\nTo display the description for distributions generated using the\npackage command, specify the \n--package-description\n option.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n--package-description\n\n        \nShow the package description.\n\n      \n\n      \n      \n\n        \n-n, --no-pager\n\n        \nDo not use a pager when showing help.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "help command"
        }, 
        {
            "location": "/docs/commands/help-cmd/#help-command", 
            "text": "Usage  Options", 
            "title": "help command"
        }, 
        {
            "location": "/docs/commands/help-cmd/#usage", 
            "text": "guild help [OPTIONS] [PATH_OR_PACKAGE] \n   \n     Show help for a path or package.  By default shows information about the models defined in the\nproject.  To display the description for distributions generated using the\npackage command, specify the  --package-description  option.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/help-cmd/#options", 
            "text": "--package-description \n         Show the package description. \n       \n      \n       \n         -n, --no-pager \n         Do not use a pager when showing help. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/index-cmd/", 
            "text": "index command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild index [OPTIONS]\n\n  \n\n    \nMange the run index.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-s, --sync\n\n        \nSynchronize index with current runs.\n\n      \n\n      \n      \n\n        \n-r, --raw-fields\n\n        \nShow raw (unformatted) index fields as JSON.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "index command"
        }, 
        {
            "location": "/docs/commands/index-cmd/#index-command", 
            "text": "Usage  Options", 
            "title": "index command"
        }, 
        {
            "location": "/docs/commands/index-cmd/#usage", 
            "text": "guild index [OPTIONS] \n   \n     Mange the run index.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/index-cmd/#options", 
            "text": "-s, --sync \n         Synchronize index with current runs. \n       \n      \n       \n         -r, --raw-fields \n         Show raw (unformatted) index fields as JSON. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/init-cmd/", 
            "text": "init command\n\n\n\n\nUsage\n\n\nRequirements\n\n\nTensorFlow\n\n\nGuild AI\n\n\nResource cache\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild init [OPTIONS] [DIR]\n\n  \n\n    \nInitialize a Guild environment.\n\n\ninit\n initializes a Guild environment in \nDIR\n, which is the\ncurrent directory by default.\n\n\ninit\n creates a virtual environment in \nDIR\n using \nvirtualenv\n.\n\n\nUse \n--python\n to specify the Python interpreter to use within the\ngenerated virtual environment. If \nno-venv\n is specified,\n\n--python\n is ignored.\n\n\nRequirements\n\n\nBy default, any required packages listed under packages.requires\nin \nguild.yml\n in the environment parent directory are installed\ninto the environment. Use \n--no-reqs\n to suppress this behavior.\n\n\nAdditionally, packages defined in \nrequirements.txt\n in the\nenvironment parent directory will be installed. Use \n--no-reqs\n to\nsuppress this behavior.\n\n\nNote that packages defined in \nguild.yml\n use Guild package names\nwhile packages defined in \nrequirements.txt\n use PyPI package\nnames.\n\n\nFor information in requirements files, see:\n\n\nhttps://pip.readthedocs.io/en/1.1/requirements.html\n\n\nYou may explicitly specify requirements file using \n-r\n or\n\n--requirement\n. If \n-r, --requirement\n is specified, Guild will\nnot automatically install packages in \nrequirements.txt\n -- that\nfile must be specified explicitly in the command.\n\n\nTensorFlow\n\n\nBy default \ninit\n installs TensorFlow in the initialized\nenvironment if it's not already installed. When Guild installs\nTensorFlow, it detects GPU support on the system and selects the\nappropriate package: \ntensorflow-gpu\n for GPU support, otherwise\n\ntensorflow\n.\n\n\nTo override the default package, use \n--gpu\n to install the\n\ntensorflow-gpu\n package or \n--no-gpu\n to install the \ntensorflow\n\npackage.\n\n\nTo skip installing TensorFlow, use \n--no-tensorflow\n.\n\n\nIf TensorFlow was installed by way of a requirements file, either\n\nrequirements.txt\n located in the environment parent directory or\na file specified by a \n--requirement\n option, Guild will not\nreinstall it.\n\n\nGuild AI\n\n\nBy default \ninit\n installs the active version of Guild AI in the\ninitialized environment. To install a different version, or to\ninstall a Guild wheel distribution file use the \n--guild\n option.\n\n\nResource cache\n\n\nBy default resources are cached and shared at the user level in\n\n~/.guild/cache/resources\n so that resources downloaded from one\nenvironment are available to other environments. You can modify\nthis behavior to have all resources downloaded local to the\nenvironment by specifying \n--local-resource-cache\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-n, --name NAME\n\n        \nEnvironment name (default is env parent directory name).\n\n      \n\n      \n      \n\n        \n--python VERSION\n\n        \nVersion of Python to use for the environment.\n\n      \n\n      \n      \n\n        \n--guild VERSION_OR_PATH\n\n        \nVersion of Guild AI to use for the environment. By default, the active version of Guild is installed. This value may alternatively be a path to a Guild wheel distribution.\n\n      \n\n      \n      \n\n        \n--gpu\n\n        \nUse the GPU enabled tensorflow package for the environment.\n\n      \n\n      \n      \n\n        \n--no-gpu\n\n        \nUse the non-GPU tensorflow package for the environment.\n\n      \n\n      \n      \n\n        \n--no-tensorflow\n\n        \nDo not install TensorFlow in the environment.\n\n      \n\n      \n      \n\n        \n-r, --requirement FILE\n\n        \nInstall packages defined in FILE. May be used multiple times.\n\n      \n\n      \n      \n\n        \n-p, --path DIR\n\n        \nInclude DIR as a Python path in the environment.\n\n      \n\n      \n      \n\n        \n--no-reqs\n\n        \nDon't install from requirements.txt or guild.yml in environment parent directory.\n\n      \n\n      \n      \n\n        \n--local-resource-cache\n\n        \nUse a local cache when initializing an environment.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nInitialize a Guild environment without prompting.\n\n      \n\n      \n      \n\n        \n--no-progress\n\n        \nDon't show progress when installing environment packages.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "init command"
        }, 
        {
            "location": "/docs/commands/init-cmd/#init-command", 
            "text": "Usage  Requirements  TensorFlow  Guild AI  Resource cache    Options", 
            "title": "init command"
        }, 
        {
            "location": "/docs/commands/init-cmd/#usage", 
            "text": "guild init [OPTIONS] [DIR] \n   \n     Initialize a Guild environment.  init  initializes a Guild environment in  DIR , which is the\ncurrent directory by default.  init  creates a virtual environment in  DIR  using  virtualenv .  Use  --python  to specify the Python interpreter to use within the\ngenerated virtual environment. If  no-venv  is specified, --python  is ignored.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/init-cmd/#requirements", 
            "text": "By default, any required packages listed under packages.requires\nin  guild.yml  in the environment parent directory are installed\ninto the environment. Use  --no-reqs  to suppress this behavior.  Additionally, packages defined in  requirements.txt  in the\nenvironment parent directory will be installed. Use  --no-reqs  to\nsuppress this behavior.  Note that packages defined in  guild.yml  use Guild package names\nwhile packages defined in  requirements.txt  use PyPI package\nnames.  For information in requirements files, see:  https://pip.readthedocs.io/en/1.1/requirements.html  You may explicitly specify requirements file using  -r  or --requirement . If  -r, --requirement  is specified, Guild will\nnot automatically install packages in  requirements.txt  -- that\nfile must be specified explicitly in the command.", 
            "title": "Requirements"
        }, 
        {
            "location": "/docs/commands/init-cmd/#tensorflow", 
            "text": "By default  init  installs TensorFlow in the initialized\nenvironment if it's not already installed. When Guild installs\nTensorFlow, it detects GPU support on the system and selects the\nappropriate package:  tensorflow-gpu  for GPU support, otherwise tensorflow .  To override the default package, use  --gpu  to install the tensorflow-gpu  package or  --no-gpu  to install the  tensorflow \npackage.  To skip installing TensorFlow, use  --no-tensorflow .  If TensorFlow was installed by way of a requirements file, either requirements.txt  located in the environment parent directory or\na file specified by a  --requirement  option, Guild will not\nreinstall it.", 
            "title": "TensorFlow"
        }, 
        {
            "location": "/docs/commands/init-cmd/#guild-ai", 
            "text": "By default  init  installs the active version of Guild AI in the\ninitialized environment. To install a different version, or to\ninstall a Guild wheel distribution file use the  --guild  option.", 
            "title": "Guild AI"
        }, 
        {
            "location": "/docs/commands/init-cmd/#resource-cache", 
            "text": "By default resources are cached and shared at the user level in ~/.guild/cache/resources  so that resources downloaded from one\nenvironment are available to other environments. You can modify\nthis behavior to have all resources downloaded local to the\nenvironment by specifying  --local-resource-cache .", 
            "title": "Resource cache"
        }, 
        {
            "location": "/docs/commands/init-cmd/#options", 
            "text": "-n, --name NAME \n         Environment name (default is env parent directory name). \n       \n      \n       \n         --python VERSION \n         Version of Python to use for the environment. \n       \n      \n       \n         --guild VERSION_OR_PATH \n         Version of Guild AI to use for the environment. By default, the active version of Guild is installed. This value may alternatively be a path to a Guild wheel distribution. \n       \n      \n       \n         --gpu \n         Use the GPU enabled tensorflow package for the environment. \n       \n      \n       \n         --no-gpu \n         Use the non-GPU tensorflow package for the environment. \n       \n      \n       \n         --no-tensorflow \n         Do not install TensorFlow in the environment. \n       \n      \n       \n         -r, --requirement FILE \n         Install packages defined in FILE. May be used multiple times. \n       \n      \n       \n         -p, --path DIR \n         Include DIR as a Python path in the environment. \n       \n      \n       \n         --no-reqs \n         Don't install from requirements.txt or guild.yml in environment parent directory. \n       \n      \n       \n         --local-resource-cache \n         Use a local cache when initializing an environment. \n       \n      \n       \n         -y, --yes \n         Initialize a Guild environment without prompting. \n       \n      \n       \n         --no-progress \n         Don't show progress when installing environment packages. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/install-cmd/", 
            "text": "install command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild install [OPTIONS] PACKAGE...\n\n  \n\n    \nInstall one or more packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-U, --upgrade\n\n        \nUpgrade specified packages to the newest available version.\n\n      \n\n      \n      \n\n        \n--reinstall\n\n        \nResinstall the package if it's already installed. Implies --upgrade.\n\n      \n\n      \n      \n\n        \n--no-cache\n\n        \nDon't use cached packages.\n\n      \n\n      \n      \n\n        \n--no-deps\n\n        \nDon't install dependencies.\n\n      \n\n      \n      \n\n        \n--pre\n\n        \nInstall pre-release versions.\n\n      \n\n      \n      \n\n        \n--target DIR\n\n        \nInstall package and requirements in DIR.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "install command"
        }, 
        {
            "location": "/docs/commands/install-cmd/#install-command", 
            "text": "Usage  Options", 
            "title": "install command"
        }, 
        {
            "location": "/docs/commands/install-cmd/#usage", 
            "text": "guild install [OPTIONS] PACKAGE... \n   \n     Install one or more packages.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/install-cmd/#options", 
            "text": "-U, --upgrade \n         Upgrade specified packages to the newest available version. \n       \n      \n       \n         --reinstall \n         Resinstall the package if it's already installed. Implies --upgrade. \n       \n      \n       \n         --no-cache \n         Don't use cached packages. \n       \n      \n       \n         --no-deps \n         Don't install dependencies. \n       \n      \n       \n         --pre \n         Install pre-release versions. \n       \n      \n       \n         --target DIR \n         Install package and requirements in DIR. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/label-cmd/", 
            "text": "label command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\nLabeling remote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild label [OPTIONS] [RUN...] [LABEL]\n\n  \n\n    \nSet run labels.\n\n\nIf \nLABEL\n is provided, the command will label the selected\nruns. To clear a run label, use the \n--clear\n option.\n\n\nSelect runs to modify using one or more \nRUN\n arguments. See\nSELECT RUNS below for information on selecting runs.\n\n\nIf \nRUN\n isn't specified, the most recent run is selected.\n\n\nBy default Guild will prompt you before making any changes. If you\nwant to apply the changes without being prompted, use the\n\n--yes\n option.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n0\n is assumed (the most\nrecent run).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n\nLabeling remote runs\n\n\nTo label remote runs, use \n--remote\n.\n\n\nREMOTE\n is the name of a configured remote. Use \nguild remotes\n\nto list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-c, --clear\n\n        \nClear the run's label.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nLabel remote runs.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before modifying labels.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "label command"
        }, 
        {
            "location": "/docs/commands/label-cmd/#label-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status  Labeling remote runs    Options", 
            "title": "label command"
        }, 
        {
            "location": "/docs/commands/label-cmd/#usage", 
            "text": "guild label [OPTIONS] [RUN...] [LABEL] \n   \n     Set run labels.  If  LABEL  is provided, the command will label the selected\nruns. To clear a run label, use the  --clear  option.  Select runs to modify using one or more  RUN  arguments. See\nSELECT RUNS below for information on selecting runs.  If  RUN  isn't specified, the most recent run is selected.  By default Guild will prompt you before making any changes. If you\nwant to apply the changes without being prompted, use the --yes  option.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/label-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  0  is assumed (the most\nrecent run).", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/label-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/label-cmd/#filtering-by-run-status", 
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.", 
            "title": "Filtering by run status"
        }, 
        {
            "location": "/docs/commands/label-cmd/#labeling-remote-runs", 
            "text": "To label remote runs, use  --remote .  REMOTE  is the name of a configured remote. Use  guild remotes \nto list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Labeling remote runs"
        }, 
        {
            "location": "/docs/commands/label-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -c, --clear \n         Clear the run's label. \n       \n      \n       \n         -r, --remote REMOTE \n         Label remote runs. \n       \n      \n       \n         -y, --yes \n         Do not prompt before modifying labels. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/models-cmd/", 
            "text": "models command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild models [OPTIONS] [FILTER]...\n\n  \n\n    \nShow available models.\n\n\nUse one or more \nFILTER\n arguments to show only models that match\nthe specified values.\n\n\nFILTER\n may a directory to indicate that only models defined in\nthat location are included in the list.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow model details.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "models command"
        }, 
        {
            "location": "/docs/commands/models-cmd/#models-command", 
            "text": "Usage  Options", 
            "title": "models command"
        }, 
        {
            "location": "/docs/commands/models-cmd/#usage", 
            "text": "guild models [OPTIONS] [FILTER]... \n   \n     Show available models.  Use one or more  FILTER  arguments to show only models that match\nthe specified values.  FILTER  may a directory to indicate that only models defined in\nthat location are included in the list.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/models-cmd/#options", 
            "text": "-v, --verbose \n         Show model details. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/operations-cmd/", 
            "text": "operations command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild operations [OPTIONS] [FILTER]...\n\n  \n\n    \nShow model operations.\n\n\nUse one or more \nFILTER\n arguments to show only operations whose\nnames or models match the specified values.\n\n\nFILTER\n may a directory to indicate that only operations of\nmodels defined in that location are included in the list.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow operation details.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "operations command"
        }, 
        {
            "location": "/docs/commands/operations-cmd/#operations-command", 
            "text": "Usage  Options", 
            "title": "operations command"
        }, 
        {
            "location": "/docs/commands/operations-cmd/#usage", 
            "text": "guild operations [OPTIONS] [FILTER]... \n   \n     Show model operations.  Use one or more  FILTER  arguments to show only operations whose\nnames or models match the specified values.  FILTER  may a directory to indicate that only operations of\nmodels defined in that location are included in the list.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/operations-cmd/#options", 
            "text": "-v, --verbose \n         Show operation details. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/package-cmd/", 
            "text": "package command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild package [OPTIONS]\n\n  \n\n    \nCreate a package for distribution.\n\n\nPackages are built from projects that contain guildfile with a\npackage definition, which describes the package to be built.\n\n\nYou may upload the generated package distribution to a PyPI\nrepository by using the \n--upload\n option or to the PyPI test site\nby using \n--upload-test\n.\n\n\nYou may upload to an alternative repository using\n\n--upload-repo\n. \nREPO\n may be a URL or the name of a section in\n\n~/.pypirc\n. For more information on the \n.pypirc\n file, see:\n\n\nhttps://docs.python.org/2/distutils/packageindex.html#pypirc\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-d, --dist-dir DIR\n\n        \nDirectory to create the package distribution in.\n\n      \n\n      \n      \n\n        \n--upload\n\n        \nUpload to PyPI after creating the package.\n\n      \n\n      \n      \n\n        \n--upload-test\n\n        \nUpload to the PyPI test site after creating the package.\n\n      \n\n      \n      \n\n        \n--repo REPO\n\n        \nUpload to \nREPO\n after creating the package.\n\n      \n\n      \n      \n\n        \n-s, --sign\n\n        \nSign a package distribution upload with gpg.\n\n      \n\n      \n      \n\n        \n-i, --identity IDENTITY\n\n        \nGPG identity used to sign upload.\n\n      \n\n      \n      \n\n        \n-u, --user USERNAME\n\n        \nPyPI user name for upload.\n\n      \n\n      \n      \n\n        \n-p, --password PASSWORD\n\n        \nPyPI password for upload.\n\n      \n\n      \n      \n\n        \n-e, --skip-existing\n\n        \nDon't upload if package already exists.\n\n      \n\n      \n      \n\n        \n-c, --comment COMMENT\n\n        \nComment to include with upload.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "package command"
        }, 
        {
            "location": "/docs/commands/package-cmd/#package-command", 
            "text": "Usage  Options", 
            "title": "package command"
        }, 
        {
            "location": "/docs/commands/package-cmd/#usage", 
            "text": "guild package [OPTIONS] \n   \n     Create a package for distribution.  Packages are built from projects that contain guildfile with a\npackage definition, which describes the package to be built.  You may upload the generated package distribution to a PyPI\nrepository by using the  --upload  option or to the PyPI test site\nby using  --upload-test .  You may upload to an alternative repository using --upload-repo .  REPO  may be a URL or the name of a section in ~/.pypirc . For more information on the  .pypirc  file, see:  https://docs.python.org/2/distutils/packageindex.html#pypirc", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/package-cmd/#options", 
            "text": "-d, --dist-dir DIR \n         Directory to create the package distribution in. \n       \n      \n       \n         --upload \n         Upload to PyPI after creating the package. \n       \n      \n       \n         --upload-test \n         Upload to the PyPI test site after creating the package. \n       \n      \n       \n         --repo REPO \n         Upload to  REPO  after creating the package. \n       \n      \n       \n         -s, --sign \n         Sign a package distribution upload with gpg. \n       \n      \n       \n         -i, --identity IDENTITY \n         GPG identity used to sign upload. \n       \n      \n       \n         -u, --user USERNAME \n         PyPI user name for upload. \n       \n      \n       \n         -p, --password PASSWORD \n         PyPI password for upload. \n       \n      \n       \n         -e, --skip-existing \n         Don't upload if package already exists. \n       \n      \n       \n         -c, --comment COMMENT \n         Comment to include with upload. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/packages-cmd/", 
            "text": "packages command\n\n\n\n\nUsage\n\n\nOptions\n\n\nSubcommands\n\n\n\n\n\n  \nUsage\n\n  \nguild packages [OPTIONS] COMMAND [ARGS]...\n\n  \n\n    \nShow or manage packages.\n\n\nIf \nCOMMAND\n is not specified, lists packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-a, --all\n\n        \nShow all packages.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nSubcommands\n\n  \n\n    \n\n      \n      \n\n        \ndelete, rm\n\n        \n...\n\n      \n\n      \n      \n\n        \ninfo\n\n        \n...\n\n      \n\n      \n      \n\n        \nlist, ls\n\n        \n...\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "packages command"
        }, 
        {
            "location": "/docs/commands/packages-cmd/#packages-command", 
            "text": "Usage  Options  Subcommands", 
            "title": "packages command"
        }, 
        {
            "location": "/docs/commands/packages-cmd/#usage", 
            "text": "guild packages [OPTIONS] COMMAND [ARGS]... \n   \n     Show or manage packages.  If  COMMAND  is not specified, lists packages.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/packages-cmd/#options", 
            "text": "-a, --all \n         Show all packages. \n       \n      \n       \n         --help \n         Show command help and exit.", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/packages-cmd/#subcommands", 
            "text": "delete, rm \n         ... \n       \n      \n       \n         info \n         ... \n       \n      \n       \n         list, ls \n         ... \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Subcommands"
        }, 
        {
            "location": "/docs/commands/packages-delete-cmd/", 
            "text": "packages delete command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild packages delete [OPTIONS] PACKAGE...\n\n  \n\n    \nUninstall one or more packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before uninstalling.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "packages delete command"
        }, 
        {
            "location": "/docs/commands/packages-delete-cmd/#packages-delete-command", 
            "text": "Usage  Options", 
            "title": "packages delete command"
        }, 
        {
            "location": "/docs/commands/packages-delete-cmd/#usage", 
            "text": "guild packages delete [OPTIONS] PACKAGE... \n   \n     Uninstall one or more packages.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/packages-delete-cmd/#options", 
            "text": "-y, --yes \n         Do not prompt before uninstalling. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/packages-info-cmd/", 
            "text": "packages info command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild packages info [OPTIONS] PACKAGE...\n\n  \n\n    \nShow information for one or more packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow more information.\n\n      \n\n      \n      \n\n        \n-f, --files\n\n        \nShow packages files.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "packages info command"
        }, 
        {
            "location": "/docs/commands/packages-info-cmd/#packages-info-command", 
            "text": "Usage  Options", 
            "title": "packages info command"
        }, 
        {
            "location": "/docs/commands/packages-info-cmd/#usage", 
            "text": "guild packages info [OPTIONS] PACKAGE... \n   \n     Show information for one or more packages.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/packages-info-cmd/#options", 
            "text": "-v, --verbose \n         Show more information. \n       \n      \n       \n         -f, --files \n         Show packages files. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/packages-list-cmd/", 
            "text": "packages list command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild packages list [OPTIONS] [TERM]...\n\n  \n\n    \nList installed packages.\n\n\nSpecify one or more \nTERM\n arguments to show packages matching any\nof the specified values.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-a, --all\n\n        \nShow all packages.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "packages list command"
        }, 
        {
            "location": "/docs/commands/packages-list-cmd/#packages-list-command", 
            "text": "Usage  Options", 
            "title": "packages list command"
        }, 
        {
            "location": "/docs/commands/packages-list-cmd/#usage", 
            "text": "guild packages list [OPTIONS] [TERM]... \n   \n     List installed packages.  Specify one or more  TERM  arguments to show packages matching any\nof the specified values.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/packages-list-cmd/#options", 
            "text": "-a, --all \n         Show all packages. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/resources-cmd/", 
            "text": "resources command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild resources [OPTIONS] [FILTER]...\n\n  \n\n    \nShow available resources.\n\n\nUse one or more \nFILTER\n arguments to show only resources that\nmatch the specified values.\n\n\nFILTER\n may a directory to indicate that only resources defined\nin that location are included in the list.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow resource details.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "resources command"
        }, 
        {
            "location": "/docs/commands/resources-cmd/#resources-command", 
            "text": "Usage  Options", 
            "title": "resources command"
        }, 
        {
            "location": "/docs/commands/resources-cmd/#usage", 
            "text": "guild resources [OPTIONS] [FILTER]... \n   \n     Show available resources.  Use one or more  FILTER  arguments to show only resources that\nmatch the specified values.  FILTER  may a directory to indicate that only resources defined\nin that location are included in the list.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/resources-cmd/#options", 
            "text": "-v, --verbose \n         Show resource details. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/run-cmd/", 
            "text": "run command\n\n\n\n\nUsage\n\n\nRe-running operations\n\n\nRestarting operations\n\n\nAlternate run directory\n\n\nControlling visible GPU devices\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild run [OPTIONS] [[MODEL:]OPERATION] [ARG...]\n\n  \n\n    \nRun a model operation.\n\n\nBy default Guild will try to run \nOPERATION\n for the default model\ndefined in a project. If a project location is not specified (see\n\n--project\n option below), Guild looks for a project in the\ncurrent directory.\n\n\nIf \nMODEL\n is specified, Guild will use it instead of the default\nmodel defined in a project.\n\n\n[MODEL]:OPERATION\n may be omitted if \n--rerun\n is specified, in\nwhich case the operation used in \nRUN\n will be used.\n\n\nRe-running operations\n\n\nIf \n--rerun\n is specified, the operation and flags used in \nRUN\n\nwill be applied to the new operation. You may add or redefine\nflags in the new operation. You may also use an alternative\noperation, in which case only the flag values from \nRUN\n will be\napplied. \nRUN\n must be a run ID or unique run ID prefix or the\nspecial value \n0\n, which indicates the latest run.\n\n\nRestarting operations\n\n\nIf \n--restart\n is specified, the specified \nRUN\n is restarted\nin-place using its operation and flags. Unlike rerun, restart does\nnot create a new run, but instead reuses the run directory of\n\nRUN\n. Like a rerun, a restart may specify a different operation\nand additional flags and may use \n0\n for the value of \nRUN\n to\nrestart the latest run. \n--run-dir\n may not be used with\n\n--restart\n.\n\n\n--rerun\n and \n--restart\n may not both be used.\n\n\nAlternate run directory\n\n\nTo run an operation outside of Guild's run management facility,\nuse \n--run-dir\n or \n--stage\n to specify an alternative run\ndirectory. These options are useful when developing or debugging\nan operation. Use \n--stage\n to prepare a run directory for an\noperation without running the operation itself. This is useful\nwhen you want to verify dependency resolution and pre-processing\nor manually run an operation in a prepared directory.\n\n\nNOTE:\n Runs started with \n--run-dir\n are not visible to Guild\nand will not appear in run listings.\n\n\nControlling visible GPU devices\n\n\nBy default, operations have access to all available GPU\ndevices. To limit the GPU devices available to a run, use\n\n--gpus\n.\n\n\nFor example, to limit visible GPU devices to \n0\n and \n1\n, run:\n\n\nguild run --gpus 0,1 ...\n\n\n\nTo disable all available GPUs, use \n--no-gpus\n.\n\n\nNOTE:\n \n--gpus\n and \n--no-gpus\n are used to construct the\n\nCUDA_VISIBLE_DEVICES\n environment variable used for the run\nprocess. If \nCUDA_VISIBLE_DEVICES\n is set, using either of these\noptions will cause it to be redefined for the run.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-l, --label LABEL\n\n        \nSet a label for the run.\n\n      \n\n      \n      \n\n        \n-d, --run-dir DIR\n\n        \nUse alternative run directory DIR. Cannot be used with --stage.\n\n      \n\n      \n      \n\n        \n--stage DIR\n\n        \nStage an operation in DIR but do not run. Cannot be used with --run-dir.\n\n      \n\n      \n      \n\n        \n--rerun RUN\n\n        \nUse the operation and flags from RUN. Flags may be added or redefined in this operation. Cannot be used with --restart.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nRun the operation remotely.\n\n      \n\n      \n      \n\n        \n--restart RUN\n\n        \nRestart RUN in-place without creating a new run. Cannot be used with --rerun or --run-dir.\n\n      \n\n      \n      \n\n        \n--disable-plugins LIST\n\n        \nA comma separated list of plugin names to disable. Use 'all' to disable all plugins.\n\n      \n\n      \n      \n\n        \n--gpus DEVICES\n\n        \nLimit availabe GPUs to DEVICES, a comma separated list of device IDs. By default all GPUs are available. Cannot beused with --no-gpus.\n\n      \n\n      \n      \n\n        \n--no-gpus\n\n        \nDisable GPUs for run. Cannot be used with --gpu.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before running operation.\n\n      \n\n      \n      \n\n        \n--background PIDFILE\n\n        \nRun operation in background. PIDFILE must be a path to a file where the background process ID is written.\n\n      \n\n      \n      \n\n        \n-n, --no-wait\n\n        \nDon't wait for a remote operation to complete. Ignored if run is local.\n\n      \n\n      \n      \n\n        \n--set-trace\n\n        \nEnter the Python debugger at the operation entry point.\n\n      \n\n      \n      \n\n        \n-q, --quiet\n\n        \nDo not show output.\n\n      \n\n      \n      \n\n        \n--print-cmd\n\n        \nShow operation command and exit.\n\n      \n\n      \n      \n\n        \n--print-env\n\n        \nShow operation environment and exit.\n\n      \n\n      \n      \n\n        \n--help-model\n\n        \nShow model help and exit.\n\n      \n\n      \n      \n\n        \n--help-op\n\n        \nShow operation help and exit.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "run command"
        }, 
        {
            "location": "/docs/commands/run-cmd/#run-command", 
            "text": "Usage  Re-running operations  Restarting operations  Alternate run directory  Controlling visible GPU devices    Options", 
            "title": "run command"
        }, 
        {
            "location": "/docs/commands/run-cmd/#usage", 
            "text": "guild run [OPTIONS] [[MODEL:]OPERATION] [ARG...] \n   \n     Run a model operation.  By default Guild will try to run  OPERATION  for the default model\ndefined in a project. If a project location is not specified (see --project  option below), Guild looks for a project in the\ncurrent directory.  If  MODEL  is specified, Guild will use it instead of the default\nmodel defined in a project.  [MODEL]:OPERATION  may be omitted if  --rerun  is specified, in\nwhich case the operation used in  RUN  will be used.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/run-cmd/#re-running-operations", 
            "text": "If  --rerun  is specified, the operation and flags used in  RUN \nwill be applied to the new operation. You may add or redefine\nflags in the new operation. You may also use an alternative\noperation, in which case only the flag values from  RUN  will be\napplied.  RUN  must be a run ID or unique run ID prefix or the\nspecial value  0 , which indicates the latest run.", 
            "title": "Re-running operations"
        }, 
        {
            "location": "/docs/commands/run-cmd/#restarting-operations", 
            "text": "If  --restart  is specified, the specified  RUN  is restarted\nin-place using its operation and flags. Unlike rerun, restart does\nnot create a new run, but instead reuses the run directory of RUN . Like a rerun, a restart may specify a different operation\nand additional flags and may use  0  for the value of  RUN  to\nrestart the latest run.  --run-dir  may not be used with --restart .  --rerun  and  --restart  may not both be used.", 
            "title": "Restarting operations"
        }, 
        {
            "location": "/docs/commands/run-cmd/#alternate-run-directory", 
            "text": "To run an operation outside of Guild's run management facility,\nuse  --run-dir  or  --stage  to specify an alternative run\ndirectory. These options are useful when developing or debugging\nan operation. Use  --stage  to prepare a run directory for an\noperation without running the operation itself. This is useful\nwhen you want to verify dependency resolution and pre-processing\nor manually run an operation in a prepared directory.  NOTE:  Runs started with  --run-dir  are not visible to Guild\nand will not appear in run listings.", 
            "title": "Alternate run directory"
        }, 
        {
            "location": "/docs/commands/run-cmd/#controlling-visible-gpu-devices", 
            "text": "By default, operations have access to all available GPU\ndevices. To limit the GPU devices available to a run, use --gpus .  For example, to limit visible GPU devices to  0  and  1 , run:  guild run --gpus 0,1 ...  To disable all available GPUs, use  --no-gpus .  NOTE:   --gpus  and  --no-gpus  are used to construct the CUDA_VISIBLE_DEVICES  environment variable used for the run\nprocess. If  CUDA_VISIBLE_DEVICES  is set, using either of these\noptions will cause it to be redefined for the run.", 
            "title": "Controlling visible GPU devices"
        }, 
        {
            "location": "/docs/commands/run-cmd/#options", 
            "text": "-l, --label LABEL \n         Set a label for the run. \n       \n      \n       \n         -d, --run-dir DIR \n         Use alternative run directory DIR. Cannot be used with --stage. \n       \n      \n       \n         --stage DIR \n         Stage an operation in DIR but do not run. Cannot be used with --run-dir. \n       \n      \n       \n         --rerun RUN \n         Use the operation and flags from RUN. Flags may be added or redefined in this operation. Cannot be used with --restart. \n       \n      \n       \n         -r, --remote REMOTE \n         Run the operation remotely. \n       \n      \n       \n         --restart RUN \n         Restart RUN in-place without creating a new run. Cannot be used with --rerun or --run-dir. \n       \n      \n       \n         --disable-plugins LIST \n         A comma separated list of plugin names to disable. Use 'all' to disable all plugins. \n       \n      \n       \n         --gpus DEVICES \n         Limit availabe GPUs to DEVICES, a comma separated list of device IDs. By default all GPUs are available. Cannot beused with --no-gpus. \n       \n      \n       \n         --no-gpus \n         Disable GPUs for run. Cannot be used with --gpu. \n       \n      \n       \n         -y, --yes \n         Do not prompt before running operation. \n       \n      \n       \n         --background PIDFILE \n         Run operation in background. PIDFILE must be a path to a file where the background process ID is written. \n       \n      \n       \n         -n, --no-wait \n         Don't wait for a remote operation to complete. Ignored if run is local. \n       \n      \n       \n         --set-trace \n         Enter the Python debugger at the operation entry point. \n       \n      \n       \n         -q, --quiet \n         Do not show output. \n       \n      \n       \n         --print-cmd \n         Show operation command and exit. \n       \n      \n       \n         --print-env \n         Show operation environment and exit. \n       \n      \n       \n         --help-model \n         Show model help and exit. \n       \n      \n       \n         --help-op \n         Show operation help and exit. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/runs-cmd/", 
            "text": "runs command\n\n\n\n\nUsage\n\n\nOptions\n\n\nSubcommands\n\n\n\n\n\n  \nUsage\n\n  \nguild runs [OPTIONS] COMMAND [ARGS]...\n\n  \n\n    \nShow or manage runs.\n\n\nIf \nCOMMAND\n is omitted, lists runs. Refer to \nruns list\n for more information on the \nlist\n command.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-d, --deleted\n\n        \nShow deleted runs.\n\n      \n\n      \n      \n\n        \n--archive DIR\n\n        \nShow archived runs in DIR\n\n      \n\n      \n      \n\n        \n-a, --all\n\n        \nShow all runs (by default only the last 20 runs are shown)\n\n      \n\n      \n      \n\n        \n-m, --more\n\n        \nShow 20 more runs. Maybe used multiple times.\n\n      \n\n      \n      \n\n        \n--json\n\n        \nFormat runs as JSON.\n\n      \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow run details.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nList runs on REMOTE rather than local runs.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nSubcommands\n\n  \n\n    \n\n      \n      \n\n        \ndelete, rm\n\n        \n...\n\n      \n\n      \n      \n\n        \nexport\n\n        \n...\n\n      \n\n      \n      \n\n        \nimport\n\n        \n...\n\n      \n\n      \n      \n\n        \ninfo\n\n        \n...\n\n      \n\n      \n      \n\n        \nlabel\n\n        \n...\n\n      \n\n      \n      \n\n        \nlist, ls\n\n        \n...\n\n      \n\n      \n      \n\n        \npull\n\n        \n...\n\n      \n\n      \n      \n\n        \npurge\n\n        \n...\n\n      \n\n      \n      \n\n        \npush\n\n        \n...\n\n      \n\n      \n      \n\n        \nrestore\n\n        \n...\n\n      \n\n      \n      \n\n        \nstop\n\n        \n...\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "runs command"
        }, 
        {
            "location": "/docs/commands/runs-cmd/#runs-command", 
            "text": "Usage  Options  Subcommands", 
            "title": "runs command"
        }, 
        {
            "location": "/docs/commands/runs-cmd/#usage", 
            "text": "guild runs [OPTIONS] COMMAND [ARGS]... \n   \n     Show or manage runs.  If  COMMAND  is omitted, lists runs. Refer to  runs list  for more information on the  list  command.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/runs-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -d, --deleted \n         Show deleted runs. \n       \n      \n       \n         --archive DIR \n         Show archived runs in DIR \n       \n      \n       \n         -a, --all \n         Show all runs (by default only the last 20 runs are shown) \n       \n      \n       \n         -m, --more \n         Show 20 more runs. Maybe used multiple times. \n       \n      \n       \n         --json \n         Format runs as JSON. \n       \n      \n       \n         -v, --verbose \n         Show run details. \n       \n      \n       \n         -r, --remote REMOTE \n         List runs on REMOTE rather than local runs. \n       \n      \n       \n         --help \n         Show command help and exit.", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/runs-cmd/#subcommands", 
            "text": "delete, rm \n         ... \n       \n      \n       \n         export \n         ... \n       \n      \n       \n         import \n         ... \n       \n      \n       \n         info \n         ... \n       \n      \n       \n         label \n         ... \n       \n      \n       \n         list, ls \n         ... \n       \n      \n       \n         pull \n         ... \n       \n      \n       \n         purge \n         ... \n       \n      \n       \n         push \n         ... \n       \n      \n       \n         restore \n         ... \n       \n      \n       \n         stop \n         ... \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Subcommands"
        }, 
        {
            "location": "/docs/commands/runs-delete-cmd/", 
            "text": "runs delete command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\nDeleting remote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs delete [OPTIONS] [RUN...]\n\n  \n\n    \nDelete one or more runs.\n\n\nRuns are deleting by selecting them with \nRUN\n arguments. If a\n\nRUN\n argument is not specified, all runs matching the filter\ncriteria are deleted. See SELECTING RUNS and FILTERING topics\nbelow for more information on how runs are selected.\n\n\nBy default, Guild will display the list of runs to be deleted and\nask you to confirm the operation. If you want to delete the runs\nwithout being prompted, use the \n--yes\n option.\n\n\nWARNING\n: Take care when deleting runs using indexes as the\nruns selected with indexes can change. Review the list of runs\ncarefully before confirming a delete operation.\n\n\nIf a run is still running, Guild will stop it first before\ndeleting it.\n\n\nIf you delete a run by mistake, provided you didn't use the\n\n--permanent\n option, you can restore it using \nguild runs\nrestore\n.\n\n\nIf you want to permanently delete runs, use the \n--permanent\n\noption.\n\n\nWARNING\n: Permanentaly deleted runs cannot be restored.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n\nDeleting remote runs\n\n\nTo delete runs on a remote, use \n--remote\n.\n\n\nREMOTE\n is the name of a configured remote. Use \nguild remotes\n\nto list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nDelete remote runs.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before deleting.\n\n      \n\n      \n      \n\n        \n-p, --permanent\n\n        \nPermanentaly delete runs so they cannot be recovered.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "runs delete command"
        }, 
        {
            "location": "/docs/commands/runs-delete-cmd/#runs-delete-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status  Deleting remote runs    Options", 
            "title": "runs delete command"
        }, 
        {
            "location": "/docs/commands/runs-delete-cmd/#usage", 
            "text": "guild runs delete [OPTIONS] [RUN...] \n   \n     Delete one or more runs.  Runs are deleting by selecting them with  RUN  arguments. If a RUN  argument is not specified, all runs matching the filter\ncriteria are deleted. See SELECTING RUNS and FILTERING topics\nbelow for more information on how runs are selected.  By default, Guild will display the list of runs to be deleted and\nask you to confirm the operation. If you want to delete the runs\nwithout being prompted, use the  --yes  option.  WARNING : Take care when deleting runs using indexes as the\nruns selected with indexes can change. Review the list of runs\ncarefully before confirming a delete operation.  If a run is still running, Guild will stop it first before\ndeleting it.  If you delete a run by mistake, provided you didn't use the --permanent  option, you can restore it using  guild runs\nrestore .  If you want to permanently delete runs, use the  --permanent \noption.  WARNING : Permanentaly deleted runs cannot be restored.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/runs-delete-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/runs-delete-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/runs-delete-cmd/#filtering-by-run-status", 
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.", 
            "title": "Filtering by run status"
        }, 
        {
            "location": "/docs/commands/runs-delete-cmd/#deleting-remote-runs", 
            "text": "To delete runs on a remote, use  --remote .  REMOTE  is the name of a configured remote. Use  guild remotes \nto list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Deleting remote runs"
        }, 
        {
            "location": "/docs/commands/runs-delete-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -r, --remote REMOTE \n         Delete remote runs. \n       \n      \n       \n         -y, --yes \n         Do not prompt before deleting. \n       \n      \n       \n         -p, --permanent \n         Permanentaly delete runs so they cannot be recovered. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/", 
            "text": "runs info command\n\n\n\n\nUsage\n\n\nSelecting a run\n\n\nAdditional information\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\nRemote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs info [OPTIONS] [RUN]\n\n  \n\n    \nShow run information.\n\n\nThis command shows information for a single run.\n\n\nSelecting a run\n\n\nYou may specify a run using a run ID, a run ID prefix, or a\nzero-based index corresponding to a run returned by the list\ncommand.\n\n\nIf RUN isn't specified, the latest run is selected.\n\n\nAdditional information\n\n\nYou can show additional run information by specifying option\nflags. You may use multiple flags to show more information. Refer\nto the options below for what additional information is available.\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n\nRemote runs\n\n\nUse \n--remote\n to show info for a remote run.\n\n\nREMOTE\n is the name of a configured remote. Use \nguild remotes\n\nto list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-f, --files\n\n        \nShow run files. Use twice (-ff) to show full paths.\n\n      \n\n      \n      \n\n        \n-O, --output\n\n        \nShow run output.\n\n      \n\n      \n      \n\n        \n-e, --env\n\n        \nShow run environment.\n\n      \n\n      \n      \n\n        \n-g, --flags\n\n        \nShow run flags.\n\n      \n\n      \n      \n\n        \n-d, --deps\n\n        \nShow resolved dependencies.\n\n      \n\n      \n      \n\n        \n-p, --page-output\n\n        \nShow only run output in a pager.\n\n      \n\n      \n      \n\n        \n-L, --follow-links\n\n        \nFollow links when showing files.\n\n      \n\n      \n      \n\n        \n-a, --all-files\n\n        \nShow all run files including those generated by Guild.\n\n      \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nShow info for remote run.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "runs info command"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/#runs-info-command", 
            "text": "Usage  Selecting a run  Additional information  Filtering by operation and label  Filtering by run status  Remote runs    Options", 
            "title": "runs info command"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/#usage", 
            "text": "guild runs info [OPTIONS] [RUN] \n   \n     Show run information.  This command shows information for a single run.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/#selecting-a-run", 
            "text": "You may specify a run using a run ID, a run ID prefix, or a\nzero-based index corresponding to a run returned by the list\ncommand.  If RUN isn't specified, the latest run is selected.", 
            "title": "Selecting a run"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/#additional-information", 
            "text": "You can show additional run information by specifying option\nflags. You may use multiple flags to show more information. Refer\nto the options below for what additional information is available.", 
            "title": "Additional information"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/#filtering-by-run-status", 
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.", 
            "title": "Filtering by run status"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/#remote-runs", 
            "text": "Use  --remote  to show info for a remote run.  REMOTE  is the name of a configured remote. Use  guild remotes \nto list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Remote runs"
        }, 
        {
            "location": "/docs/commands/runs-info-cmd/#options", 
            "text": "-f, --files \n         Show run files. Use twice (-ff) to show full paths. \n       \n      \n       \n         -O, --output \n         Show run output. \n       \n      \n       \n         -e, --env \n         Show run environment. \n       \n      \n       \n         -g, --flags \n         Show run flags. \n       \n      \n       \n         -d, --deps \n         Show resolved dependencies. \n       \n      \n       \n         -p, --page-output \n         Show only run output in a pager. \n       \n      \n       \n         -L, --follow-links \n         Follow links when showing files. \n       \n      \n       \n         -a, --all-files \n         Show all run files including those generated by Guild. \n       \n      \n       \n         -o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -r, --remote REMOTE \n         Show info for remote run. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/runs-label-cmd/", 
            "text": "runs label command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\nLabeling remote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs label [OPTIONS] [RUN...] [LABEL]\n\n  \n\n    \nSet run labels.\n\n\nIf \nLABEL\n is provided, the command will label the selected\nruns. To clear a run label, use the \n--clear\n option.\n\n\nSelect runs to modify using one or more \nRUN\n arguments. See\nSELECT RUNS below for information on selecting runs.\n\n\nIf \nRUN\n isn't specified, the most recent run is selected.\n\n\nBy default Guild will prompt you before making any changes. If you\nwant to apply the changes without being prompted, use the\n\n--yes\n option.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n0\n is assumed (the most\nrecent run).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n\nLabeling remote runs\n\n\nTo label remote runs, use \n--remote\n.\n\n\nREMOTE\n is the name of a configured remote. Use \nguild remotes\n\nto list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-c, --clear\n\n        \nClear the run's label.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nLabel remote runs.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before modifying labels.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "runs label command"
        }, 
        {
            "location": "/docs/commands/runs-label-cmd/#runs-label-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status  Labeling remote runs    Options", 
            "title": "runs label command"
        }, 
        {
            "location": "/docs/commands/runs-label-cmd/#usage", 
            "text": "guild runs label [OPTIONS] [RUN...] [LABEL] \n   \n     Set run labels.  If  LABEL  is provided, the command will label the selected\nruns. To clear a run label, use the  --clear  option.  Select runs to modify using one or more  RUN  arguments. See\nSELECT RUNS below for information on selecting runs.  If  RUN  isn't specified, the most recent run is selected.  By default Guild will prompt you before making any changes. If you\nwant to apply the changes without being prompted, use the --yes  option.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/runs-label-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  0  is assumed (the most\nrecent run).", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/runs-label-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/runs-label-cmd/#filtering-by-run-status", 
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.", 
            "title": "Filtering by run status"
        }, 
        {
            "location": "/docs/commands/runs-label-cmd/#labeling-remote-runs", 
            "text": "To label remote runs, use  --remote .  REMOTE  is the name of a configured remote. Use  guild remotes \nto list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Labeling remote runs"
        }, 
        {
            "location": "/docs/commands/runs-label-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -c, --clear \n         Clear the run's label. \n       \n      \n       \n         -r, --remote REMOTE \n         Label remote runs. \n       \n      \n       \n         -y, --yes \n         Do not prompt before modifying labels. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/", 
            "text": "runs list command\n\n\n\n\nUsage\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\nShow deleted runs\n\n\nShow archives runs\n\n\nShow remote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs list [OPTIONS]\n\n  \n\n    \nList runs.\n\n\nRun lists may be filtered using a variety of options. See below\nfor details.\n\n\nBy default, the last 20 runs are shown. Use \n-a, --all\n to show\nall runs, or \n-m\n to show more 20 more runs. You may use \n-m\n\nmultiple times.\n\n\nRun indexes are included in list output for a specific listing,\nwhich is based on the available runs, their states, and the\nspecified filters. You may use the indexes in run selection\ncommands (e.g. \nruns delete\n, \ncompare\n, etc.) but note that\nthese indexes will change as runs are started, deleted, or run\nstatus changes.\n\n\nTo show run detail, use \n--verbose\n.\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n\nShow deleted runs\n\n\nUse \n--deleted\n to show deleted runs. You can use the listing for\nrun IDs and indexes to use in \nruns restore\n (restore runs) and\n\nruns purge\n (permanently delete runs).\n\n\nShow archives runs\n\n\nUse \n--archive\n to show runs in an archive directory. This option\nmay not be used with \n--delete\n.\n\n\nShow remote runs\n\n\nTo list runs on a remote, specify \n--remote REMOTE\n. Use \nguild\nremotes\n to list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-d, --deleted\n\n        \nShow deleted runs.\n\n      \n\n      \n      \n\n        \n--archive DIR\n\n        \nShow archived runs in DIR\n\n      \n\n      \n      \n\n        \n-a, --all\n\n        \nShow all runs (by default only the last 20 runs are shown)\n\n      \n\n      \n      \n\n        \n-m, --more\n\n        \nShow 20 more runs. Maybe used multiple times.\n\n      \n\n      \n      \n\n        \n--json\n\n        \nFormat runs as JSON.\n\n      \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow run details.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nList runs on REMOTE rather than local runs.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "runs list command"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/#runs-list-command", 
            "text": "Usage  Filtering by operation and label  Filtering by run status  Show deleted runs  Show archives runs  Show remote runs    Options", 
            "title": "runs list command"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/#usage", 
            "text": "guild runs list [OPTIONS] \n   \n     List runs.  Run lists may be filtered using a variety of options. See below\nfor details.  By default, the last 20 runs are shown. Use  -a, --all  to show\nall runs, or  -m  to show more 20 more runs. You may use  -m \nmultiple times.  Run indexes are included in list output for a specific listing,\nwhich is based on the available runs, their states, and the\nspecified filters. You may use the indexes in run selection\ncommands (e.g.  runs delete ,  compare , etc.) but note that\nthese indexes will change as runs are started, deleted, or run\nstatus changes.  To show run detail, use  --verbose .", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/#filtering-by-run-status", 
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.", 
            "title": "Filtering by run status"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/#show-deleted-runs", 
            "text": "Use  --deleted  to show deleted runs. You can use the listing for\nrun IDs and indexes to use in  runs restore  (restore runs) and runs purge  (permanently delete runs).", 
            "title": "Show deleted runs"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/#show-archives-runs", 
            "text": "Use  --archive  to show runs in an archive directory. This option\nmay not be used with  --delete .", 
            "title": "Show archives runs"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/#show-remote-runs", 
            "text": "To list runs on a remote, specify  --remote REMOTE . Use  guild\nremotes  to list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Show remote runs"
        }, 
        {
            "location": "/docs/commands/runs-list-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -d, --deleted \n         Show deleted runs. \n       \n      \n       \n         --archive DIR \n         Show archived runs in DIR \n       \n      \n       \n         -a, --all \n         Show all runs (by default only the last 20 runs are shown) \n       \n      \n       \n         -m, --more \n         Show 20 more runs. Maybe used multiple times. \n       \n      \n       \n         --json \n         Format runs as JSON. \n       \n      \n       \n         -v, --verbose \n         Show run details. \n       \n      \n       \n         -r, --remote REMOTE \n         List runs on REMOTE rather than local runs. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/runs-purge-cmd/", 
            "text": "runs purge command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\nPermanently deleting remote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs purge [OPTIONS] [RUN...]\n\n  \n\n    \nPermanentaly delete one or more deleted runs.\n\n\nWARNING\n: Purged runs cannot be recovered!\n\n\nRuns are purged (i.e. permanently deleted) by selecting them with\n\nRUN\n arguments. If a \nRUN\n argument is not specified, all runs\nmatching the filter criteria are purged. See SELECTING RUNS and\nFILTERING topics below for more information on how runs are\nselected.\n\n\nUse \nguild runs list --deleted\n for a list of runs that can be\npurged.\n\n\nBy default, Guild will display the list of runs to be purged and\nask you to confirm the operation. If you want to purge the runs\nwithout being prompted, use the \n--yes\n option.\n\n\nWARNING\n: Take care when purging runs using indexes as the runs\nselected with indexes can change. Review the list of runs\ncarefully before confirming a purge operation.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n\nPermanently deleting remote runs\n\n\nIf a run has been deleted remotely, you can permanently delete it\nusing \n--remote\n provided the remote type supports deleted run\nrecovery.\n\n\nREMOTE\n is the name of a configured remote. Use \nguild remotes\n\nto list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nPermanently delete remote runs.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before purging.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "runs purge command"
        }, 
        {
            "location": "/docs/commands/runs-purge-cmd/#runs-purge-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status  Permanently deleting remote runs    Options", 
            "title": "runs purge command"
        }, 
        {
            "location": "/docs/commands/runs-purge-cmd/#usage", 
            "text": "guild runs purge [OPTIONS] [RUN...] \n   \n     Permanentaly delete one or more deleted runs.  WARNING : Purged runs cannot be recovered!  Runs are purged (i.e. permanently deleted) by selecting them with RUN  arguments. If a  RUN  argument is not specified, all runs\nmatching the filter criteria are purged. See SELECTING RUNS and\nFILTERING topics below for more information on how runs are\nselected.  Use  guild runs list --deleted  for a list of runs that can be\npurged.  By default, Guild will display the list of runs to be purged and\nask you to confirm the operation. If you want to purge the runs\nwithout being prompted, use the  --yes  option.  WARNING : Take care when purging runs using indexes as the runs\nselected with indexes can change. Review the list of runs\ncarefully before confirming a purge operation.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/runs-purge-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/runs-purge-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/runs-purge-cmd/#filtering-by-run-status", 
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.", 
            "title": "Filtering by run status"
        }, 
        {
            "location": "/docs/commands/runs-purge-cmd/#permanently-deleting-remote-runs", 
            "text": "If a run has been deleted remotely, you can permanently delete it\nusing  --remote  provided the remote type supports deleted run\nrecovery.  REMOTE  is the name of a configured remote. Use  guild remotes \nto list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Permanently deleting remote runs"
        }, 
        {
            "location": "/docs/commands/runs-purge-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -r, --remote REMOTE \n         Permanently delete remote runs. \n       \n      \n       \n         -y, --yes \n         Do not prompt before purging. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/runs-restore-cmd/", 
            "text": "runs restore command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\nRestoring remote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs restore [OPTIONS] [RUN...]\n\n  \n\n    \nRestore one or more deleted runs.\n\n\nRuns are restored by selecting them with \nRUN\n arguments. If a\n\nRUN\n argument is not specified, all runs matching the filter\ncriteria are restored. See SELECTING RUNS and FILTERING topics\nbelow for more information on how runs are selected.\n\n\nUse \nguild runs list --deleted\n for a list of runs that can be\nrestored.\n\n\nBy default, Guild will display the list of runs to be restored and\nask you to confirm the operation. If you want to restore the runs\nwithout being prompted, use the \n--yes\n option.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n\nRestoring remote runs\n\n\nIf a run has been deleted remotely, you can restore it using\n\n--remote\n provided the remote type supports this feature.\n\n\nREMOTE\n is the name of a configured remote. Use \nguild remotes\n\nto list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nRestore remote runs.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before restoring.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "runs restore command"
        }, 
        {
            "location": "/docs/commands/runs-restore-cmd/#runs-restore-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status  Restoring remote runs    Options", 
            "title": "runs restore command"
        }, 
        {
            "location": "/docs/commands/runs-restore-cmd/#usage", 
            "text": "guild runs restore [OPTIONS] [RUN...] \n   \n     Restore one or more deleted runs.  Runs are restored by selecting them with  RUN  arguments. If a RUN  argument is not specified, all runs matching the filter\ncriteria are restored. See SELECTING RUNS and FILTERING topics\nbelow for more information on how runs are selected.  Use  guild runs list --deleted  for a list of runs that can be\nrestored.  By default, Guild will display the list of runs to be restored and\nask you to confirm the operation. If you want to restore the runs\nwithout being prompted, use the  --yes  option.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/runs-restore-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/runs-restore-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/runs-restore-cmd/#filtering-by-run-status", 
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.", 
            "title": "Filtering by run status"
        }, 
        {
            "location": "/docs/commands/runs-restore-cmd/#restoring-remote-runs", 
            "text": "If a run has been deleted remotely, you can restore it using --remote  provided the remote type supports this feature.  REMOTE  is the name of a configured remote. Use  guild remotes \nto list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Restoring remote runs"
        }, 
        {
            "location": "/docs/commands/runs-restore-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -r, --remote REMOTE \n         Restore remote runs. \n       \n      \n       \n         -y, --yes \n         Do not prompt before restoring. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/runs-stop-cmd/", 
            "text": "runs stop command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nStopping remote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs stop [OPTIONS] [RUN...]\n\n  \n\n    \nStop one or more runs.\n\n\nRuns are stopped by specifying one or more RUN arguments. See\nSELECTING RUNS and FILTER topics below for information on\nselecting runs to be stopped.\n\n\nOnly runs with status of 'running' are considered for this\noperation.\n\n\nIf a \nRUN\n is not specified, the latest selected run is stopped.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n0\n is assumed (the most\nrecent run with status 'running').\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nStopping remote runs\n\n\nTo stop remote runs, use \n--remote\n.\n\n\nREMOTE\n is the name of a configured remote. Use \nguild remotes\n\nto list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nStop remote runs.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before stopping.\n\n      \n\n      \n      \n\n        \n-n, --no-wait\n\n        \nDon't wait for remote runs to stop.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "runs stop command"
        }, 
        {
            "location": "/docs/commands/runs-stop-cmd/#runs-stop-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label  Stopping remote runs    Options", 
            "title": "runs stop command"
        }, 
        {
            "location": "/docs/commands/runs-stop-cmd/#usage", 
            "text": "guild runs stop [OPTIONS] [RUN...] \n   \n     Stop one or more runs.  Runs are stopped by specifying one or more RUN arguments. See\nSELECTING RUNS and FILTER topics below for information on\nselecting runs to be stopped.  Only runs with status of 'running' are considered for this\noperation.  If a  RUN  is not specified, the latest selected run is stopped.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/runs-stop-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  0  is assumed (the most\nrecent run with status 'running').", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/runs-stop-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/runs-stop-cmd/#stopping-remote-runs", 
            "text": "To stop remote runs, use  --remote .  REMOTE  is the name of a configured remote. Use  guild remotes \nto list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Stopping remote runs"
        }, 
        {
            "location": "/docs/commands/runs-stop-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -r, --remote REMOTE \n         Stop remote runs. \n       \n      \n       \n         -y, --yes \n         Do not prompt before stopping. \n       \n      \n       \n         -n, --no-wait \n         Don't wait for remote runs to stop. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/search-cmd/", 
            "text": "search command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild search [OPTIONS] TERM...\n\n  \n\n    \nSearch for a package.\n\n\nSpecify one or more \nTERM\n arguments to search for.\n\n\nBy default, only Guild packages are returned. To search all\npackages, use the \n--all\n option.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-a, --all\n\n        \nSearch all packages.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "search command"
        }, 
        {
            "location": "/docs/commands/search-cmd/#search-command", 
            "text": "Usage  Options", 
            "title": "search command"
        }, 
        {
            "location": "/docs/commands/search-cmd/#usage", 
            "text": "guild search [OPTIONS] TERM... \n   \n     Search for a package.  Specify one or more  TERM  arguments to search for.  By default, only Guild packages are returned. To search all\npackages, use the  --all  option.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/search-cmd/#options", 
            "text": "-a, --all \n         Search all packages. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/shell-cmd/", 
            "text": "shell command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild shell [OPTIONS]\n\n  \n\n    \nStart a Python shell for API use.\n\n\nNOTE:\n This is a developer feature.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "shell command"
        }, 
        {
            "location": "/docs/commands/shell-cmd/#shell-command", 
            "text": "Usage  Options", 
            "title": "shell command"
        }, 
        {
            "location": "/docs/commands/shell-cmd/#usage", 
            "text": "guild shell [OPTIONS] \n   \n     Start a Python shell for API use.  NOTE:  This is a developer feature.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/shell-cmd/#options", 
            "text": "--help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/stop-cmd/", 
            "text": "stop command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nStopping remote runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild stop [OPTIONS] [RUN...]\n\n  \n\n    \nStop one or more runs.\n\n\nRuns are stopped by specifying one or more RUN arguments. See\nSELECTING RUNS and FILTER topics below for information on\nselecting runs to be stopped.\n\n\nOnly runs with status of 'running' are considered for this\noperation.\n\n\nIf a \nRUN\n is not specified, the latest selected run is stopped.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n0\n is assumed (the most\nrecent run with status 'running').\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nStopping remote runs\n\n\nTo stop remote runs, use \n--remote\n.\n\n\nREMOTE\n is the name of a configured remote. Use \nguild remotes\n\nto list available remotes.\n\n\nFor information on configuring remotes, see \nremotes\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-r, --remote REMOTE\n\n        \nStop remote runs.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before stopping.\n\n      \n\n      \n      \n\n        \n-n, --no-wait\n\n        \nDon't wait for remote runs to stop.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "stop command"
        }, 
        {
            "location": "/docs/commands/stop-cmd/#stop-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label  Stopping remote runs    Options", 
            "title": "stop command"
        }, 
        {
            "location": "/docs/commands/stop-cmd/#usage", 
            "text": "guild stop [OPTIONS] [RUN...] \n   \n     Stop one or more runs.  Runs are stopped by specifying one or more RUN arguments. See\nSELECTING RUNS and FILTER topics below for information on\nselecting runs to be stopped.  Only runs with status of 'running' are considered for this\noperation.  If a  RUN  is not specified, the latest selected run is stopped.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/stop-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  0  is assumed (the most\nrecent run with status 'running').", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/stop-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/stop-cmd/#stopping-remote-runs", 
            "text": "To stop remote runs, use  --remote .  REMOTE  is the name of a configured remote. Use  guild remotes \nto list available remotes.  For information on configuring remotes, see  remotes .", 
            "title": "Stopping remote runs"
        }, 
        {
            "location": "/docs/commands/stop-cmd/#options", 
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -r, --remote REMOTE \n         Stop remote runs. \n       \n      \n       \n         -y, --yes \n         Do not prompt before stopping. \n       \n      \n       \n         -n, --no-wait \n         Don't wait for remote runs to stop. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/sync-cmd/", 
            "text": "sync command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild sync [OPTIONS] [RUN...]\n\n  \n\n    \nSynchronize remote runs.\n\n\nA remote run is an operation that runs on another system. Guild\nkeeps track of where each remote run is located and can\nsynchronize with it. This includes downloading files generated by\nthe run as well as updating run status.\n\n\nBy default, Guild synchronizes once with the remote run and\nexits. If you want to automatically synchronize with the run while\nwatching its output, use the \n--watch\n option.\n\n\nYou can only watch one running operation at a time. If you don't\nspecify a RUN with the \n--watch\n option, Guild will watch the most\nrecently started running operation.\n\n\nWhen a remote status stops (it finished successfully, is\nterminated, or exits with an error), Guild will no longer\nsynchronize with it.\n\n\nYou can synchronize specific runs by selecting them using \nRUN\n\narguments. For more information, see SELECTING RUNS and FILTERING\ntopics below.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-w, --watch\n\n        \nWatch a remote run and synchronize in the background.\n\n      \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "sync command"
        }, 
        {
            "location": "/docs/commands/sync-cmd/#sync-command", 
            "text": "Usage  Selecting runs  Filtering by operation and label    Options", 
            "title": "sync command"
        }, 
        {
            "location": "/docs/commands/sync-cmd/#usage", 
            "text": "guild sync [OPTIONS] [RUN...] \n   \n     Synchronize remote runs.  A remote run is an operation that runs on another system. Guild\nkeeps track of where each remote run is located and can\nsynchronize with it. This includes downloading files generated by\nthe run as well as updating run status.  By default, Guild synchronizes once with the remote run and\nexits. If you want to automatically synchronize with the run while\nwatching its output, use the  --watch  option.  You can only watch one running operation at a time. If you don't\nspecify a RUN with the  --watch  option, Guild will watch the most\nrecently started running operation.  When a remote status stops (it finished successfully, is\nterminated, or exits with an error), Guild will no longer\nsynchronize with it.  You can synchronize specific runs by selecting them using  RUN \narguments. For more information, see SELECTING RUNS and FILTERING\ntopics below.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/sync-cmd/#selecting-runs", 
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).", 
            "title": "Selecting runs"
        }, 
        {
            "location": "/docs/commands/sync-cmd/#filtering-by-operation-and-label", 
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .", 
            "title": "Filtering by operation and label"
        }, 
        {
            "location": "/docs/commands/sync-cmd/#options", 
            "text": "-w, --watch \n         Watch a remote run and synchronize in the background. \n       \n      \n       \n         -o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/tensorboard-cmd/", 
            "text": "tensorboard command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild tensorboard [OPTIONS] [RUN...]\n\n  \n\n    \nVisualize runs with TensorBoard.\n\n\nThis command will start a TensorBoard process and open a browser\nwindow for you. TensorBoard will show the views that are selected\nusing the commands filters. This list corresponds to the the runs\nshown when running \nguild runs\n.\n\n\nThis command will not exit until you type \nCTRL-c\n to stop it.\n\n\nIf you'd like to change the filters used to select runs, stop the\ncommand and re-run it with a different set of filters. You may\nalternatively start another instance of TensorBoard in a separate\nconsole.\n\n\nTensorBoard will automatically refresh with the current run data.\n\n\nIf you're prefer that Guild not open a browser window, run the\ncommand with the \n--no-open\n option.\n\n\nBy default, Guild will start the TensorBoard process on a randomly\nselected free port. If you'd like to specify the port that\nTensorBoard runs on, use the \n--port\n option.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-h, --host HOST\n\n        \nName of host interface to listen on.\n\n      \n\n      \n      \n\n        \n-p, --port PORT\n\n        \nPort to listen on.\n\n      \n\n      \n      \n\n        \n--refresh-interval SECONDS\n\n        \nRefresh interval (defaults to 5 seconds).\n\n      \n\n      \n      \n\n        \n-n, --no-open\n\n        \nDon't open TensorBoard in a browser.\n\n      \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "tensorboard command"
        }, 
        {
            "location": "/docs/commands/tensorboard-cmd/#tensorboard-command", 
            "text": "Usage  Options", 
            "title": "tensorboard command"
        }, 
        {
            "location": "/docs/commands/tensorboard-cmd/#usage", 
            "text": "guild tensorboard [OPTIONS] [RUN...] \n   \n     Visualize runs with TensorBoard.  This command will start a TensorBoard process and open a browser\nwindow for you. TensorBoard will show the views that are selected\nusing the commands filters. This list corresponds to the the runs\nshown when running  guild runs .  This command will not exit until you type  CTRL-c  to stop it.  If you'd like to change the filters used to select runs, stop the\ncommand and re-run it with a different set of filters. You may\nalternatively start another instance of TensorBoard in a separate\nconsole.  TensorBoard will automatically refresh with the current run data.  If you're prefer that Guild not open a browser window, run the\ncommand with the  --no-open  option.  By default, Guild will start the TensorBoard process on a randomly\nselected free port. If you'd like to specify the port that\nTensorBoard runs on, use the  --port  option.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/tensorboard-cmd/#options", 
            "text": "-h, --host HOST \n         Name of host interface to listen on. \n       \n      \n       \n         -p, --port PORT \n         Port to listen on. \n       \n      \n       \n         --refresh-interval SECONDS \n         Refresh interval (defaults to 5 seconds). \n       \n      \n       \n         -n, --no-open \n         Don't open TensorBoard in a browser. \n       \n      \n       \n         -o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/tensorflow-cmd/", 
            "text": "tensorflow command\n\n\n\n\nUsage\n\n\nOptions\n\n\nSubcommands\n\n\n\n\n\n  \nUsage\n\n  \nguild tensorflow [OPTIONS] COMMAND [ARGS]...\n\n  \n\n    \nCollection of TensorFlow tools.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nSubcommands\n\n  \n\n    \n\n      \n      \n\n        \ninspect\n\n        \n...\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "tensorflow command"
        }, 
        {
            "location": "/docs/commands/tensorflow-cmd/#tensorflow-command", 
            "text": "Usage  Options  Subcommands", 
            "title": "tensorflow command"
        }, 
        {
            "location": "/docs/commands/tensorflow-cmd/#usage", 
            "text": "guild tensorflow [OPTIONS] COMMAND [ARGS]... \n   \n     Collection of TensorFlow tools.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/tensorflow-cmd/#options", 
            "text": "--help \n         Show command help and exit.", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/tensorflow-cmd/#subcommands", 
            "text": "inspect \n         ... \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Subcommands"
        }, 
        {
            "location": "/docs/commands/tensorflow-inspect-cmd/", 
            "text": "tensorflow inspect command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild tensorflow inspect [OPTIONS] PATH\n\n  \n\n    \nInspect a TensorFlow checkpoint file.\n\n\nPATH\n is the path to the checkpoint file (usually ending in\n\n.ckpt\n).\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n--tensor-name NAME\n\n        \nName of the tensor to inspect.\n\n      \n\n      \n      \n\n        \n--all-tensors\n\n        \nPrint the values of all the tensors.\n\n      \n\n      \n      \n\n        \n--all-tensor-names\n\n        \nPrint the name of all the tensors\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "tensorflow inspect command"
        }, 
        {
            "location": "/docs/commands/tensorflow-inspect-cmd/#tensorflow-inspect-command", 
            "text": "Usage  Options", 
            "title": "tensorflow inspect command"
        }, 
        {
            "location": "/docs/commands/tensorflow-inspect-cmd/#usage", 
            "text": "guild tensorflow inspect [OPTIONS] PATH \n   \n     Inspect a TensorFlow checkpoint file.  PATH  is the path to the checkpoint file (usually ending in .ckpt ).", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/tensorflow-inspect-cmd/#options", 
            "text": "--tensor-name NAME \n         Name of the tensor to inspect. \n       \n      \n       \n         --all-tensors \n         Print the values of all the tensors. \n       \n      \n       \n         --all-tensor-names \n         Print the name of all the tensors \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/uninstall-cmd/", 
            "text": "uninstall command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild uninstall [OPTIONS] PACKAGE...\n\n  \n\n    \nUninstall one or more packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before uninstalling.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "uninstall command"
        }, 
        {
            "location": "/docs/commands/uninstall-cmd/#uninstall-command", 
            "text": "Usage  Options", 
            "title": "uninstall command"
        }, 
        {
            "location": "/docs/commands/uninstall-cmd/#usage", 
            "text": "guild uninstall [OPTIONS] PACKAGE... \n   \n     Uninstall one or more packages.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/uninstall-cmd/#options", 
            "text": "-y, --yes \n         Do not prompt before uninstalling. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/commands/view-cmd/", 
            "text": "view command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild view [OPTIONS] [RUN...]\n\n  \n\n    \nVisualize runs.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-h, --host HOST\n\n        \nName of host interface to listen on.\n\n      \n\n      \n      \n\n        \n-p, --port PORT\n\n        \nPort to listen on.\n\n      \n\n      \n      \n\n        \n-n, --no-open\n\n        \nDon't open Guild View in a browser.\n\n      \n\n      \n      \n\n        \n--logging\n\n        \nLog requests.\n\n      \n\n      \n      \n\n        \n--files\n\n        \nView run files using file browser rather than start Guild View. Guild View related options (\n--no-open\n, \n--logging\n) are ignored.\n\n      \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.5.0.dev16", 
            "title": "view command"
        }, 
        {
            "location": "/docs/commands/view-cmd/#view-command", 
            "text": "Usage  Options", 
            "title": "view command"
        }, 
        {
            "location": "/docs/commands/view-cmd/#usage", 
            "text": "guild view [OPTIONS] [RUN...] \n   \n     Visualize runs.", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/commands/view-cmd/#options", 
            "text": "-h, --host HOST \n         Name of host interface to listen on. \n       \n      \n       \n         -p, --port PORT \n         Port to listen on. \n       \n      \n       \n         -n, --no-open \n         Don't open Guild View in a browser. \n       \n      \n       \n         --logging \n         Log requests. \n       \n      \n       \n         --files \n         View run files using file browser rather than start Guild View. Guild View related options ( --no-open ,  --logging ) are ignored. \n       \n      \n       \n         -o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.5.0.dev16", 
            "title": "Options"
        }, 
        {
            "location": "/docs/reference/guild-file/", 
            "text": "Guild file reference\n\n\n\n\nTop level objects\n\n\nExamples\n\n\n\n\n\n\nInheritance\n\n\nExamples\n\n\n\n\n\n\nPackages\n\n\nAttributes\n\n\nExamples\n\n\n\n\n\n\nModels\n\n\nAttributes\n\n\nExamples\n\n\n\n\n\n\nOperations\n\n\nAttributes\n\n\n\n\n\n\nFlags\n\n\nAttributes\n\n\n\n\n\n\nFlag choices\n\n\nAttributes\n\n\nExamples\n\n\n\n\n\n\nResources\n\n\nAttributes\n\n\n\n\n\n\nResource sources\n\n\nSource type\n\n\nAttributes\n\n\nExamples\n\n\n\n\n\n\n\n\nGuild files are files named \nguild.yml\n that contain information that\nGuild needs to perform an operation.\n\n\nGuild files are authored in \nYAML\n.\n\n\nTop level objects\n\n\nGuild files may contain a single top-level object or a list of\ntop-level objects.\n\n\nA top-level object may one of:\n\n\n\n\npackage\n\n\nmodel\n\n\nconfig\n\n\n\n\nTop-level objects are identified by the presence of an identifying\nattribute: \npackage\n, \nmodel\n, or \nconfig\n. A Guild file may contain\ntop-level objects that do not have one of these identifying\nattributes, but these objects are ignored by Guild.\n\n\nThe value of identifying attributes are used as object identifiers.\n\n\nA top-level object must only contain one and only one identifying\nattribute.\n\n\nExamples\n\n\nTop-level package:\n\n\npackage: my-package\ndescription: My package\n\n\n\n\nTop-level model:\n\n\nmodel: my-model\ndescription: My model\n\n\n\n\nTop-level config:\n\n\nconfig: my-config\ndescription: My config\n\n\n\n\nList of top-level objects:\n\n\n- package: my-package\n  description: My package\n\n- model: model-a\n  description: Model A\n\n- model: model-b\n  description: Model B\n\n- config: shared\n  description: Share config\n\n\n\n\nIllegal top-level object (multiple identifiers):\n\n\nmodel: my-model\npackage: my-package\n\n\n\n\nInheritance\n\n\nTop-level objects may extend other top-level objects by specifying an\n\nextends\n attribute. The value of \nextends\n may be a string, which\nidentifies a single object to extend, or a list of strings, which\nidentifies multiple objects to extend.\n\n\nWhen an object extends another object, it inherits its\nattributes. Extending objects may redefine attributes of the objects\nthey extend.\n\n\nWhen more than one object is extended, attributes of objects later in\nthe list take precedence of those higher in the list.\n\n\nConfig objects are used exclusively for inheritance and are not\notherwise used by Guild.\n\n\nExamples\n\n\nIn the following example, two models extend a base config. The first\nmodel redefines its description while the second does not.\n\n\n- config: base\n  description: A base config\n\n- model: model-a\n  extends: base\n  description: My model\n\n- model: model-b\n  extends: base\n\n\n\n\nPackages\n\n\nA Guild file must contain at most one \npackage\n object.\n\n\nPackages contain information used by Guild to generate Guild packages.\n\n\nAttributes\n\n\n\n\npackage\n\n\nPackage name (required string)\n\n\ndescription\n\n\nProject description (string)\n  \n\n  This may be a multi-line description.\n\n\nversion\n\n\nProject version (required string)\n\n\nurl\n\n\nURL to package website (URL)\n\n\nmaintainer\n\n\nName of individual or organization package maintainer (string)\n\n\nmaintainer-email\n\n\nEmail of package maintainer (email address)\n\n\nlicense\n\n\nName of package license (string)\n\n\ntags\n\n\nList of packages tags (list of strings)\n\n\npython-tag\n\n\nValue used as the Python tag when generating the package (string)\n\n\ndata_files\n\n\nList of additional files to include in the package (list of strings)\n\n\nresources\n\n\nList of package resources (list of \nresources\n)\n\n\npython-requires\n\n\nVersion of Python required by the package (string)\n\n\nrequires\n\n\nList of other packages required by the package (list of strings)\n\n\n\n\nExamples\n\n\nPackage definition for \nslim.resnet\n:\n\n\npackage: slim.resnet\nversion: 0.3.0\ndescription:\n  TF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2)\nurl: https://github.com/guildai/index/tree/master/slim/resnet\nmaintainer: Guild AI\nmaintainer-email: packages@guild.ai\nrequires:\n  - slim\n=0.3.0.dev11\n  - slim.datasets\n=0.3.0.dev3\nlicense: Apache 2.0\ntags: [resnet, images, model]\n\n\n\n\nModels\n\n\nModels may may be defined as top-level Guild file objects using the\n\nmodel\n identifying attribute.\n\n\nHere\ns a Guild file that defines two models:\n\n\n- model: model-a\n- model: model-b\n\n\n\n\nModels define operations, which can be run using the \nrun\n\ncommand.\n\n\nModels may also define resources that operations require.\n\n\nAttributes\n\n\n\n\nmodel\n\n\nModel name (required string)\n\n\ndescription\n\n\nModel description (string)\n  \n\n  This may be a multi-line description.\n\n\noperations\n\n\nModel operations (list of \noperations\n)\n\n\nresources\n\n\nModel resources (list of \nresources\n)\n\n\nreferences\n\n\nModel references (list of URLs)\n  \n\n  References are displayed in help text.\n\n\nextra\n\n\nAdditional information used by Guild and Guild plugins\n\n\n\n\nExamples\n\n\nComplete example of \nmnist-layers\n (from\n\ntensorflow.mnist\n):\n\n\n- model: mnist-layers\n  description: CNN estimator for MNIST using tf.layers\n  operations:\n    train:\n      description: Train the CNN\n      main: mnist/mnist --data_dir mnist-idx-data --model_dir . --export_dir .\n      requires:\n        - mnist-lib\n        - mnist/dataset\n      flags:\n        batch-size:\n          description: Number of images to process in a batch\n          default: 100\n        epochs:\n          description: Number of epochs to train\n          default: 40\n          arg-name: train_epochs\n  resources:\n    mnist-lib:\n      description: Python library for tensorflow.mnist\n      private: yes\n      sources:\n        - url: https://github.com/tensorflow/models/archive/v.1.6.0.zip\n          sha256: ed8fd7066bb014feccaed2cd2a46e516468ef24c40be8ef21a96a09849db7ff5\n          select: models-v.1.6.0/official/mnist\n  references:\n    - https://github.com/tensorflow/models/tree/v.1.6.0/official/mnist\n\n\n\n\nOperations\n\n\nOperations define commands that are run for a model. Operations are\ndefined as named objects under the \noperations\n model attribute.\n\n\nHere\ns model with two operations, \ntrain\n and \ntest\n:\n\n\nmodel: my-model\noperations:\n  train:\n    main: train --epochs 1\n  test:\n    main: test --data .\n\n\n\n\nAttributes\n\n\n\n\ndescription\n\n\nOperation description (string)\n  \n\n  This may be a multi-line description.\n\n\nhandle-keyboard-interrupt\n\n\nHandle keyboard interrupts from the user (boolean)\n  \n\n  By default, an operation must explicitly handle keyboard interrupts,\n  which are generated when the user types \nCtrl-C\n, by catching\n  Python\ns \nKeyboardInterrupt\n or the process will terminate with an\n  error and a Python traceback. Set \nhandle-keyboard-interrupt\n to\n  \nyes\n to indicate that Guild should handle \nKeyboardInterrupt\n\n  and exit without printing an error message.\n  \n\n  If the operation is run with \n--debug\n, Guild will print the full\n  traceback as a log message.\n  \n\n  Note that an operation terminated with \nCtrl-C\n will still have a\n  status of \nterminated\n even if the interrupt is handled by\n  Guild. To indicate that the operation should be considered\n  \ncompleted\n, set the operation\ns \nstoppable\n attribute to \nyes\n.\n\n\nmain\n\n\nMain command module (required string unless \nplugin-op\n is used)\n  \n\n  Operation commands must be in the form \n[MODULE] [ARG...]\n. \nMODULE\n\n  may reference a Python module defined in the model Guild file\n  directory or any Python available on the system. \nARG\n values are\n  passed through as arguments to the Python module.\n  \n\n  \nMODULE\n must not end in \n.py\n.\n  \n\n  \nARG\n values may contain references to \n#flags\n in the\n  format \n${FLAG_NAME}\n. Such references are resolved to the current\n  flag value when the command is executed.\n\n\nflags\n\n\nOperation flags (list of \nflags\n)\n  \n\n  Flags define the arguments that are passed to \nmain\n when the\n  command is executed. For more information, see \nFlags\n.\n\n\nplugin-op\n\n\nThe name of a plugin operation to used instead of \nmain\n (string)\n  \n\n  \nmain\n and \nplugin-op\n cannot both be used.\n\n\npre-process\n\n\nPre-processing shell command\n  \n\n  The command is executed as a shell script after required resources\n  are resolved and before the operation itself is started.\n  \n\n  Commands are executed in the run directory and have access to the\n  same set of environment variables as the operation itself. See\n  \nOperations\n for the\n  list of supported environment variables.\n\n\nrequired\n\n\nOne or more required resources (string or list of strings)\n  \n\n  Values must be in the form \n[PACKAGE_OR_MODEL/]RESOURCE\n.\n\n\nremote\n\n\nFlag indicating whether or not the operation is remote (boolean)\n\n\nstoppable\n\n\nFlag indicating that a terminated run should be considered completed\n  (boolean)\n  \n\n  By default, a terminated run (i.e. a run stopped by typing \nCtrl-C\n\n  or stopped with a \nSIGTERM\n signal such as that issued by the\n  \nstop\n command) has a status of \nterminated\n. If \nstoppable\n\n  is true however, the run status will be \ncompleted\n. Set this\n  value to \nyes\n when the operation is designed to be terminated\n  explicitly by the user.\n\n\n\n\nFlags\n\n\nFlags are defined for \noperations\n under the \nflags\n\nattribute as named objects.\n\n\nAttributes\n\n\n\n\ndescription\n\n\nFlag description (string)\n  \n\n  This may be a multi-line description.\n\n\ndefault\n\n\nDefault value if not specified by the user (string or number)\n\n\nrequired\n\n\nFlag indicating whether or not the flag is required (boolean)\n\n\narg-name\n\n\nName of the command argument used for flag values (string)\n  \n\n  Defaults to the flag name.\n\n\narg-skip\n\n\nBoolean indicating whether not to include the flag as a command\n  argument (boolean)\n\n\nchoices\n\n\nAllowed choices for the flag (list of \nchoices\n)\n\n\n\n\nFlag choices\n\n\nFlag choices limit the available values for a flag. They can also be\nused to apply multiple argument to a command when specified.\n\n\nAttributes\n\n\n\n\nvalue\n\n\nFlag value when choice is specified (string or number)\n\n\ndescription\n\n\nFlag choice description (string)\n\n\nargs\n\n\nMap of argument names to values (object)\n  \nThis attribute is used to define additional arguments that are\n  applied when the choice is selected. Arguments are applied in the\n  form \n--NAME VALUE\n where \nNAME\n and \nVALUE\n correspond to the\n  respective object name value pairs. Use \narg-skip\n to omit the flag\n  argument itself.\n\n\n\n\nExamples\n\n\nOperation that can train one of two model versions (default is \n1\n):\n\n\nmodel: my-model\noperations:\n  train:\n    main: train\n    flags:\n      version:\n        default: 1\n        choices: [1, 2]\n\n\n\n\nSnippet from the \nshared\nconfiguration\n\nin the \nslim\n package. Note that when \nimagenet\n is specified, the\narguments \ninput-mean\n and \ninput-std\n are included in the command\nargument.\n\n\n- config: slim-image-classifier\n  operations:\n    predict:\n      main: label_image --graph graph.pb --labels data/labels.txt\n      flags:\n        ...\n        dataset:\n          description: Dataset name to use for labels and image transformation\n          required: yes\n          arg-skip: yes\n          choices:\n            - cifar10\n            - mnist\n            - flowers\n            - value: imagenet\n              args:\n                input-mean: 0.0\n                input-std: 255\n            - custom\n\n\n\n\nResources\n\n\nResources may be included in packages and models under the \nresources\n\nobject attribute. Resources are identified by their object key.\n\n\nResources may be required by operations. Required resources are known\nas \noperation dependencies\n.\n\n\nA resource must contain at least one source. Sources may be files,\nURLs, operation output, or Python modules.\n\n\nAll required resource sources are resolved before an operation is run\nto ensure the operation has what it needs to run. Guild creates\nsymbolic links to resource sources in the run directory. For more\ninformation, see \nResource sources\n below.\n\n\nHere\ns a model with two resources, each with a single source file.\n\n\nmodel: my-model\nresources:\n  resource-a:\n    sources: [file-a]\n  resource-b:\n    sources: [file-b]\n\n\n\n\nAttributes\n\n\n\n\ndescription\n\n\nResource description (string)\n\n\npath\n\n\nRelative path within the run directory where resolved sources are\n  saved (string)\n\n\nsources\n\n\nList of resource sources (list of \nresource sources\n)\n\n\nprivate\n\n\nFlag indicating whether or not the resource is private (boolean)\n  \n\n  Private resources don\nt appear in resource lists.\n\n\nreferences\n\n\nList of reference URLs associated with the resource (list of URLs)\n  \n\n  References are displayed in help text.\n\n\n\n\nResource sources\n\n\nA resource source defines what is resolved and therefore available to\nan operation that requires the resource.\n\n\nSource files are provided to runs within the run directory as symbolic\nlinks.\n\n\nSource type\n\n\nSources have a \ntype\n, which is identified by the use of one and only\none of the following type attribute:\n\n\n\n\nfile\n\n\nSource is a file relative to the defining Guild file\n\n\nurl\n\n\nSource is a URL (\nhttp\n and \nhttps\n protocols are supported)\n\n\noperation\n\n\nSource is generated from an operation\n  \n\n  Value is an operation spec consisting of\n  \n[PACKAGE/[MODEL:]]OPERATION\n.  Multiple operation specs may be\n  specified separated with a comma. By default Guild will use the\n  latest completed or terminated run matching any of the operation\n  specs. Users may alternatively specify a run ID for the resource\n  when running the requiring operation.\n\n\nmodule\n\n\nSource is a Python module\n\n\n\n\nIf source is a string, the value is treated as a \nfile\n source type.\n\n\nAttributes\n\n\n\n\nsha256\n\n\nSHA 256 hash used to verify the source (string)\n\n\nunpack\n\n\nA flag indicating whether or not the source should be unpacked (boolean)\n  \n\n  By default Guild attempts to unpack files with common archive\n  extensions (\n.zip\n, \n.tar\n, \n.tar.gz\n, \n.tgz\n).\n\n\nselect\n\n\nOne or more regular expressions used to select sources from a\n  directory or unpacked archive (string or list of strings)\n\n\npost-process\n\n\nShell command executed after source has been resolved (string)\n  \n\n  This applies to \nurl\n sources only.\n  \n\n  Commands are executed in the context of the resource cache directory\n  containing the downloaded and unpacked URL source. Commands may use\n  the \n$RESDEF_DIR\n environment variable to reference files relative\n  to the directory containing the resource declaration (i.e. the\n  directory containing the project or package Guild file).\n\n\nhelp\n\n\nHelp text displayed when a source cannot be resolved (string)\n  \n\n  This can be used to help a user install a missing Python module, run\n  a required operation, etc.\n\n\n\n\nExamples\n\n\nFile \ndata-train.csv\n and \ndata-test.csv\n provided as a part of \ndata\n\nresource:\n\n\npackage: my-package\nresources:\n  data:\n    description: Data files\n    sources:\n    - data-train.csv\n    - data-test.csv\n\n\n\n\nMNIST IDX sources as a \ndataset\n resource, stored under\n\nmnist-idx-data\n in the run directory:\n\n\npackage: mnist\nresources:\n  dataset:\n    description: \nYann Lecun's MNIST dataset in compressed IDX format\n\n    path: mnist-idx-data\n    sources:\n      - url: http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n        sha256: 440fcabf73cc546fa21475e81ea370265605f56be210a4024d2ca8f203523609\n      - url: http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n        sha256: 3552534a0a558bbed6aed32b30c495cca23d567ec52cac8be1a0730e8010255c\n      - url: http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n        sha256: 8d422c7b0a1c1c79245a5bcf07fe86e33eeafee792b84584aec276f5a2dbc4e6\n      - url: http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n        sha256: f7ae60f92e00ec6debd23a6088c31dbd2371eca3ffa0defaefb259924204aec6\n\n\n\n\nOperation \nfinetune\n depends on the output of \ntrain\n, which is stored\nunder \nmodel\n in the run directory:\n\n\nmodel: my-model\noperations:\n  train:\n    main: train\n  finetune:\n    main: finetune\n    requires: trained-model\nresources:\n  trained-model:\n    path: model\n    sources:\n      - operation: train\n        select: checkpoint|model\\.ckpt.*", 
            "title": "Guild file reference"
        }, 
        {
            "location": "/docs/reference/guild-file/#guild-file-reference", 
            "text": "Top level objects  Examples    Inheritance  Examples    Packages  Attributes  Examples    Models  Attributes  Examples    Operations  Attributes    Flags  Attributes    Flag choices  Attributes  Examples    Resources  Attributes    Resource sources  Source type  Attributes  Examples     Guild files are files named  guild.yml  that contain information that\nGuild needs to perform an operation.  Guild files are authored in  YAML .", 
            "title": "Guild file reference"
        }, 
        {
            "location": "/docs/reference/guild-file/#top-level-objects", 
            "text": "Guild files may contain a single top-level object or a list of\ntop-level objects.  A top-level object may one of:   package  model  config   Top-level objects are identified by the presence of an identifying\nattribute:  package ,  model , or  config . A Guild file may contain\ntop-level objects that do not have one of these identifying\nattributes, but these objects are ignored by Guild.  The value of identifying attributes are used as object identifiers.  A top-level object must only contain one and only one identifying\nattribute.", 
            "title": "Top level objects"
        }, 
        {
            "location": "/docs/reference/guild-file/#examples", 
            "text": "Top-level package:  package: my-package\ndescription: My package  Top-level model:  model: my-model\ndescription: My model  Top-level config:  config: my-config\ndescription: My config  List of top-level objects:  - package: my-package\n  description: My package\n\n- model: model-a\n  description: Model A\n\n- model: model-b\n  description: Model B\n\n- config: shared\n  description: Share config  Illegal top-level object (multiple identifiers):  model: my-model\npackage: my-package", 
            "title": "Examples"
        }, 
        {
            "location": "/docs/reference/guild-file/#inheritance", 
            "text": "Top-level objects may extend other top-level objects by specifying an extends  attribute. The value of  extends  may be a string, which\nidentifies a single object to extend, or a list of strings, which\nidentifies multiple objects to extend.  When an object extends another object, it inherits its\nattributes. Extending objects may redefine attributes of the objects\nthey extend.  When more than one object is extended, attributes of objects later in\nthe list take precedence of those higher in the list.  Config objects are used exclusively for inheritance and are not\notherwise used by Guild.", 
            "title": "Inheritance"
        }, 
        {
            "location": "/docs/reference/guild-file/#examples_1", 
            "text": "In the following example, two models extend a base config. The first\nmodel redefines its description while the second does not.  - config: base\n  description: A base config\n\n- model: model-a\n  extends: base\n  description: My model\n\n- model: model-b\n  extends: base", 
            "title": "Examples"
        }, 
        {
            "location": "/docs/reference/guild-file/#packages", 
            "text": "A Guild file must contain at most one  package  object.  Packages contain information used by Guild to generate Guild packages.", 
            "title": "Packages"
        }, 
        {
            "location": "/docs/reference/guild-file/#attributes", 
            "text": "package  Package name (required string)  description  Project description (string)\n   \n  This may be a multi-line description.  version  Project version (required string)  url  URL to package website (URL)  maintainer  Name of individual or organization package maintainer (string)  maintainer-email  Email of package maintainer (email address)  license  Name of package license (string)  tags  List of packages tags (list of strings)  python-tag  Value used as the Python tag when generating the package (string)  data_files  List of additional files to include in the package (list of strings)  resources  List of package resources (list of  resources )  python-requires  Version of Python required by the package (string)  requires  List of other packages required by the package (list of strings)", 
            "title": "Attributes"
        }, 
        {
            "location": "/docs/reference/guild-file/#examples_2", 
            "text": "Package definition for  slim.resnet :  package: slim.resnet\nversion: 0.3.0\ndescription:\n  TF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2)\nurl: https://github.com/guildai/index/tree/master/slim/resnet\nmaintainer: Guild AI\nmaintainer-email: packages@guild.ai\nrequires:\n  - slim =0.3.0.dev11\n  - slim.datasets =0.3.0.dev3\nlicense: Apache 2.0\ntags: [resnet, images, model]", 
            "title": "Examples"
        }, 
        {
            "location": "/docs/reference/guild-file/#models", 
            "text": "Models may may be defined as top-level Guild file objects using the model  identifying attribute.  Here s a Guild file that defines two models:  - model: model-a\n- model: model-b  Models define operations, which can be run using the  run \ncommand.  Models may also define resources that operations require.", 
            "title": "Models"
        }, 
        {
            "location": "/docs/reference/guild-file/#attributes_1", 
            "text": "model  Model name (required string)  description  Model description (string)\n   \n  This may be a multi-line description.  operations  Model operations (list of  operations )  resources  Model resources (list of  resources )  references  Model references (list of URLs)\n   \n  References are displayed in help text.  extra  Additional information used by Guild and Guild plugins", 
            "title": "Attributes"
        }, 
        {
            "location": "/docs/reference/guild-file/#examples_3", 
            "text": "Complete example of  mnist-layers  (from tensorflow.mnist ):  - model: mnist-layers\n  description: CNN estimator for MNIST using tf.layers\n  operations:\n    train:\n      description: Train the CNN\n      main: mnist/mnist --data_dir mnist-idx-data --model_dir . --export_dir .\n      requires:\n        - mnist-lib\n        - mnist/dataset\n      flags:\n        batch-size:\n          description: Number of images to process in a batch\n          default: 100\n        epochs:\n          description: Number of epochs to train\n          default: 40\n          arg-name: train_epochs\n  resources:\n    mnist-lib:\n      description: Python library for tensorflow.mnist\n      private: yes\n      sources:\n        - url: https://github.com/tensorflow/models/archive/v.1.6.0.zip\n          sha256: ed8fd7066bb014feccaed2cd2a46e516468ef24c40be8ef21a96a09849db7ff5\n          select: models-v.1.6.0/official/mnist\n  references:\n    - https://github.com/tensorflow/models/tree/v.1.6.0/official/mnist", 
            "title": "Examples"
        }, 
        {
            "location": "/docs/reference/guild-file/#operations", 
            "text": "Operations define commands that are run for a model. Operations are\ndefined as named objects under the  operations  model attribute.  Here s model with two operations,  train  and  test :  model: my-model\noperations:\n  train:\n    main: train --epochs 1\n  test:\n    main: test --data .", 
            "title": "Operations"
        }, 
        {
            "location": "/docs/reference/guild-file/#attributes_2", 
            "text": "description  Operation description (string)\n   \n  This may be a multi-line description.  handle-keyboard-interrupt  Handle keyboard interrupts from the user (boolean)\n   \n  By default, an operation must explicitly handle keyboard interrupts,\n  which are generated when the user types  Ctrl-C , by catching\n  Python s  KeyboardInterrupt  or the process will terminate with an\n  error and a Python traceback. Set  handle-keyboard-interrupt  to\n   yes  to indicate that Guild should handle  KeyboardInterrupt \n  and exit without printing an error message.\n   \n  If the operation is run with  --debug , Guild will print the full\n  traceback as a log message.\n   \n  Note that an operation terminated with  Ctrl-C  will still have a\n  status of  terminated  even if the interrupt is handled by\n  Guild. To indicate that the operation should be considered\n   completed , set the operation s  stoppable  attribute to  yes .  main  Main command module (required string unless  plugin-op  is used)\n   \n  Operation commands must be in the form  [MODULE] [ARG...] .  MODULE \n  may reference a Python module defined in the model Guild file\n  directory or any Python available on the system.  ARG  values are\n  passed through as arguments to the Python module.\n   \n   MODULE  must not end in  .py .\n   \n   ARG  values may contain references to  #flags  in the\n  format  ${FLAG_NAME} . Such references are resolved to the current\n  flag value when the command is executed.  flags  Operation flags (list of  flags )\n   \n  Flags define the arguments that are passed to  main  when the\n  command is executed. For more information, see  Flags .  plugin-op  The name of a plugin operation to used instead of  main  (string)\n   \n   main  and  plugin-op  cannot both be used.  pre-process  Pre-processing shell command\n   \n  The command is executed as a shell script after required resources\n  are resolved and before the operation itself is started.\n   \n  Commands are executed in the run directory and have access to the\n  same set of environment variables as the operation itself. See\n   Operations  for the\n  list of supported environment variables.  required  One or more required resources (string or list of strings)\n   \n  Values must be in the form  [PACKAGE_OR_MODEL/]RESOURCE .  remote  Flag indicating whether or not the operation is remote (boolean)  stoppable  Flag indicating that a terminated run should be considered completed\n  (boolean)\n   \n  By default, a terminated run (i.e. a run stopped by typing  Ctrl-C \n  or stopped with a  SIGTERM  signal such as that issued by the\n   stop  command) has a status of  terminated . If  stoppable \n  is true however, the run status will be  completed . Set this\n  value to  yes  when the operation is designed to be terminated\n  explicitly by the user.", 
            "title": "Attributes"
        }, 
        {
            "location": "/docs/reference/guild-file/#flags", 
            "text": "Flags are defined for  operations  under the  flags \nattribute as named objects.", 
            "title": "Flags"
        }, 
        {
            "location": "/docs/reference/guild-file/#attributes_3", 
            "text": "description  Flag description (string)\n   \n  This may be a multi-line description.  default  Default value if not specified by the user (string or number)  required  Flag indicating whether or not the flag is required (boolean)  arg-name  Name of the command argument used for flag values (string)\n   \n  Defaults to the flag name.  arg-skip  Boolean indicating whether not to include the flag as a command\n  argument (boolean)  choices  Allowed choices for the flag (list of  choices )", 
            "title": "Attributes"
        }, 
        {
            "location": "/docs/reference/guild-file/#flag-choices", 
            "text": "Flag choices limit the available values for a flag. They can also be\nused to apply multiple argument to a command when specified.", 
            "title": "Flag choices"
        }, 
        {
            "location": "/docs/reference/guild-file/#attributes_4", 
            "text": "value  Flag value when choice is specified (string or number)  description  Flag choice description (string)  args  Map of argument names to values (object)\n   This attribute is used to define additional arguments that are\n  applied when the choice is selected. Arguments are applied in the\n  form  --NAME VALUE  where  NAME  and  VALUE  correspond to the\n  respective object name value pairs. Use  arg-skip  to omit the flag\n  argument itself.", 
            "title": "Attributes"
        }, 
        {
            "location": "/docs/reference/guild-file/#examples_4", 
            "text": "Operation that can train one of two model versions (default is  1 ):  model: my-model\noperations:\n  train:\n    main: train\n    flags:\n      version:\n        default: 1\n        choices: [1, 2]  Snippet from the  shared\nconfiguration \nin the  slim  package. Note that when  imagenet  is specified, the\narguments  input-mean  and  input-std  are included in the command\nargument.  - config: slim-image-classifier\n  operations:\n    predict:\n      main: label_image --graph graph.pb --labels data/labels.txt\n      flags:\n        ...\n        dataset:\n          description: Dataset name to use for labels and image transformation\n          required: yes\n          arg-skip: yes\n          choices:\n            - cifar10\n            - mnist\n            - flowers\n            - value: imagenet\n              args:\n                input-mean: 0.0\n                input-std: 255\n            - custom", 
            "title": "Examples"
        }, 
        {
            "location": "/docs/reference/guild-file/#resources", 
            "text": "Resources may be included in packages and models under the  resources \nobject attribute. Resources are identified by their object key.  Resources may be required by operations. Required resources are known\nas  operation dependencies .  A resource must contain at least one source. Sources may be files,\nURLs, operation output, or Python modules.  All required resource sources are resolved before an operation is run\nto ensure the operation has what it needs to run. Guild creates\nsymbolic links to resource sources in the run directory. For more\ninformation, see  Resource sources  below.  Here s a model with two resources, each with a single source file.  model: my-model\nresources:\n  resource-a:\n    sources: [file-a]\n  resource-b:\n    sources: [file-b]", 
            "title": "Resources"
        }, 
        {
            "location": "/docs/reference/guild-file/#attributes_5", 
            "text": "description  Resource description (string)  path  Relative path within the run directory where resolved sources are\n  saved (string)  sources  List of resource sources (list of  resource sources )  private  Flag indicating whether or not the resource is private (boolean)\n   \n  Private resources don t appear in resource lists.  references  List of reference URLs associated with the resource (list of URLs)\n   \n  References are displayed in help text.", 
            "title": "Attributes"
        }, 
        {
            "location": "/docs/reference/guild-file/#resource-sources", 
            "text": "A resource source defines what is resolved and therefore available to\nan operation that requires the resource.  Source files are provided to runs within the run directory as symbolic\nlinks.", 
            "title": "Resource sources"
        }, 
        {
            "location": "/docs/reference/guild-file/#source-type", 
            "text": "Sources have a  type , which is identified by the use of one and only\none of the following type attribute:   file  Source is a file relative to the defining Guild file  url  Source is a URL ( http  and  https  protocols are supported)  operation  Source is generated from an operation\n   \n  Value is an operation spec consisting of\n   [PACKAGE/[MODEL:]]OPERATION .  Multiple operation specs may be\n  specified separated with a comma. By default Guild will use the\n  latest completed or terminated run matching any of the operation\n  specs. Users may alternatively specify a run ID for the resource\n  when running the requiring operation.  module  Source is a Python module   If source is a string, the value is treated as a  file  source type.", 
            "title": "Source type"
        }, 
        {
            "location": "/docs/reference/guild-file/#attributes_6", 
            "text": "sha256  SHA 256 hash used to verify the source (string)  unpack  A flag indicating whether or not the source should be unpacked (boolean)\n   \n  By default Guild attempts to unpack files with common archive\n  extensions ( .zip ,  .tar ,  .tar.gz ,  .tgz ).  select  One or more regular expressions used to select sources from a\n  directory or unpacked archive (string or list of strings)  post-process  Shell command executed after source has been resolved (string)\n   \n  This applies to  url  sources only.\n   \n  Commands are executed in the context of the resource cache directory\n  containing the downloaded and unpacked URL source. Commands may use\n  the  $RESDEF_DIR  environment variable to reference files relative\n  to the directory containing the resource declaration (i.e. the\n  directory containing the project or package Guild file).  help  Help text displayed when a source cannot be resolved (string)\n   \n  This can be used to help a user install a missing Python module, run\n  a required operation, etc.", 
            "title": "Attributes"
        }, 
        {
            "location": "/docs/reference/guild-file/#examples_5", 
            "text": "File  data-train.csv  and  data-test.csv  provided as a part of  data \nresource:  package: my-package\nresources:\n  data:\n    description: Data files\n    sources:\n    - data-train.csv\n    - data-test.csv  MNIST IDX sources as a  dataset  resource, stored under mnist-idx-data  in the run directory:  package: mnist\nresources:\n  dataset:\n    description:  Yann Lecun's MNIST dataset in compressed IDX format \n    path: mnist-idx-data\n    sources:\n      - url: http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n        sha256: 440fcabf73cc546fa21475e81ea370265605f56be210a4024d2ca8f203523609\n      - url: http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n        sha256: 3552534a0a558bbed6aed32b30c495cca23d567ec52cac8be1a0730e8010255c\n      - url: http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n        sha256: 8d422c7b0a1c1c79245a5bcf07fe86e33eeafee792b84584aec276f5a2dbc4e6\n      - url: http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n        sha256: f7ae60f92e00ec6debd23a6088c31dbd2371eca3ffa0defaefb259924204aec6  Operation  finetune  depends on the output of  train , which is stored\nunder  model  in the run directory:  model: my-model\noperations:\n  train:\n    main: train\n  finetune:\n    main: finetune\n    requires: trained-model\nresources:\n  trained-model:\n    path: model\n    sources:\n      - operation: train\n        select: checkpoint|model\\.ckpt.*", 
            "title": "Examples"
        }, 
        {
            "location": "/docs/reference/guild-home/", 
            "text": "Guild home reference\n\n\n\n\nLayout\n\n\nRuns\n\n\nGuild metadata\n\n\n\n\n\n\nResource cache\n\n\nResource cache and virtual environments\n\n\n\n\n\n\nRuns index\n\n\n\n\nGuild home is a directory that Guild AI uses to store various files.\n\n\nEvery Guild command is associated with a Guild home. If Guild home\ndoes not exist, Guild will automatically create it.\n\n\nBy default, Guild home is one of two locations depending on whether or\nnot a command is run in a virtual environment.\n\n\nIf the command is not run in a virtual environment, Guild home is\n\n~/.guild\n by default. If the command is run in a virtual environment,\nGuild home is \n$VIRTUAL_ENV/.guild\n where \n$VIRTUAL_ENV\n is the\nvirtual environment directory.\n\n\nGuild home may be set explicitly by either defining the environment\nvariable \nGUILD_HOME\n or by using the \n-H\n option when running a\nGuild command.\n\n\nLayout\n\n\nGuild home is structured as follows:\n\n\n\n\n\n\nGuild home\nE.g. \n~/.guild\n or \n$VIRTUAL_ENV/.guild\n\n\ncache \nCaches used to optimize various Guild functions\n\n \n\n \nresources \nCached resources\n\n \nruns \nIndexed run data\n\n \n\n\n\n\nruns \nActive runs\n\n\ntrash \nDeleted runs\n\n\n\n\n\n\n\nRuns\n\n\nActive runs (i.e. non-deleted runs) are stored in\n\n$GUILD_HOME/runs\n. Each run is stored in a subdirectory with the same\nname as the run ID.\n\n\nRun directories contain all files associated with a run, including:\n\n\n\n\nResource source links\n\n\nOutput generated during the run\n\n\nGuild metadata\n\n\n\n\nGuild metadata is stored in a subdirectory named \n.guild\n in each run\ndirectory.\n\n\nGuild metadata\n\n\n\n\n\n\nRun directory\n\n.guild \nGuild metadata\n\n \n\n \noutput \nOutput generated by the run OS process\n\n \noutput.index \nTimestamp and stream information associated with run output\n\n \nattrs \nRun attributes\n\n   \n\n     \ncmd \nRun OS process command\n\n     \ndeps \nRun dependencies (required resources)\n\n     \nenv \nRun OS process environment\n\n     \nexit_status \nRun OS process exit status\n\n     \n_extra_NAME \nExtra config for NAME, defined for the model operation\n\n     \nflags \nFlag values used for the run\n\n     \nopref \nReference to the operation associated with the run\n\n     \nstarted \nStarted timestamp\n\n     \nstopped \nStopped timestamp\n\n   \n\n \n\n \n\n\n\n\n\n\n\n\n\nResource cache\n\n\nWhen Guild resolves resources, it uses \nGUILD_HOME/cache/resources\n\nfor two purposes:\n\n\n\n\nLocation for downloading URL resource sources\n\n\nLocation for unpacking archives\n\n\n\n\nResources are stored in directories that are named using hashes of the\ncorresponding resource source URI, which is an internal representation\nof a resource source location. If a resource exists in the cache, it\nis used rather than downloading or unpacking the resource again.\n\n\nGuild does not remove items from the resource cache.\n\n\nYou may clear the resource cache by deleting\n\nGUILD_HOME/cache/resources\n. Guild commands will continue to function\nnormally and the cache will be repopulated as needed.\n\n\nResource cache and virtual environments\n\n\nBy default, Guild reuses the resource cache in\n\n~/.guild/cache/resources\n (the user-level resource cache) for all\nvirtual environments. This means that cached resources available\nwithin virtual environments without having to re-download\nfiles. However, it also means that virtual environments can modify the\nuser-level resource cache.\n\n\nIf you want to isolate a virtual environment from the user-level\ncache, run the following command after you first activate the\nenvironment:\n\n\nguild init --local-resource-cache\n\n\n\n\nRuns index\n\n\nGuild maintains an index of run data in \nGUILD_HOME/cache/runs\n. This\nindex lets Guild lookup values such as training accuracy and loss\nquickly when performing comparisons.\n\n\nGuild updates the runs index lazily as it discovers new runs or new\ndata associated with a run.\n\n\nYou may clear the runs cache by deleting\n\nGUILD_HOME/cache/runs\n. Guild commands will continue to function\nnormally and runs will be indexed again as needed.", 
            "title": "Guild home reference"
        }, 
        {
            "location": "/docs/reference/guild-home/#guild-home-reference", 
            "text": "Layout  Runs  Guild metadata    Resource cache  Resource cache and virtual environments    Runs index   Guild home is a directory that Guild AI uses to store various files.  Every Guild command is associated with a Guild home. If Guild home\ndoes not exist, Guild will automatically create it.  By default, Guild home is one of two locations depending on whether or\nnot a command is run in a virtual environment.  If the command is not run in a virtual environment, Guild home is ~/.guild  by default. If the command is run in a virtual environment,\nGuild home is  $VIRTUAL_ENV/.guild  where  $VIRTUAL_ENV  is the\nvirtual environment directory.  Guild home may be set explicitly by either defining the environment\nvariable  GUILD_HOME  or by using the  -H  option when running a\nGuild command.", 
            "title": "Guild home reference"
        }, 
        {
            "location": "/docs/reference/guild-home/#layout", 
            "text": "Guild home is structured as follows:    Guild home E.g.  ~/.guild  or  $VIRTUAL_ENV/.guild  cache  Caches used to optimize various Guild functions \n  \n  resources  Cached resources \n  runs  Indexed run data \n    runs  Active runs  trash  Deleted runs", 
            "title": "Layout"
        }, 
        {
            "location": "/docs/reference/guild-home/#runs", 
            "text": "Active runs (i.e. non-deleted runs) are stored in $GUILD_HOME/runs . Each run is stored in a subdirectory with the same\nname as the run ID.  Run directories contain all files associated with a run, including:   Resource source links  Output generated during the run  Guild metadata   Guild metadata is stored in a subdirectory named  .guild  in each run\ndirectory.", 
            "title": "Runs"
        }, 
        {
            "location": "/docs/reference/guild-home/#guild-metadata", 
            "text": "Run directory .guild  Guild metadata \n  \n  output  Output generated by the run OS process \n  output.index  Timestamp and stream information associated with run output \n  attrs  Run attributes \n    \n      cmd  Run OS process command \n      deps  Run dependencies (required resources) \n      env  Run OS process environment \n      exit_status  Run OS process exit status \n      _extra_NAME  Extra config for NAME, defined for the model operation \n      flags  Flag values used for the run \n      opref  Reference to the operation associated with the run \n      started  Started timestamp \n      stopped  Stopped timestamp", 
            "title": "Guild metadata"
        }, 
        {
            "location": "/docs/reference/guild-home/#resource-cache", 
            "text": "When Guild resolves resources, it uses  GUILD_HOME/cache/resources \nfor two purposes:   Location for downloading URL resource sources  Location for unpacking archives   Resources are stored in directories that are named using hashes of the\ncorresponding resource source URI, which is an internal representation\nof a resource source location. If a resource exists in the cache, it\nis used rather than downloading or unpacking the resource again.  Guild does not remove items from the resource cache.  You may clear the resource cache by deleting GUILD_HOME/cache/resources . Guild commands will continue to function\nnormally and the cache will be repopulated as needed.", 
            "title": "Resource cache"
        }, 
        {
            "location": "/docs/reference/guild-home/#resource-cache-and-virtual-environments", 
            "text": "By default, Guild reuses the resource cache in ~/.guild/cache/resources  (the user-level resource cache) for all\nvirtual environments. This means that cached resources available\nwithin virtual environments without having to re-download\nfiles. However, it also means that virtual environments can modify the\nuser-level resource cache.  If you want to isolate a virtual environment from the user-level\ncache, run the following command after you first activate the\nenvironment:  guild init --local-resource-cache", 
            "title": "Resource cache and virtual environments"
        }, 
        {
            "location": "/docs/reference/guild-home/#runs-index", 
            "text": "Guild maintains an index of run data in  GUILD_HOME/cache/runs . This\nindex lets Guild lookup values such as training accuracy and loss\nquickly when performing comparisons.  Guild updates the runs index lazily as it discovers new runs or new\ndata associated with a run.  You may clear the runs cache by deleting GUILD_HOME/cache/runs . Guild commands will continue to function\nnormally and runs will be indexed again as needed.", 
            "title": "Runs index"
        }, 
        {
            "location": "/docs/guides/", 
            "text": "Guides\n\n\n\n\nGet started\n\n\nAnalyze\n\n\nRemote support\n\n\nModel development\n\n\nTips and tricks\n\n\n\n\n\n\n\n\nGet started\n\n\nCreate a Guild project\nConvert a Jupyter Notebook\nCreate an image classifier\n\n\n\n\nAnalyze\n\n\nCompare runs\n\n\n\n\nRemote support\n\n\nTrain on EC2\nBackup to S3\n\n\n\n\nModel development\n\n\nDebugging an operation\n\n\n\n\nTips and tricks\n\n\nSpecifying available GPUs for a run", 
            "title": "Guides"
        }, 
        {
            "location": "/docs/guides/#guides", 
            "text": "Get started  Analyze  Remote support  Model development  Tips and tricks", 
            "title": "Guides"
        }, 
        {
            "location": "/docs/guides/#get-started", 
            "text": "Create a Guild project Convert a Jupyter Notebook Create an image classifier", 
            "title": "Get started"
        }, 
        {
            "location": "/docs/guides/#analyze", 
            "text": "Compare runs", 
            "title": "Analyze"
        }, 
        {
            "location": "/docs/guides/#remote-support", 
            "text": "Train on EC2 Backup to S3", 
            "title": "Remote support"
        }, 
        {
            "location": "/docs/guides/#model-development", 
            "text": "Debugging an operation", 
            "title": "Model development"
        }, 
        {
            "location": "/docs/guides/#tips-and-tricks", 
            "text": "Specifying available GPUs for a run", 
            "title": "Tips and tricks"
        }, 
        {
            "location": "/docs/guides/create-a-project/", 
            "text": "Create a Guild project", 
            "title": "Create a Guild project"
        }, 
        {
            "location": "/docs/guides/create-a-project/#create-a-guild-project", 
            "text": "", 
            "title": "Create a Guild project"
        }, 
        {
            "location": "/docs/guides/convert-jupyter/", 
            "text": "Convert a Jupyter Notebook to a Guild project\n\n\n\n\n\n\nfoo bar baz", 
            "title": "Convert a Jupyter Notebook"
        }, 
        {
            "location": "/docs/guides/convert-jupyter/#convert-a-jupyter-notebook-to-a-guild-project", 
            "text": "foo bar baz", 
            "title": "Convert a Jupyter Notebook to a Guild project"
        }, 
        {
            "location": "/docs/guides/create-an-image-classifier/", 
            "text": "Create an image classifier", 
            "title": "Create an image classifier"
        }, 
        {
            "location": "/docs/guides/create-an-image-classifier/#create-an-image-classifier", 
            "text": "", 
            "title": "Create an image classifier"
        }, 
        {
            "location": "/docs/guides/train-on-ec2/", 
            "text": "Train on EC2", 
            "title": "Train on EC2"
        }, 
        {
            "location": "/docs/guides/train-on-ec2/#train-on-ec2", 
            "text": "", 
            "title": "Train on EC2"
        }, 
        {
            "location": "/docs/guides/backup-to-s3/", 
            "text": "Backup to S3", 
            "title": "Backup to S3"
        }, 
        {
            "location": "/docs/guides/backup-to-s3/#backup-to-s3", 
            "text": "", 
            "title": "Backup to S3"
        }, 
        {
            "location": "/docs/guides/compare-runs/", 
            "text": "Compare runs with Guild", 
            "title": "Compare runs"
        }, 
        {
            "location": "/docs/guides/compare-runs/#compare-runs-with-guild", 
            "text": "", 
            "title": "Compare runs with Guild"
        }, 
        {
            "location": "/docs/guides/publish-to-pypi/", 
            "text": "Publish to PyPI", 
            "title": "Publish to PyPI"
        }, 
        {
            "location": "/docs/guides/publish-to-pypi/#publish-to-pypi", 
            "text": "", 
            "title": "Publish to PyPI"
        }, 
        {
            "location": "/docs/guides/debug-an-operation/", 
            "text": "Debugging an operation\n\n\n\n\n\n\nTODO\n\n\nGuild AI is designed to simplify complex TensorFlow operations. This\nis great when everything works as expected. But what if you need to\nknow exactly what Guild is doing?\n\n\n\n\nGuild philosophy of light wrapping\n\n\nOverview of models and operations\n\n\nExample of running a complex operation\n\n\nUse of \nprint-cmd to see what Guild will run\n\n\nUse of \nprint-env to see what environment variables will be used\n\n\nUse of \nrun-dir when working on a specific problem\n\n\nUse of \nstage to setup a run directory without running", 
            "title": "Debugging an operation"
        }, 
        {
            "location": "/docs/guides/debug-an-operation/#debugging-an-operation", 
            "text": "TODO  Guild AI is designed to simplify complex TensorFlow operations. This\nis great when everything works as expected. But what if you need to\nknow exactly what Guild is doing?   Guild philosophy of light wrapping  Overview of models and operations  Example of running a complex operation  Use of  print-cmd to see what Guild will run  Use of  print-env to see what environment variables will be used  Use of  run-dir when working on a specific problem  Use of  stage to setup a run directory without running", 
            "title": "Debugging an operation"
        }, 
        {
            "location": "/docs/guides/specify-run-gpus/", 
            "text": "Specifying available GPUs for a run\n\n\nTo limit the GPUs available for a run, use the \n--gpus\n option with\nthe \nrun\n command. This option accepts a comma-separated list of\nGPU IDs.\n\n\nFor example, to limit a run to GPU \n0\n, use:\n\n\nguild run train --gpus 0\n\n\n\n\nYou can disable all GPU access by using \n--no-gpus\n:\n\n\nguild run evaluate --no-gpus\n\n\n\n\nThis is useful for running operations on CPU that might otherwise fail\n(e.g. other GPUs are in use or there isn\nt enough GPU memory to run\nthe operation.\n\n\n\n\nNote\n\n\nOperations that benefit from GPU acceleration will likely\nrun considerably slower on a GPU.\n\n\n\n\n\n\nNote\n\n\nThe GPU options only support CUDA devices. To view the list of\navailable CUDA devices and their respective IDs, use\n\nnvidia-smi\nthe \nNVIDIA System Management Interface", 
            "title": "Specifying available GPUs for a run"
        }, 
        {
            "location": "/docs/guides/specify-run-gpus/#specifying-available-gpus-for-a-run", 
            "text": "To limit the GPUs available for a run, use the  --gpus  option with\nthe  run  command. This option accepts a comma-separated list of\nGPU IDs.  For example, to limit a run to GPU  0 , use:  guild run train --gpus 0  You can disable all GPU access by using  --no-gpus :  guild run evaluate --no-gpus  This is useful for running operations on CPU that might otherwise fail\n(e.g. other GPUs are in use or there isn t enough GPU memory to run\nthe operation.   Note  Operations that benefit from GPU acceleration will likely\nrun considerably slower on a GPU.    Note  The GPU options only support CUDA devices. To view the list of\navailable CUDA devices and their respective IDs, use nvidia-smi the  NVIDIA System Management Interface", 
            "title": "Specifying available GPUs for a run"
        }, 
        {
            "location": "/models/", 
            "text": "Guild AI models\n\n\n\n\n\n\nCloud ML\n\n\nCensus Income Predictor\nMNIST\nDatasets\n\n\n\n\nGuild AI\n\n\nMNIST\n\n\n\n\nKeras\n\n\nDeep Dream\nMNIST\n\n\n\n\nMagenta\n\n\nImage\nMusic\n\n\n\n\nOfficial TensorFlow\n\n\nMNIST\nResNet\nWide and Deep\n\n\n\n\nTF Slim\n\n\nInception\nResNet\nVGG\n\n\n\n\nWikipedia\n\n\nDatasets", 
            "title": "Models"
        }, 
        {
            "location": "/models/#guild-ai-models", 
            "text": "", 
            "title": "Guild AI models"
        }, 
        {
            "location": "/models/#cloud-ml", 
            "text": "Census Income Predictor MNIST Datasets", 
            "title": "Cloud ML"
        }, 
        {
            "location": "/models/#guild-ai", 
            "text": "MNIST", 
            "title": "Guild AI"
        }, 
        {
            "location": "/models/#keras", 
            "text": "Deep Dream MNIST", 
            "title": "Keras"
        }, 
        {
            "location": "/models/#magenta", 
            "text": "Image Music", 
            "title": "Magenta"
        }, 
        {
            "location": "/models/#official-tensorflow", 
            "text": "MNIST ResNet Wide and Deep", 
            "title": "Official TensorFlow"
        }, 
        {
            "location": "/models/#tf-slim", 
            "text": "Inception ResNet VGG", 
            "title": "TF Slim"
        }, 
        {
            "location": "/models/#wikipedia", 
            "text": "Datasets", 
            "title": "Wikipedia"
        }, 
        {
            "location": "/models/cloudml/", 
            "text": "Cloud ML\n\n\nGoogle\ns Cloud Machine Learning Engine (Cloud ML) can be used to train\nand deploy TensorFlow models. The models below are provided by the\nCloud ML team as working examples that you can experiment and learn\nfrom in developing your own.\n\n\nCloud ML enabled models can be trained locally or in the cloud. They\nstart and manage remote runs such as training or predicting.\n\n\n\n\n\n\nPackages\n\n\nCensus Income Predictor\nMNIST\nDatasets", 
            "title": "Cloud ML"
        }, 
        {
            "location": "/models/cloudml/#cloud-ml", 
            "text": "Google s Cloud Machine Learning Engine (Cloud ML) can be used to train\nand deploy TensorFlow models. The models below are provided by the\nCloud ML team as working examples that you can experiment and learn\nfrom in developing your own.  Cloud ML enabled models can be trained locally or in the cloud. They\nstart and manage remote runs such as training or predicting.", 
            "title": "Cloud ML"
        }, 
        {
            "location": "/models/cloudml/#packages", 
            "text": "Census Income Predictor MNIST Datasets", 
            "title": "Packages"
        }, 
        {
            "location": "/models/cloudml/census/", 
            "text": "Cloud ML Census Income Predictor\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ncloudml.census\n\n    \n\n    \n\n      \nDescription\n\n      \nCensus models for Cloud ML\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/cloudml/census\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \ncensus-dnn\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \ncensus-dnn model\n\n\n  \nWide and deep classifier for census income data using TensorFlow estimators.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ncloudml-batch-predict\n\n    \nSubmit a prediction job using a model deployed in Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ninstances\n\n          \nInstances to use for prediction.\n\n          \n\n            \n            \nprediction-samples.json\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ncloudml-deploy\n\n    \nDeploy a trained classifier in Cloud ML.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ncloudml-hptune\n\n    \nTune model hyperparameters in Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nconfig\n\n          \nHyperparameter tuning configuration.\n\n          \n\n            \n            \nhptuning_config.yaml\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ncloudml-predict\n\n    \nPerform an online prediction using a model deployed in Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ninstances\n\n          \nInstances to use for prediction.\n\n          \n\n            \n            \nprediction-samples.json\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ncloudml-train\n\n    \nTrain the classifier in Cloud ML.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain the classifier locally.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nembedding-size\n\n          \nNumber of embedding dimensions for categorical columns.\n\n          \n\n            \n            \n8\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of training data epochs\nIf both train-steps and epochs are specified, the training job will run for train-steps or epochs, whichever occurs first. If unspecified will run for train-steps.\n.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \neval-batch-size\n\n          \nBatch size for evaluation steps.\n\n          \n\n            \n            \n40\n\n            \n          \n\n        \n\n        \n        \n\n          \neval-steps\n\n          \nNumber of steps to run evalution for at each checkpoint.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nexport-format\n\n          \nThe input format of the exported SavedModel binary\nValues may be JSON, CSV, or EXAMPLE.\n.\n\n          \n\n            \n            \nJSON\n\n            \n          \n\n        \n\n        \n        \n\n          \nfirst-layer-size\n\n          \nNumber of nodes in the first layer of the DNN.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nlayers\n\n          \nNumber of layers in the DNN.\n\n          \n\n            \n            \n4\n\n            \n          \n\n        \n\n        \n        \n\n          \nscale-factor\n\n          \nHow quickly the size of the layers in the DNN decay.\n\n          \n\n            \n            \n0.7\n\n            \n          \n\n        \n\n        \n        \n\n          \ntrain-batch-size\n\n          \nBatch size for training steps.\n\n          \n\n            \n            \n40\n\n            \n          \n\n        \n\n        \n        \n\n          \ntrain-steps\n\n          \nSteps to run the training job for.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \nverbosity\n\n          \nLog level (use DEBUG for more information)\nValues may be DEBUG, INFO, WARN, ERROR, or FATAL.\n.\n\n          \n\n            \n            \nINFO", 
            "title": "Cloud ML Census Income Predictor"
        }, 
        {
            "location": "/models/cloudml/census/#cloud-ml-census-income-predictor", 
            "text": "Name \n       cloudml.census \n     \n     \n       Description \n       Census models for Cloud ML \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/cloudml/census \n     \n     \n       Maintainer", 
            "title": "Cloud ML Census Income Predictor"
        }, 
        {
            "location": "/models/cloudml/census/#models", 
            "text": "census-dnn", 
            "title": "Models"
        }, 
        {
            "location": "/models/cloudml/census/#census-dnn", 
            "text": "Wide and deep classifier for census income data using TensorFlow estimators. \n\n   \n     Operations", 
            "title": "census-dnn model"
        }, 
        {
            "location": "/models/cloudml/census/#census-dnn-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-batch-predict", 
            "text": "Submit a prediction job using a model deployed in Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           instances \n           Instances to use for prediction. \n           \n            \n             prediction-samples.json", 
            "title": "cloudml-batch-predict"
        }, 
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-deploy", 
            "text": "Deploy a trained classifier in Cloud ML. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "cloudml-deploy"
        }, 
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-hptune", 
            "text": "Tune model hyperparameters in Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           config \n           Hyperparameter tuning configuration. \n           \n            \n             hptuning_config.yaml", 
            "title": "cloudml-hptune"
        }, 
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-predict", 
            "text": "Perform an online prediction using a model deployed in Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           instances \n           Instances to use for prediction. \n           \n            \n             prediction-samples.json", 
            "title": "cloudml-predict"
        }, 
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-train", 
            "text": "Train the classifier in Cloud ML. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "cloudml-train"
        }, 
        {
            "location": "/models/cloudml/census/#census-dnn-train", 
            "text": "Train the classifier locally. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           embedding-size \n           Number of embedding dimensions for categorical columns. \n           \n            \n             8 \n            \n           \n         \n        \n         \n           epochs \n           Number of training data epochs\nIf both train-steps and epochs are specified, the training job will run for train-steps or epochs, whichever occurs first. If unspecified will run for train-steps.\n. \n           \n            \n           \n         \n        \n         \n           eval-batch-size \n           Batch size for evaluation steps. \n           \n            \n             40 \n            \n           \n         \n        \n         \n           eval-steps \n           Number of steps to run evalution for at each checkpoint. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           export-format \n           The input format of the exported SavedModel binary\nValues may be JSON, CSV, or EXAMPLE.\n. \n           \n            \n             JSON \n            \n           \n         \n        \n         \n           first-layer-size \n           Number of nodes in the first layer of the DNN. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           layers \n           Number of layers in the DNN. \n           \n            \n             4 \n            \n           \n         \n        \n         \n           scale-factor \n           How quickly the size of the layers in the DNN decay. \n           \n            \n             0.7 \n            \n           \n         \n        \n         \n           train-batch-size \n           Batch size for training steps. \n           \n            \n             40 \n            \n           \n         \n        \n         \n           train-steps \n           Steps to run the training job for. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           verbosity \n           Log level (use DEBUG for more information)\nValues may be DEBUG, INFO, WARN, ERROR, or FATAL.\n. \n           \n            \n             INFO", 
            "title": "train"
        }, 
        {
            "location": "/models/cloudml/mnist/", 
            "text": "Cloud ML MNIST\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ncloudml.mnist\n\n    \n\n    \n\n      \nDescription\n\n      \nMNIST model for Cloud ML\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/cloudml/mnist\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmnist-cnn\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-cnn model\n\n\n  \nCNN classifier for MNIST data.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ncloudml-deploy\n\n    \nDeploy a model to Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nruntime-version\n\n          \nTensorFlow runtime version.\n\n          \n\n            \n            \n1.2\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ncloudml-predict\n\n    \nSend a prediction request to Cloud ML.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ncloudml-train\n\n    \nTrain the classifier in Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nruntime-version\n\n          \nTensorFlow runtime version.\n\n          \n\n            \n            \n1.2\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nprepare-data\n\n    \nPrepare the MNIST data for training.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nvalidation-size\n\n          \nNumber of examples to separate from the training data for the validation\n.\n\n          \n\n            \n            \n5000\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nprepare-request\n\n    \nGenerate a sample request file.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain the classifier locally.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \neval-batch-size\n\n          \nBatch size for evaluation steps.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \neval-steps\n\n          \nNumber of steps to run evalution for at each checkpoint.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \ntrain-batch-size\n\n          \nBatch size for training steps.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \ntrain-steps\n\n          \nSteps to run the training job for.\n\n          \n\n            \n            \n10000\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ndata\n\n    \nPrepared MNIST data.\n\n    \n\n      \n\n        \n        \n['.*\\\\.tfrecords']\n from \nprepare-data\n operation", 
            "title": "Cloud ML MNIST"
        }, 
        {
            "location": "/models/cloudml/mnist/#cloud-ml-mnist", 
            "text": "Name \n       cloudml.mnist \n     \n     \n       Description \n       MNIST model for Cloud ML \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/cloudml/mnist \n     \n     \n       Maintainer", 
            "title": "Cloud ML MNIST"
        }, 
        {
            "location": "/models/cloudml/mnist/#models", 
            "text": "mnist-cnn", 
            "title": "Models"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn", 
            "text": "CNN classifier for MNIST data. \n\n   \n     Operations \n    \n     Resources", 
            "title": "mnist-cnn model"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-cloudml-deploy", 
            "text": "Deploy a model to Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           runtime-version \n           TensorFlow runtime version. \n           \n            \n             1.2", 
            "title": "cloudml-deploy"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-cloudml-predict", 
            "text": "Send a prediction request to Cloud ML. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "cloudml-predict"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-cloudml-train", 
            "text": "Train the classifier in Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           runtime-version \n           TensorFlow runtime version. \n           \n            \n             1.2", 
            "title": "cloudml-train"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-prepare-data", 
            "text": "Prepare the MNIST data for training. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           validation-size \n           Number of examples to separate from the training data for the validation\n. \n           \n            \n             5000", 
            "title": "prepare-data"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-prepare-request", 
            "text": "Generate a sample request file. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "prepare-request"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-train", 
            "text": "Train the classifier locally. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           eval-batch-size \n           Batch size for evaluation steps. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           eval-steps \n           Number of steps to run evalution for at each checkpoint. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           train-batch-size \n           Batch size for training steps. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           train-steps \n           Steps to run the training job for. \n           \n            \n             10000", 
            "title": "train"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-data-res", 
            "text": "Prepared MNIST data. \n     \n       \n        \n         ['.*\\\\.tfrecords']  from  prepare-data  operation", 
            "title": "data"
        }, 
        {
            "location": "/models/cloudml/datasets/", 
            "text": "Cloud ML Datasets\n\n\nCloud ML datasets are used with Cloud ML models. They are provided as\npackage resources and automatically downloaded when needed by a Cloud\nML model operation.\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ncloudml.datasets\n\n    \n\n    \n\n      \nDescription\n\n      \nCloud ML datasets\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/cloudml/datasets\n\n    \n\n    \n\n      \nMaintainer", 
            "title": "Cloud ML Datasets"
        }, 
        {
            "location": "/models/cloudml/datasets/#cloud-ml-datasets", 
            "text": "Cloud ML datasets are used with Cloud ML models. They are provided as\npackage resources and automatically downloaded when needed by a Cloud\nML model operation.  \n\n   \n     \n       Name \n       cloudml.datasets \n     \n     \n       Description \n       Cloud ML datasets \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/cloudml/datasets \n     \n     \n       Maintainer", 
            "title": "Cloud ML Datasets"
        }, 
        {
            "location": "/models/mnist/", 
            "text": "Guild AI MNIST\n\n\nThis package contains softmax and CNN models for training with the\nMNIST dataset. These are used primarily for introducing Guild AI and\nfor testing.\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nmnist\n\n    \n\n    \n\n      \nDescription\n\n      \nCNN and softmax regression classifiers for MNIST digits\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/mnist\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmnist-cnn\n\n    \n    \nmnist-samples\n\n    \n    \nmnist-softmax\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-cnn model\n\n\n  \nCNN classifier for MNIST.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained CNN.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain the CNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images to include in a training batch.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-samples model\n\n\n  \nSample MNIST images.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nprepare\n\n    \nGenerate a set of sample MNIST images.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ncount\n\n          \nNumber of images to generate.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-softmax model\n\n\n  \nSoftmax regression classifier for MNIST.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained softmax regression.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain the softmax regression.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images to include in a training batch.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n10", 
            "title": "Guild AI MNIST"
        }, 
        {
            "location": "/models/mnist/#guild-ai-mnist", 
            "text": "This package contains softmax and CNN models for training with the\nMNIST dataset. These are used primarily for introducing Guild AI and\nfor testing.  \n\n   \n     \n       Name \n       mnist \n     \n     \n       Description \n       CNN and softmax regression classifiers for MNIST digits \n     \n     \n       Version \n       0.4.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/mnist \n     \n     \n       Maintainer", 
            "title": "Guild AI MNIST"
        }, 
        {
            "location": "/models/mnist/#models", 
            "text": "mnist-cnn \n    \n     mnist-samples \n    \n     mnist-softmax", 
            "title": "Models"
        }, 
        {
            "location": "/models/mnist/#mnist-cnn", 
            "text": "CNN classifier for MNIST. \n\n   \n     Operations", 
            "title": "mnist-cnn model"
        }, 
        {
            "location": "/models/mnist/#mnist-cnn-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/mnist/#mnist-cnn-evaluate", 
            "text": "Evaluate a trained CNN. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/mnist/#mnist-cnn-train", 
            "text": "Train the CNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images to include in a training batch. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             10", 
            "title": "train"
        }, 
        {
            "location": "/models/mnist/#mnist-samples", 
            "text": "Sample MNIST images. \n\n   \n     Operations", 
            "title": "mnist-samples model"
        }, 
        {
            "location": "/models/mnist/#mnist-samples-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/mnist/#mnist-samples-prepare", 
            "text": "Generate a set of sample MNIST images. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           count \n           Number of images to generate. \n           \n            \n             100", 
            "title": "prepare"
        }, 
        {
            "location": "/models/mnist/#mnist-softmax", 
            "text": "Softmax regression classifier for MNIST. \n\n   \n     Operations", 
            "title": "mnist-softmax model"
        }, 
        {
            "location": "/models/mnist/#mnist-softmax-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/mnist/#mnist-softmax-evaluate", 
            "text": "Evaluate a trained softmax regression. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/mnist/#mnist-softmax-train", 
            "text": "Train the softmax regression. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images to include in a training batch. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             10", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/", 
            "text": "Keras\n\n\nKeras\n is an independent neural networks API\nthat has been adopted as a core interface to TensorFlow. Keras\nprovides a number of excellent \nmodels\n, many of\nwhich have been adapted as Guild packages.\n\n\nKeras models are supported without modification. You can specify\ntraining epochs, batch size, and model hyper parameters without\nediting Keras scripts. Guild AI also enables TensorFlow event logging\nfor models so you can view training metrics using\n\nTensorBoard\n.\n\n\n\n\n\n\nPackages\n\n\nDeep Dream\nMNIST", 
            "title": "Keras"
        }, 
        {
            "location": "/models/keras/#keras", 
            "text": "Keras  is an independent neural networks API\nthat has been adopted as a core interface to TensorFlow. Keras\nprovides a number of excellent  models , many of\nwhich have been adapted as Guild packages.  Keras models are supported without modification. You can specify\ntraining epochs, batch size, and model hyper parameters without\nediting Keras scripts. Guild AI also enables TensorFlow event logging\nfor models so you can view training metrics using TensorBoard .", 
            "title": "Keras"
        }, 
        {
            "location": "/models/keras/#packages", 
            "text": "Deep Dream MNIST", 
            "title": "Packages"
        }, 
        {
            "location": "/models/keras/deep-dream/", 
            "text": "Keras Deep Dream\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nkeras.deep-dream\n\n    \n\n    \n\n      \nDescription\n\n      \nDeep dream generator in Keras\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/keras/deep-dream\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \ndeep-dream\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \ndeep-dream model\n\n\n  \n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ngenerate\n\n    \nGenerate a Deep Dream image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nimage\n\n          \nInput image used by Deep Dream.\n\n          \n\n            \n            \nrequired", 
            "title": "Keras Deep Dream"
        }, 
        {
            "location": "/models/keras/deep-dream/#keras-deep-dream", 
            "text": "Name \n       keras.deep-dream \n     \n     \n       Description \n       Deep dream generator in Keras \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/keras/deep-dream \n     \n     \n       Maintainer", 
            "title": "Keras Deep Dream"
        }, 
        {
            "location": "/models/keras/deep-dream/#models", 
            "text": "deep-dream", 
            "title": "Models"
        }, 
        {
            "location": "/models/keras/deep-dream/#deep-dream", 
            "text": "Operations", 
            "title": "deep-dream model"
        }, 
        {
            "location": "/models/keras/deep-dream/#deep-dream-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/deep-dream/#deep-dream-generate", 
            "text": "Generate a Deep Dream image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           image \n           Input image used by Deep Dream. \n           \n            \n             required", 
            "title": "generate"
        }, 
        {
            "location": "/models/keras/mnist/", 
            "text": "Keras MNIST\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nkeras.mnist\n\n    \n\n    \n\n      \nDescription\n\n      \nMNIST related models in Keras\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/keras/mnist\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmnist-acgan\n\n    \n    \nmnist-cnn\n\n    \n    \nmnist-denoising-autoencoder\n\n    \n    \nmnist-hierarchical-rnn\n\n    \n    \nmnist-irnn\n\n    \n    \nmnist-mlp\n\n    \n    \nmnist-net2net\n\n    \n    \nmnist-siamese\n\n    \n    \nmnist-swwae\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-acgan model\n\n\n  \nAuxiliary Classifier Generative Adversarial Network (ACGAN) for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the ACGAN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nbeta-1\n\n          \nBeta 1.\n\n          \n\n            \n            \n0.5\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nLearning rate.\n\n          \n\n            \n            \n0.0002\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_acgan.py\n\n      \n      \narXiv: 1511.06434\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-cnn model\n\n\n  \nConvolutional neural network (CNN) classifier for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the CNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n12\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_cnn.py\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-denoising-autoencoder model\n\n\n  \nDenoising autoencoder for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the autoencoder.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n30\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_denoising_autoencoder.py\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-hierarchical-rnn model\n\n\n  \nHierarchical RNN (HRNN) classifier for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the HRNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n5\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_hierarchical_rnn.py\n\n      \n      \narXiv: 1506.01057\n\n      \n      \nhttp://ieeexplore.ieee.org/document/7298714/\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-irnn model\n\n\n  \nImplementation of 'A Simple Way to Initialize Recurrent Networks of Rectified Linear Units' with MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the RNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n200\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nLearning rate.\n\n          \n\n            \n            \n1e-06\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_irnn.py\n\n      \n      \nhttp://arxiv.org/pdf/1504.00941v2.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-mlp model\n\n\n  \nMultilayer perceptron (MLP) classifier for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the MLP.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n20\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_mlp.py\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-net2net model\n\n\n  \nImplementation of 'Net2Net: Accelerating Learning via Knowledge Transfer' with MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the network.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n3\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_net2net.py\n\n      \n      \nhttp://arxiv.org/abs/1511.05641\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-siamese model\n\n\n  \nSiamese MLP classifier for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the MLP.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n20\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_siamese.py\n\n      \n      \nhttp://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-swwae model\n\n\n  \nStacked what-where autoencoder for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the MLP.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n5\n\n            \n          \n\n        \n\n        \n        \n\n          \npool-size\n\n          \nkernel size used for the MaxPooling2D.\n\n          \n\n            \n            \n2\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_swwae.py\n\n      \n      \narXiv: 1311.2901v3\n\n      \n      \narXiv: 1506.02351v8", 
            "title": "Keras MNIST"
        }, 
        {
            "location": "/models/keras/mnist/#keras-mnist", 
            "text": "Name \n       keras.mnist \n     \n     \n       Description \n       MNIST related models in Keras \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/keras/mnist \n     \n     \n       Maintainer", 
            "title": "Keras MNIST"
        }, 
        {
            "location": "/models/keras/mnist/#models", 
            "text": "mnist-acgan \n    \n     mnist-cnn \n    \n     mnist-denoising-autoencoder \n    \n     mnist-hierarchical-rnn \n    \n     mnist-irnn \n    \n     mnist-mlp \n    \n     mnist-net2net \n    \n     mnist-siamese \n    \n     mnist-swwae", 
            "title": "Models"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-acgan", 
            "text": "Auxiliary Classifier Generative Adversarial Network (ACGAN) for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-acgan model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-acgan-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-acgan-train", 
            "text": "Train the ACGAN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           beta-1 \n           Beta 1. \n           \n            \n             0.5 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           learning-rate \n           Learning rate. \n           \n            \n             0.0002", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-acgan-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_acgan.py \n      \n       arXiv: 1511.06434", 
            "title": "References"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-cnn", 
            "text": "Convolutional neural network (CNN) classifier for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-cnn model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-cnn-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-cnn-train", 
            "text": "Train the CNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             12", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-cnn-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_cnn.py", 
            "title": "References"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-denoising-autoencoder", 
            "text": "Denoising autoencoder for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-denoising-autoencoder model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-denoising-autoencoder-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-denoising-autoencoder-train", 
            "text": "Train the autoencoder. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             30", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-denoising-autoencoder-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_denoising_autoencoder.py", 
            "title": "References"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-hierarchical-rnn", 
            "text": "Hierarchical RNN (HRNN) classifier for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-hierarchical-rnn model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-hierarchical-rnn-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-hierarchical-rnn-train", 
            "text": "Train the HRNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             5", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-hierarchical-rnn-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_hierarchical_rnn.py \n      \n       arXiv: 1506.01057 \n      \n       http://ieeexplore.ieee.org/document/7298714/", 
            "title": "References"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-irnn", 
            "text": "Implementation of 'A Simple Way to Initialize Recurrent Networks of Rectified Linear Units' with MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-irnn model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-irnn-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-irnn-train", 
            "text": "Train the RNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             200 \n            \n           \n         \n        \n         \n           learning-rate \n           Learning rate. \n           \n            \n             1e-06", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-irnn-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_irnn.py \n      \n       http://arxiv.org/pdf/1504.00941v2.pdf", 
            "title": "References"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-mlp", 
            "text": "Multilayer perceptron (MLP) classifier for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-mlp model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-mlp-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-mlp-train", 
            "text": "Train the MLP. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             20", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-mlp-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_mlp.py", 
            "title": "References"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-net2net", 
            "text": "Implementation of 'Net2Net: Accelerating Learning via Knowledge Transfer' with MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-net2net model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-net2net-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-net2net-train", 
            "text": "Train the network. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             3", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-net2net-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_net2net.py \n      \n       http://arxiv.org/abs/1511.05641", 
            "title": "References"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-siamese", 
            "text": "Siamese MLP classifier for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-siamese model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-siamese-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-siamese-train", 
            "text": "Train the MLP. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             20", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-siamese-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_siamese.py \n      \n       http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf", 
            "title": "References"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-swwae", 
            "text": "Stacked what-where autoencoder for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-swwae model"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-swwae-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-swwae-train", 
            "text": "Train the MLP. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             5 \n            \n           \n         \n        \n         \n           pool-size \n           kernel size used for the MaxPooling2D. \n           \n            \n             2", 
            "title": "train"
        }, 
        {
            "location": "/models/keras/mnist/#mnist-swwae-references", 
            "text": "GitHub: keras-team/keras/examples/mnist_swwae.py \n      \n       arXiv: 1311.2901v3 \n      \n       arXiv: 1506.02351v8", 
            "title": "References"
        }, 
        {
            "location": "/models/magenta/", 
            "text": "Magenta\n\n\nMagenta\n is a project started by\nGoogle Brain to explore the role of machine learning in the creation\nof visual and musical art. Since its inception many outside Google\nhave made significant contributions to the project.\n\n\nGuild AI supports a number of Magenta models and more are being\nadded. See each of the packages below for its list of supported\nmodels.\n\n\n\n\n\n\nPackages\n\n\nImage\nMusic", 
            "title": "Magenta"
        }, 
        {
            "location": "/models/magenta/#magenta", 
            "text": "Magenta  is a project started by\nGoogle Brain to explore the role of machine learning in the creation\nof visual and musical art. Since its inception many outside Google\nhave made significant contributions to the project.  Guild AI supports a number of Magenta models and more are being\nadded. See each of the packages below for its list of supported\nmodels.", 
            "title": "Magenta"
        }, 
        {
            "location": "/models/magenta/#packages", 
            "text": "Image Music", 
            "title": "Packages"
        }, 
        {
            "location": "/models/magenta/image/", 
            "text": "Magenta Image\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nmagenta.image\n\n    \n\n    \n\n      \nDescription\n\n      \nImage generators from the Magenta project\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/magenta/image\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmagenta-arbitrary-stylize\n\n    \n    \nmagenta-image-stylize\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmagenta-arbitrary-stylize model\n\n\n  \nFast artistic style transfer using arbitrary painting styles.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ngenerate\n\n    \nGenerate a stylized using model pretrained on PNB and DRD images.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ncontent-images\n\n          \nPath to content images (include glob pattern matching images).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage-size\n\n          \nSize of images.\n\n          \n\n            \n            \n1024\n\n            \n          \n\n        \n\n        \n        \n\n          \ninterpolation-weights\n\n          \nInterpolation weights\nThis value is a list of float values inside square brackets. Each value is a weight for interpolation between the parameters of the identity transform and the style parameters of the style image.\nThe larger the weight is the strength of stylization is more. Weight of 1.0 means the normal style transfer and weight of 0.0 means identity transform.\n\n          \n\n            \n            \n[1.0]\n\n            \n          \n\n        \n\n        \n        \n\n          \nstyle-images\n\n          \nPath to style images (include glob pattern matching images).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \nmagenta\n\n    \nMagenta Python distribution.\n\n    \n\n      \n\n        \n        \nmodule:magenta\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization\n\n      \n      \narXiv: 1705.06830\n\n      \n      \narXiv: 1610.07629\n\n      \n      \narXiv: 1603.08155\n\n      \n      \narXiv: 1508.06576\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmagenta-image-stylize model\n\n\n  \nImplementation of 'A Learned Representation for Artistic Style'.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ngenerate\n\n    \nGenerate a stylized image using a pretrained models.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nimage\n\n          \nImage to stylize.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nstyle\n\n          \nStyle to apply (monet or varied).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \nmagenta\n\n    \nMagenta Python distribution.\n\n    \n\n      \n\n        \n        \nmodule:magenta\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization\n\n      \n      \narXiv: 1610.07629", 
            "title": "Magenta Image"
        }, 
        {
            "location": "/models/magenta/image/#magenta-image", 
            "text": "Name \n       magenta.image \n     \n     \n       Description \n       Image generators from the Magenta project \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/magenta/image \n     \n     \n       Maintainer", 
            "title": "Magenta Image"
        }, 
        {
            "location": "/models/magenta/image/#models", 
            "text": "magenta-arbitrary-stylize \n    \n     magenta-image-stylize", 
            "title": "Models"
        }, 
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize", 
            "text": "Fast artistic style transfer using arbitrary painting styles. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "magenta-arbitrary-stylize model"
        }, 
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-generate", 
            "text": "Generate a stylized using model pretrained on PNB and DRD images. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           content-images \n           Path to content images (include glob pattern matching images). \n           \n            \n             required \n            \n           \n         \n        \n         \n           image-size \n           Size of images. \n           \n            \n             1024 \n            \n           \n         \n        \n         \n           interpolation-weights \n           Interpolation weights\nThis value is a list of float values inside square brackets. Each value is a weight for interpolation between the parameters of the identity transform and the style parameters of the style image.\nThe larger the weight is the strength of stylization is more. Weight of 1.0 means the normal style transfer and weight of 0.0 means identity transform. \n           \n            \n             [1.0] \n            \n           \n         \n        \n         \n           style-images \n           Path to style images (include glob pattern matching images). \n           \n            \n             required", 
            "title": "generate"
        }, 
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-magenta-res", 
            "text": "Magenta Python distribution. \n     \n       \n        \n         module:magenta", 
            "title": "magenta"
        }, 
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-references", 
            "text": "https://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization \n      \n       arXiv: 1705.06830 \n      \n       arXiv: 1610.07629 \n      \n       arXiv: 1603.08155 \n      \n       arXiv: 1508.06576", 
            "title": "References"
        }, 
        {
            "location": "/models/magenta/image/#magenta-image-stylize", 
            "text": "Implementation of 'A Learned Representation for Artistic Style'. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "magenta-image-stylize model"
        }, 
        {
            "location": "/models/magenta/image/#magenta-image-stylize-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/magenta/image/#magenta-image-stylize-generate", 
            "text": "Generate a stylized image using a pretrained models. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           image \n           Image to stylize. \n           \n            \n             required \n            \n           \n         \n        \n         \n           style \n           Style to apply (monet or varied). \n           \n            \n             required", 
            "title": "generate"
        }, 
        {
            "location": "/models/magenta/image/#magenta-image-stylize-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/magenta/image/#magenta-image-stylize-magenta-res", 
            "text": "Magenta Python distribution. \n     \n       \n        \n         module:magenta", 
            "title": "magenta"
        }, 
        {
            "location": "/models/magenta/image/#magenta-image-stylize-references", 
            "text": "https://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization \n      \n       arXiv: 1610.07629", 
            "title": "References"
        }, 
        {
            "location": "/models/magenta/music/", 
            "text": "Magenta Music\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nmagenta.music\n\n    \n\n    \n\n      \nDescription\n\n      \nMusic generators from the Magenta project\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/magenta/music\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmagenta-melody\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmagenta-melody model\n\n\n  \nApplies language modeling to melody generation using an LSTM.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ngenerate\n\n    \nCompose melodies using one of three available pretrained models.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nconfig\n\n          \nModel configuration (basic, lookback, or attention).\n\n          \n\n            \n            \nbasic_rnn\n\n            \n          \n\n        \n\n        \n        \n\n          \noutputs\n\n          \nNumber of melodies to generate.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n        \n\n          \nprimary-midi\n\n          \nMIDI file used to prime the generator.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nprimer-melody\n\n          \nMelody to prime the generator.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nsteps\n\n          \nMelody length (16 steps = 1 bar).\n\n          \n\n            \n            \n128", 
            "title": "Magenta Music"
        }, 
        {
            "location": "/models/magenta/music/#magenta-music", 
            "text": "Name \n       magenta.music \n     \n     \n       Description \n       Music generators from the Magenta project \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/magenta/music \n     \n     \n       Maintainer", 
            "title": "Magenta Music"
        }, 
        {
            "location": "/models/magenta/music/#models", 
            "text": "magenta-melody", 
            "title": "Models"
        }, 
        {
            "location": "/models/magenta/music/#magenta-melody", 
            "text": "Applies language modeling to melody generation using an LSTM. \n\n   \n     Operations", 
            "title": "magenta-melody model"
        }, 
        {
            "location": "/models/magenta/music/#magenta-melody-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/magenta/music/#magenta-melody-generate", 
            "text": "Compose melodies using one of three available pretrained models. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           config \n           Model configuration (basic, lookback, or attention). \n           \n            \n             basic_rnn \n            \n           \n         \n        \n         \n           outputs \n           Number of melodies to generate. \n           \n            \n             10 \n            \n           \n         \n        \n         \n           primary-midi \n           MIDI file used to prime the generator. \n           \n            \n           \n         \n        \n         \n           primer-melody \n           Melody to prime the generator. \n           \n            \n           \n         \n        \n         \n           steps \n           Melody length (16 steps = 1 bar). \n           \n            \n             128", 
            "title": "generate"
        }, 
        {
            "location": "/models/tensorflow/", 
            "text": "Official TensorFlow models\n\n\nThe TensorFlow project maintains a set of \nofficial models on GitHub\n. These\nare each supported by Guild AI and are available from the packages\nbelow.\n\n\n\n\n\n\nPackages\n\n\nMNIST\nResNet\nWide and Deep", 
            "title": "Official TensorFlow models"
        }, 
        {
            "location": "/models/tensorflow/#official-tensorflow-models", 
            "text": "The TensorFlow project maintains a set of  official models on GitHub . These\nare each supported by Guild AI and are available from the packages\nbelow.", 
            "title": "Official TensorFlow models"
        }, 
        {
            "location": "/models/tensorflow/#packages", 
            "text": "MNIST ResNet Wide and Deep", 
            "title": "Packages"
        }, 
        {
            "location": "/models/tensorflow/mnist/", 
            "text": "TensorFlow MNIST\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ntensorflow.mnist\n\n    \n\n    \n\n      \nDescription\n\n      \nOfficial TensorFlow CNN classifiers for MNIST\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/tensorflow/mnist\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmnist-cnn\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-cnn model\n\n\n  \nCNN classifier for MNIST using Estimators.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the CNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images to process in a batch.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n40\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/models/tree/v.1.6.0/official/mnist", 
            "title": "TensorFlow MNIST"
        }, 
        {
            "location": "/models/tensorflow/mnist/#tensorflow-mnist", 
            "text": "Name \n       tensorflow.mnist \n     \n     \n       Description \n       Official TensorFlow CNN classifiers for MNIST \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/tensorflow/mnist \n     \n     \n       Maintainer", 
            "title": "TensorFlow MNIST"
        }, 
        {
            "location": "/models/tensorflow/mnist/#models", 
            "text": "mnist-cnn", 
            "title": "Models"
        }, 
        {
            "location": "/models/tensorflow/mnist/#mnist-cnn", 
            "text": "CNN classifier for MNIST using Estimators. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "mnist-cnn model"
        }, 
        {
            "location": "/models/tensorflow/mnist/#mnist-cnn-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/tensorflow/mnist/#mnist-cnn-train", 
            "text": "Train the CNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images to process in a batch. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             40", 
            "title": "train"
        }, 
        {
            "location": "/models/tensorflow/mnist/#mnist-cnn-references", 
            "text": "https://github.com/tensorflow/models/tree/v.1.6.0/official/mnist", 
            "title": "References"
        }, 
        {
            "location": "/models/tensorflow/resnet/", 
            "text": "TensorFlow ResNet\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ntensorflow.resnet\n\n    \n\n    \n\n      \nDescription\n\n      \nOfficial TensorFlow ResNet classifier\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/tensorflow/resnet\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nresnet-32-cifar10\n\n    \n    \nresnet-imagenet\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nresnet-32-cifar10 model\n\n\n  \nResNet-32 classifier for CIFAR-10.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the ResNet model on CIFAR-10 data.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images per batch.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n250\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs-per-eval\n\n          \nNumber of epochs to run in between evaluations.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ndataset\n\n    \nCIFAR10 dataset.\n\n    \n\n      \n\n        \n        \nhttps://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/models/tree/v.1.6.0/official/resnet\n\n      \n      \nhttps://arxiv.org/pdf/1512.03385.pdf\n\n      \n      \nhttps://arxiv.org/pdf/1603.05027.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nresnet-imagenet model\n\n\n  \nResNet classifier for ImageNet\nNote, this model requires that the ImageNet dataset be available and prepared as TFRrecords. Refer to https://git.io/vFQNF for instructions on preparing the dataset.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the ResNet model on ImageNet data.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images per batch.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \ndata-dir\n\n          \nImageNet data location.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n250\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs-per-eval\n\n          \nNumber of epochs to run in between evaluations.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n        \n\n          \nresnet-size\n\n          \nSize of the ResNet model to use (18, 34, 50, 101, 152, or 200).\n\n          \n\n            \n            \n50\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/models/tree/v.1.6.0/official/resnet\n\n      \n      \nhttps://arxiv.org/pdf/1512.03385.pdf\n\n      \n      \nhttps://arxiv.org/pdf/1603.05027.pdf", 
            "title": "TensorFlow ResNet"
        }, 
        {
            "location": "/models/tensorflow/resnet/#tensorflow-resnet", 
            "text": "Name \n       tensorflow.resnet \n     \n     \n       Description \n       Official TensorFlow ResNet classifier \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/tensorflow/resnet \n     \n     \n       Maintainer", 
            "title": "TensorFlow ResNet"
        }, 
        {
            "location": "/models/tensorflow/resnet/#models", 
            "text": "resnet-32-cifar10 \n    \n     resnet-imagenet", 
            "title": "Models"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10", 
            "text": "ResNet-32 classifier for CIFAR-10. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "resnet-32-cifar10 model"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-train", 
            "text": "Train the ResNet model on CIFAR-10 data. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images per batch. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             250 \n            \n           \n         \n        \n         \n           epochs-per-eval \n           Number of epochs to run in between evaluations. \n           \n            \n             10", 
            "title": "train"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-dataset-res", 
            "text": "CIFAR10 dataset. \n     \n       \n        \n         https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz", 
            "title": "dataset"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-references", 
            "text": "https://github.com/tensorflow/models/tree/v.1.6.0/official/resnet \n      \n       https://arxiv.org/pdf/1512.03385.pdf \n      \n       https://arxiv.org/pdf/1603.05027.pdf", 
            "title": "References"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-imagenet", 
            "text": "ResNet classifier for ImageNet\nNote, this model requires that the ImageNet dataset be available and prepared as TFRrecords. Refer to https://git.io/vFQNF for instructions on preparing the dataset. \n\n   \n     Operations \n    \n    \n     References", 
            "title": "resnet-imagenet model"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-imagenet-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-imagenet-train", 
            "text": "Train the ResNet model on ImageNet data. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images per batch. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           data-dir \n           ImageNet data location. \n           \n            \n             required \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             250 \n            \n           \n         \n        \n         \n           epochs-per-eval \n           Number of epochs to run in between evaluations. \n           \n            \n             10 \n            \n           \n         \n        \n         \n           resnet-size \n           Size of the ResNet model to use (18, 34, 50, 101, 152, or 200). \n           \n            \n             50", 
            "title": "train"
        }, 
        {
            "location": "/models/tensorflow/resnet/#resnet-imagenet-references", 
            "text": "https://github.com/tensorflow/models/tree/v.1.6.0/official/resnet \n      \n       https://arxiv.org/pdf/1512.03385.pdf \n      \n       https://arxiv.org/pdf/1603.05027.pdf", 
            "title": "References"
        }, 
        {
            "location": "/models/tensorflow/wide-deep/", 
            "text": "TensorFlow Wide and Deep\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ntensorflow.wide-deep\n\n    \n\n    \n\n      \nDescription\n\n      \nWide and deep income predictor for Census Income data\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/tensorflow/wide-deep\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nwide-deep-census\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nwide-deep-census model\n\n\n  \nWide and deep income predictor for Census Income data.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nprepare\n\n    \nDownload and prepare the Census Income data.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a wide and deep model on Census Income data.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of examples per batch.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of training epochs.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs-per-eval\n\n          \nNumber of training epochs to run between evaluations.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nmodel-type\n\n          \nModel type (wide, deep, wide_deep).", 
            "title": "TensorFlow Wide and Deep"
        }, 
        {
            "location": "/models/tensorflow/wide-deep/#tensorflow-wide-and-deep", 
            "text": "Name \n       tensorflow.wide-deep \n     \n     \n       Description \n       Wide and deep income predictor for Census Income data \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/tensorflow/wide-deep \n     \n     \n       Maintainer", 
            "title": "TensorFlow Wide and Deep"
        }, 
        {
            "location": "/models/tensorflow/wide-deep/#models", 
            "text": "wide-deep-census", 
            "title": "Models"
        }, 
        {
            "location": "/models/tensorflow/wide-deep/#wide-deep-census", 
            "text": "Wide and deep income predictor for Census Income data. \n\n   \n     Operations", 
            "title": "wide-deep-census model"
        }, 
        {
            "location": "/models/tensorflow/wide-deep/#wide-deep-census-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/tensorflow/wide-deep/#wide-deep-census-prepare", 
            "text": "Download and prepare the Census Income data. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "prepare"
        }, 
        {
            "location": "/models/tensorflow/wide-deep/#wide-deep-census-train", 
            "text": "Train a wide and deep model on Census Income data. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of examples per batch. \n           \n            \n           \n         \n        \n         \n           epochs \n           Number of training epochs. \n           \n            \n           \n         \n        \n         \n           epochs-per-eval \n           Number of training epochs to run between evaluations. \n           \n            \n           \n         \n        \n         \n           model-type \n           Model type (wide, deep, wide_deep).", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/", 
            "text": "TF Slim\n\n\nTensorFlow Slim is a high level API to TensorFlow. There are a number\nof excellent models in the \nTensorFlow model zoo\n, many of which have\nbeen adapted as Guild packages.\n\n\n\n\n\n\nPackages\n\n\nInception\nResNet\nVGG", 
            "title": "TF Slim"
        }, 
        {
            "location": "/models/slim/#tf-slim", 
            "text": "TensorFlow Slim is a high level API to TensorFlow. There are a number\nof excellent models in the  TensorFlow model zoo , many of which have\nbeen adapted as Guild packages.", 
            "title": "TF Slim"
        }, 
        {
            "location": "/models/slim/#packages", 
            "text": "Inception ResNet VGG", 
            "title": "Packages"
        }, 
        {
            "location": "/models/slim/inception/", 
            "text": "TF Slim Inception\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nslim.inception\n\n    \n\n    \n\n      \nDescription\n\n      \nTF-Slim Inception models (v1, v2, v3, v4, and Inception ResNet v2)\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/slim/inception\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nslim-inception-resnet-v2\n\n    \n    \nslim-inception-v1\n\n    \n    \nslim-inception-v2\n\n    \n    \nslim-inception-v3\n\n    \n    \nslim-inception-v4\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-resnet-v2 model\n\n\n  \nInception ResNet v2 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception ResNet v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception ResNet v2 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception ResNet v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception ResNet v2 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception ResNet v2 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception ResNet v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception ResNet v2 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception ResNet v2 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception ResNet v2 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception ResNet v2 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_resnet_v2.py\n\n      \n      \nhttp://arxiv.org/abs/1602.07261\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-v1 model\n\n\n  \nInception v1 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception v1 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception v1 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception v1 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception v1 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception v1 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception v1 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception v1 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception v1 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception v1 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception v1 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_v1.py\n\n      \n      \nhttp://arxiv.org/pdf/1409.4842v1.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-v2 model\n\n\n  \nInception v2 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception v2 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception v2 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception v2 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception v2 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_v2_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception v2 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception v2 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception v2 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_v2.py\n\n      \n      \nhttp://arxiv.org/abs/1502.03167\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-v3 model\n\n\n  \nInception v3 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception v3 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception v3 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception v3 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception v3 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception v3 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception v3 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception v3 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception v3 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception v3 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception v3 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_v3.py\n\n      \n      \nhttp://arxiv.org/abs/1512.00567\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-v4 model\n\n\n  \nInception v4 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception v4 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception v4 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception v4 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception v4 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception v4 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception v4 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception v4 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception v4 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception v4 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception v4 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_v4.py\n\n      \n      \nhttp://arxiv.org/abs/1602.07261", 
            "title": "TF Slim Inception"
        }, 
        {
            "location": "/models/slim/inception/#tf-slim-inception", 
            "text": "Name \n       slim.inception \n     \n     \n       Description \n       TF-Slim Inception models (v1, v2, v3, v4, and Inception ResNet v2) \n     \n     \n       Version \n       0.4.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/slim/inception \n     \n     \n       Maintainer", 
            "title": "TF Slim Inception"
        }, 
        {
            "location": "/models/slim/inception/#models", 
            "text": "slim-inception-resnet-v2 \n    \n     slim-inception-v1 \n    \n     slim-inception-v2 \n    \n     slim-inception-v3 \n    \n     slim-inception-v4", 
            "title": "Models"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2", 
            "text": "Inception ResNet v2 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-inception-resnet-v2 model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-evaluate", 
            "text": "Evaluate a trained Inception ResNet v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-export", 
            "text": "Generate a Inception ResNet v2 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-finetune", 
            "text": "Fine tune a Inception ResNet v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-freeze", 
            "text": "Generate a Inception ResNet v2 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-predict", 
            "text": "Use TensorFlow label_image and Inception ResNet v2 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-train", 
            "text": "Train a Inception ResNet v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-checkpoint-res", 
            "text": "Pretrained Inception ResNet v2 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-export-res", 
            "text": "Exported Inception ResNet v2 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-frozen-model-res", 
            "text": "Frozen Inception ResNet v2 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-trained-model-res", 
            "text": "Trained Inception ResNet v2 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_resnet_v2.py \n      \n       http://arxiv.org/abs/1602.07261", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1", 
            "text": "Inception v1 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-inception-v1 model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-evaluate", 
            "text": "Evaluate a trained Inception v1 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-export", 
            "text": "Generate a Inception v1 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-finetune", 
            "text": "Fine tune a Inception v1 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-freeze", 
            "text": "Generate a Inception v1 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-predict", 
            "text": "Use TensorFlow label_image and Inception v1 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-train", 
            "text": "Train a Inception v1 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-checkpoint-res", 
            "text": "Pretrained Inception v1 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-export-res", 
            "text": "Exported Inception v1 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-frozen-model-res", 
            "text": "Frozen Inception v1 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-trained-model-res", 
            "text": "Trained Inception v1 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v1-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_v1.py \n      \n       http://arxiv.org/pdf/1409.4842v1.pdf", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2", 
            "text": "Inception v2 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-inception-v2 model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-evaluate", 
            "text": "Evaluate a trained Inception v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-export", 
            "text": "Generate a Inception v2 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-finetune", 
            "text": "Fine tune a Inception v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-freeze", 
            "text": "Generate a Inception v2 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-predict", 
            "text": "Use TensorFlow label_image and Inception v2 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-train", 
            "text": "Train a Inception v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-checkpoint-res", 
            "text": "Pretrained Inception v2 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_v2_2016_08_28.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-export-res", 
            "text": "Exported Inception v2 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-frozen-model-res", 
            "text": "Frozen Inception v2 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-trained-model-res", 
            "text": "Trained Inception v2 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v2-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_v2.py \n      \n       http://arxiv.org/abs/1502.03167", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3", 
            "text": "Inception v3 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-inception-v3 model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-evaluate", 
            "text": "Evaluate a trained Inception v3 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-export", 
            "text": "Generate a Inception v3 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-finetune", 
            "text": "Fine tune a Inception v3 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-freeze", 
            "text": "Generate a Inception v3 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-predict", 
            "text": "Use TensorFlow label_image and Inception v3 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-train", 
            "text": "Train a Inception v3 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-checkpoint-res", 
            "text": "Pretrained Inception v3 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-export-res", 
            "text": "Exported Inception v3 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-frozen-model-res", 
            "text": "Frozen Inception v3 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-trained-model-res", 
            "text": "Trained Inception v3 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v3-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_v3.py \n      \n       http://arxiv.org/abs/1512.00567", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4", 
            "text": "Inception v4 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-inception-v4 model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-evaluate", 
            "text": "Evaluate a trained Inception v4 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-export", 
            "text": "Generate a Inception v4 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-finetune", 
            "text": "Fine tune a Inception v4 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-freeze", 
            "text": "Generate a Inception v4 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-predict", 
            "text": "Use TensorFlow label_image and Inception v4 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-train", 
            "text": "Train a Inception v4 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-checkpoint-res", 
            "text": "Pretrained Inception v4 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-export-res", 
            "text": "Exported Inception v4 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-frozen-model-res", 
            "text": "Frozen Inception v4 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-trained-model-res", 
            "text": "Trained Inception v4 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/inception/#slim-inception-v4-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_v4.py \n      \n       http://arxiv.org/abs/1602.07261", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/resnet/", 
            "text": "TF Slim ResNet\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nslim.resnet\n\n    \n\n    \n\n      \nDescription\n\n      \nTF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2)\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/slim/resnet\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nslim-resnet-101\n\n    \n    \nslim-resnet-152\n\n    \n    \nslim-resnet-200\n\n    \n    \nslim-resnet-50\n\n    \n    \nslim-resnet-v2-101\n\n    \n    \nslim-resnet-v2-152\n\n    \n    \nslim-resnet-v2-200\n\n    \n    \nslim-resnet-v2-50\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-101 model\n\n\n  \nResNet-101 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-101 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-101 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-101 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-101 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-101 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-101 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-101 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v1.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-152 model\n\n\n  \nResNet-152 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-152 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-152 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-152 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-152 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-152 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-152 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-152 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v1.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-200 model\n\n\n  \nResNet-200 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-200 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-200 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-200 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-200 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-200 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-200 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-200 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-200 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v1.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-50 model\n\n\n  \nResNet-50 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-50 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-50 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-50 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-50 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-50 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-50 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-50 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v1.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-v2-101 model\n\n\n  \nResNet-v2-101 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-v2-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-v2-101 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-v2-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-v2-101 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-v2-101 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-v2-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-v2-101 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v2_101_2017_04_14.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-v2-101 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-v2-101 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-v2-101 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v2.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-v2-152 model\n\n\n  \nResNet-v2-152 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-v2-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-v2-152 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-v2-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-v2-152 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-v2-152 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-v2-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-v2-152 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v2_152_2017_04_14.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-v2-152 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-v2-152 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-v2-152 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v2.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-v2-200 model\n\n\n  \nResNet-v2-200 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-v2-200 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-v2-200 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-v2-200 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-v2-200 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-v2-200 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-v2-200 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-v2-200 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-v2-200 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v2.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-v2-50 model\n\n\n  \nResNet-v2-50 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-v2-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-v2-50 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-v2-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-v2-50 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-v2-50 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-v2-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-v2-50 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v2_50_2017_04_14.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-v2-50 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-v2-50 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-v2-50 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v2.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027", 
            "title": "TF Slim ResNet"
        }, 
        {
            "location": "/models/slim/resnet/#tf-slim-resnet", 
            "text": "Name \n       slim.resnet \n     \n     \n       Description \n       TF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2) \n     \n     \n       Version \n       0.4.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/slim/resnet \n     \n     \n       Maintainer", 
            "title": "TF Slim ResNet"
        }, 
        {
            "location": "/models/slim/resnet/#models", 
            "text": "slim-resnet-101 \n    \n     slim-resnet-152 \n    \n     slim-resnet-200 \n    \n     slim-resnet-50 \n    \n     slim-resnet-v2-101 \n    \n     slim-resnet-v2-152 \n    \n     slim-resnet-v2-200 \n    \n     slim-resnet-v2-50", 
            "title": "Models"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101", 
            "text": "ResNet-101 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-resnet-101 model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-evaluate", 
            "text": "Evaluate a trained ResNet-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-export", 
            "text": "Generate a ResNet-101 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-finetune", 
            "text": "Fine tune a ResNet-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-freeze", 
            "text": "Generate a ResNet-101 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-predict", 
            "text": "Use TensorFlow label_image and ResNet-101 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-train", 
            "text": "Train a ResNet-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-checkpoint-res", 
            "text": "Pretrained ResNet-101 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-export-res", 
            "text": "Exported ResNet-101 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-frozen-model-res", 
            "text": "Frozen ResNet-101 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-trained-model-res", 
            "text": "Trained ResNet-101 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-101-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v1.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152", 
            "text": "ResNet-152 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-resnet-152 model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-evaluate", 
            "text": "Evaluate a trained ResNet-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-export", 
            "text": "Generate a ResNet-152 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-finetune", 
            "text": "Fine tune a ResNet-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-freeze", 
            "text": "Generate a ResNet-152 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-predict", 
            "text": "Use TensorFlow label_image and ResNet-152 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-train", 
            "text": "Train a ResNet-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-checkpoint-res", 
            "text": "Pretrained ResNet-152 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-export-res", 
            "text": "Exported ResNet-152 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-frozen-model-res", 
            "text": "Frozen ResNet-152 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-trained-model-res", 
            "text": "Trained ResNet-152 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-152-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v1.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200", 
            "text": "ResNet-200 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-resnet-200 model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-evaluate", 
            "text": "Evaluate a trained ResNet-200 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-export", 
            "text": "Generate a ResNet-200 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-freeze", 
            "text": "Generate a ResNet-200 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-predict", 
            "text": "Use TensorFlow label_image and ResNet-200 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-train", 
            "text": "Train a ResNet-200 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-export-res", 
            "text": "Exported ResNet-200 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-frozen-model-res", 
            "text": "Frozen ResNet-200 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-trained-model-res", 
            "text": "Trained ResNet-200 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-200-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v1.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50", 
            "text": "ResNet-50 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-resnet-50 model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-evaluate", 
            "text": "Evaluate a trained ResNet-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-export", 
            "text": "Generate a ResNet-50 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-finetune", 
            "text": "Fine tune a ResNet-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-freeze", 
            "text": "Generate a ResNet-50 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-predict", 
            "text": "Use TensorFlow label_image and ResNet-50 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-train", 
            "text": "Train a ResNet-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-checkpoint-res", 
            "text": "Pretrained ResNet-50 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-export-res", 
            "text": "Exported ResNet-50 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-frozen-model-res", 
            "text": "Frozen ResNet-50 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-trained-model-res", 
            "text": "Trained ResNet-50 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-50-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v1.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101", 
            "text": "ResNet-v2-101 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-resnet-v2-101 model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-evaluate", 
            "text": "Evaluate a trained ResNet-v2-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-export", 
            "text": "Generate a ResNet-v2-101 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-finetune", 
            "text": "Fine tune a ResNet-v2-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-freeze", 
            "text": "Generate a ResNet-v2-101 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-predict", 
            "text": "Use TensorFlow label_image and ResNet-v2-101 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-train", 
            "text": "Train a ResNet-v2-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-checkpoint-res", 
            "text": "Pretrained ResNet-v2-101 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v2_101_2017_04_14.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-export-res", 
            "text": "Exported ResNet-v2-101 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-frozen-model-res", 
            "text": "Frozen ResNet-v2-101 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-trained-model-res", 
            "text": "Trained ResNet-v2-101 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v2.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152", 
            "text": "ResNet-v2-152 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-resnet-v2-152 model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-evaluate", 
            "text": "Evaluate a trained ResNet-v2-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-export", 
            "text": "Generate a ResNet-v2-152 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-finetune", 
            "text": "Fine tune a ResNet-v2-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-freeze", 
            "text": "Generate a ResNet-v2-152 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-predict", 
            "text": "Use TensorFlow label_image and ResNet-v2-152 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-train", 
            "text": "Train a ResNet-v2-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-checkpoint-res", 
            "text": "Pretrained ResNet-v2-152 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v2_152_2017_04_14.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-export-res", 
            "text": "Exported ResNet-v2-152 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-frozen-model-res", 
            "text": "Frozen ResNet-v2-152 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-trained-model-res", 
            "text": "Trained ResNet-v2-152 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v2.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200", 
            "text": "ResNet-v2-200 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-resnet-v2-200 model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-evaluate", 
            "text": "Evaluate a trained ResNet-v2-200 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-export", 
            "text": "Generate a ResNet-v2-200 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-freeze", 
            "text": "Generate a ResNet-v2-200 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-predict", 
            "text": "Use TensorFlow label_image and ResNet-v2-200 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-train", 
            "text": "Train a ResNet-v2-200 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-export-res", 
            "text": "Exported ResNet-v2-200 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-frozen-model-res", 
            "text": "Frozen ResNet-v2-200 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-trained-model-res", 
            "text": "Trained ResNet-v2-200 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v2.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50", 
            "text": "ResNet-v2-50 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-resnet-v2-50 model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-evaluate", 
            "text": "Evaluate a trained ResNet-v2-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-export", 
            "text": "Generate a ResNet-v2-50 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-finetune", 
            "text": "Fine tune a ResNet-v2-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-freeze", 
            "text": "Generate a ResNet-v2-50 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-predict", 
            "text": "Use TensorFlow label_image and ResNet-v2-50 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-train", 
            "text": "Train a ResNet-v2-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-checkpoint-res", 
            "text": "Pretrained ResNet-v2-50 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v2_50_2017_04_14.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-export-res", 
            "text": "Exported ResNet-v2-50 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-frozen-model-res", 
            "text": "Frozen ResNet-v2-50 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-trained-model-res", 
            "text": "Trained ResNet-v2-50 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v2.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/vgg/", 
            "text": "TF Slim VGG\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nslim.vgg\n\n    \n\n    \n\n      \nDescription\n\n      \nOxford VGG models (11-Layers verion A, 16-Layers version D, 19-Layers version E)\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/slim/vgg\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nslim-vgg-11\n\n    \n    \nslim-vgg-16\n\n    \n    \nslim-vgg-19\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-vgg-11 model\n\n\n  \nOxford VGG 11-Layers version A model.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained VGG-11 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a VGG-11 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a VGG-11 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and VGG-11 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a VGG-11 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported VGG-11 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen VGG-11 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained VGG-11 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/vgg.py\n\n      \n      \nhttp://arxiv.org/pdf/1409.1556.pdf\n\n      \n      \nhttp://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-vgg-16 model\n\n\n  \nOxford VGG 16-Layers version D model.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained VGG-16 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a VGG-16 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a VGG-16 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a VGG-16 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and VGG-16 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a VGG-16 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained VGG-16 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported VGG-16 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen VGG-16 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained VGG-16 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/vgg.py\n\n      \n      \nhttp://arxiv.org/pdf/1409.1556.pdf\n\n      \n      \nhttp://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-vgg-19 model\n\n\n  \nOxford VGG 19-Layers version E model.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained VGG-19 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a VGG-19 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a VGG-19 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a VGG-19 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and VGG-19 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a VGG-19 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained VGG-19 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported VGG-19 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen VGG-19 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained VGG-19 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/vgg.py\n\n      \n      \nhttp://arxiv.org/pdf/1409.1556.pdf\n\n      \n      \nhttp://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf", 
            "title": "TF Slim VGG"
        }, 
        {
            "location": "/models/slim/vgg/#tf-slim-vgg", 
            "text": "Name \n       slim.vgg \n     \n     \n       Description \n       Oxford VGG models (11-Layers verion A, 16-Layers version D, 19-Layers version E) \n     \n     \n       Version \n       0.4.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/slim/vgg \n     \n     \n       Maintainer", 
            "title": "TF Slim VGG"
        }, 
        {
            "location": "/models/slim/vgg/#models", 
            "text": "slim-vgg-11 \n    \n     slim-vgg-16 \n    \n     slim-vgg-19", 
            "title": "Models"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11", 
            "text": "Oxford VGG 11-Layers version A model. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-vgg-11 model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-evaluate", 
            "text": "Evaluate a trained VGG-11 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-export", 
            "text": "Generate a VGG-11 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-freeze", 
            "text": "Generate a VGG-11 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-predict", 
            "text": "Use TensorFlow label_image and VGG-11 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-train", 
            "text": "Train a VGG-11 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-export-res", 
            "text": "Exported VGG-11 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-frozen-model-res", 
            "text": "Frozen VGG-11 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-trained-model-res", 
            "text": "Trained VGG-11 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-11-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/vgg.py \n      \n       http://arxiv.org/pdf/1409.1556.pdf \n      \n       http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16", 
            "text": "Oxford VGG 16-Layers version D model. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-vgg-16 model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-evaluate", 
            "text": "Evaluate a trained VGG-16 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-export", 
            "text": "Generate a VGG-16 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-finetune", 
            "text": "Fine tune a VGG-16 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-freeze", 
            "text": "Generate a VGG-16 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-predict", 
            "text": "Use TensorFlow label_image and VGG-16 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-train", 
            "text": "Train a VGG-16 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-checkpoint-res", 
            "text": "Pretrained VGG-16 model. \n     \n       \n        \n         http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-export-res", 
            "text": "Exported VGG-16 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-frozen-model-res", 
            "text": "Frozen VGG-16 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-trained-model-res", 
            "text": "Trained VGG-16 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-16-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/vgg.py \n      \n       http://arxiv.org/pdf/1409.1556.pdf \n      \n       http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf", 
            "title": "References"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19", 
            "text": "Oxford VGG 19-Layers version E model. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References", 
            "title": "slim-vgg-19 model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-evaluate", 
            "text": "Evaluate a trained VGG-19 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-export", 
            "text": "Generate a VGG-19 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-finetune", 
            "text": "Fine tune a VGG-19 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "finetune"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-freeze", 
            "text": "Generate a VGG-19 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.", 
            "title": "freeze"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-predict", 
            "text": "Use TensorFlow label_image and VGG-19 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0", 
            "title": "predict"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-train", 
            "text": "Train a VGG-19 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05", 
            "title": "train"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-checkpoint-res", 
            "text": "Pretrained VGG-19 model. \n     \n       \n        \n         http://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz", 
            "title": "checkpoint"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-cifar10-res", 
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation", 
            "title": "cifar10"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-custom-res", 
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation", 
            "title": "custom"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-export-res", 
            "text": "Exported VGG-19 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation", 
            "title": "export"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-flowers-res", 
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation", 
            "title": "flowers"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-frozen-model-res", 
            "text": "Frozen VGG-19 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation", 
            "title": "frozen-model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-mnist-res", 
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation", 
            "title": "mnist"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-trained-model-res", 
            "text": "Trained VGG-19 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations", 
            "title": "trained-model"
        }, 
        {
            "location": "/models/slim/vgg/#slim-vgg-19-references", 
            "text": "GitHub: tensorflow/models/research/slim/nets/vgg.py \n      \n       http://arxiv.org/pdf/1409.1556.pdf \n      \n       http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf", 
            "title": "References"
        }, 
        {
            "location": "/models/wikipedia/datasets/", 
            "text": "Wikipedia Datasets\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nwikipedia.datasets\n\n    \n\n    \n\n      \nDescription\n\n      \nSupport for downloading images from Wikipedia Commons\n\n    \n\n    \n\n      \nVersion\n\n      \n0.4.0\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/wikipedia/commons\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \ncommons-images\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \ncommons-images model\n\n\n  \nSupport for downloading images from Wikipedia Commons.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nprepare\n\n    \nDownload images from Wikipedia Commons\nSpecify the category to download. See https://commons.wikimedia.org/wiki/Category:Images for a list of categories.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ncategory\n\n          \nCategory of images to download.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nwidth\n\n          \nImage width.\n\n          \n\n            \n            \n100", 
            "title": "Wikipedia Datasets"
        }, 
        {
            "location": "/models/wikipedia/datasets/#wikipedia-datasets", 
            "text": "Name \n       wikipedia.datasets \n     \n     \n       Description \n       Support for downloading images from Wikipedia Commons \n     \n     \n       Version \n       0.4.0 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/wikipedia/commons \n     \n     \n       Maintainer", 
            "title": "Wikipedia Datasets"
        }, 
        {
            "location": "/models/wikipedia/datasets/#models", 
            "text": "commons-images", 
            "title": "Models"
        }, 
        {
            "location": "/models/wikipedia/datasets/#commons-images", 
            "text": "Support for downloading images from Wikipedia Commons. \n\n   \n     Operations", 
            "title": "commons-images model"
        }, 
        {
            "location": "/models/wikipedia/datasets/#commons-images-operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/models/wikipedia/datasets/#commons-images-prepare", 
            "text": "Download images from Wikipedia Commons\nSpecify the category to download. See https://commons.wikimedia.org/wiki/Category:Images for a list of categories. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           category \n           Category of images to download. \n           \n            \n             required \n            \n           \n         \n        \n         \n           width \n           Image width. \n           \n            \n             100", 
            "title": "prepare"
        }, 
        {
            "location": "/support/", 
            "text": "Support\n\n\nTo resolve an issue you\nre having with Guild AI, search this\ndocumentation or review the links below\nthere\ns a good chance your\ntopic has been covered here.\n\n\nIf you\nre still facing issues, \nopen an issue on GitHub\n and we\nll help!\n\n\n\n\n\n\nLinks\n\n\nDocumentation\nFAQ", 
            "title": "Support"
        }, 
        {
            "location": "/support/#support", 
            "text": "To resolve an issue you re having with Guild AI, search this\ndocumentation or review the links below there s a good chance your\ntopic has been covered here.  If you re still facing issues,  open an issue on GitHub  and we ll help!", 
            "title": "Support"
        }, 
        {
            "location": "/support/#links", 
            "text": "Documentation FAQ", 
            "title": "Links"
        }, 
        {
            "location": "/faq/", 
            "text": "FAQ\n\n\n\n\nManage runs\n\n\nHow do I quickly delete failed runs?\n\n\n\n\n\n\nResources\n\n\nIf a source is referenced multiple times, does Guild download each occurrence?\n\n\n\n\n\n\nRuntime characteristics\n\n\nHow much overhead does Guild incur when running an operation?\n\n\n\n\n\n\nTroubleshooting\n\n\nHow do I know which library version I\nm using?\n\n\n\n\n\n\n\n\nManage runs\n\n\nHow do I quickly delete failed runs?\n\n\nGuild saves every run whether it succeeds or not. This lets you\ntroubleshoot issues But over time failed runs can accumulate and\nyou\nll want to delete them.\n\n\nUse this command to delete failed runs:\n\n\nguild runs delete --error\n\n\n\n\nUse this command to delete terminated runs (i.e. runs that were\nstopped by the user by typing \nCTRL-c\n):\n\n\nguild runs delete --terminated\n\n\n\n\nYou can delete both failed and terminated runs by using both\n\n--error\n and \n--terminated\n options at the same time, or using\nthis short form:\n\n\nguild runs delete -ET\n\n\n\n\nGuild will let you confirm the list of runs before deleting them.\n\n\nYou can later restore a deleted run using (cmd:runs-restore)[runs\nrestore].\n\n\nResources\n\n\nIf a source is referenced multiple times, does Guild download each occurrence?\n\n\nNo, Guild will only download the source once. There is no performance\npenalty for referencing a resource source multiple times.\n\n\nRuntime characteristics\n\n\nHow much overhead does Guild incur when running an operation?\n\n\nGuild runs operations in a separate OS process to ensure that the\noperation is isolated. As of Guild 0.3.0, the additional overhead\nincurred when running an operation is as follows:\n\n\n\n\n\n\nAdditional time: typically less than 100 milliseconds but may be\n  more on slower systems or loaded systems\n\n\n\n\n\n\nAdditional resident RAM: less than 40 MB\n\n\n\n\n\n\nTroubleshooting\n\n\nHow do I know which library version I\nm using?\n\n\nThe \ncheck\n command shows software library versions:\n\n\n\n\nGuild AI\n\n\nPython\n\n\nTensorFlow\n\n\nCUDA and cuDNN\n\n\n\n\nTo show this information, run:\n\n\nguild check\n\n\n\n\nTo show more information, use the \n--verbose\n option:\n\n\nguild check --verbose", 
            "title": "FAQ"
        }, 
        {
            "location": "/faq/#faq", 
            "text": "Manage runs  How do I quickly delete failed runs?    Resources  If a source is referenced multiple times, does Guild download each occurrence?    Runtime characteristics  How much overhead does Guild incur when running an operation?    Troubleshooting  How do I know which library version I m using?", 
            "title": "FAQ"
        }, 
        {
            "location": "/faq/#manage-runs", 
            "text": "", 
            "title": "Manage runs"
        }, 
        {
            "location": "/faq/#how-do-i-quickly-delete-failed-runs", 
            "text": "Guild saves every run whether it succeeds or not. This lets you\ntroubleshoot issues But over time failed runs can accumulate and\nyou ll want to delete them.  Use this command to delete failed runs:  guild runs delete --error  Use this command to delete terminated runs (i.e. runs that were\nstopped by the user by typing  CTRL-c ):  guild runs delete --terminated  You can delete both failed and terminated runs by using both --error  and  --terminated  options at the same time, or using\nthis short form:  guild runs delete -ET  Guild will let you confirm the list of runs before deleting them.  You can later restore a deleted run using (cmd:runs-restore)[runs\nrestore].", 
            "title": "How do I quickly delete failed runs?"
        }, 
        {
            "location": "/faq/#resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/faq/#if-a-source-is-referenced-multiple-times-does-guild-download-each-occurrence", 
            "text": "No, Guild will only download the source once. There is no performance\npenalty for referencing a resource source multiple times.", 
            "title": "If a source is referenced multiple times, does Guild download each occurrence?"
        }, 
        {
            "location": "/faq/#runtime-characteristics", 
            "text": "", 
            "title": "Runtime characteristics"
        }, 
        {
            "location": "/faq/#how-much-overhead-does-guild-incur-when-running-an-operation", 
            "text": "Guild runs operations in a separate OS process to ensure that the\noperation is isolated. As of Guild 0.3.0, the additional overhead\nincurred when running an operation is as follows:    Additional time: typically less than 100 milliseconds but may be\n  more on slower systems or loaded systems    Additional resident RAM: less than 40 MB", 
            "title": "How much overhead does Guild incur when running an operation?"
        }, 
        {
            "location": "/faq/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/faq/#how-do-i-know-which-library-version-im-using", 
            "text": "The  check  command shows software library versions:   Guild AI  Python  TensorFlow  CUDA and cuDNN   To show this information, run:  guild check  To show more information, use the  --verbose  option:  guild check --verbose", 
            "title": "How do I know which library version I&rsquo;m using?"
        }, 
        {
            "location": "/about/", 
            "text": "Guild AI\n\n\nLinks\n\n\nGitHub project\n\n\nContributing\n\n\nGuild AI does not currently have a contributors policy but we are\nhappy to work with you to add features that you need and resolve\nissues.\n\n\nBefore spending time on a pull request, please\n\nopen an issue on GitHub\n to let us know what problems you\nd like to\nsolve and we\nll help identify next-steps with you!", 
            "title": "About"
        }, 
        {
            "location": "/about/#guild-ai", 
            "text": "", 
            "title": "Guild AI"
        }, 
        {
            "location": "/about/#links", 
            "text": "GitHub project", 
            "title": "Links"
        }, 
        {
            "location": "/about/#contributing", 
            "text": "Guild AI does not currently have a contributors policy but we are\nhappy to work with you to add features that you need and resolve\nissues.  Before spending time on a pull request, please open an issue on GitHub  to let us know what problems you d like to\nsolve and we ll help identify next-steps with you!", 
            "title": "Contributing"
        }
    ]
}