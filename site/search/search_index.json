{
    "docs": [
        {
            "location": "/",
            "text": "Guild AI \n\u00a0streamline your TensorFlow and Keras development\n\n\nGuild AI automates TensorFlow and Keras deep learning workflow,\nletting you focus on optimizing your models and getting them into\nproduction as quickly as possible. The command line toolset, supports\nyour deep learning work end-to-end \u2014 from model acquisition, through\ntraining and testing, to deployment. You can even publish your models\nfor your collaborators to use!\n\n\n\n  \n\n    \nTrack\n\n    \n\n      Automatically save each training run for analysis and\n      deployment.\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nCompare\n\n    \n\n      Compare performance across runs including accuracy, loss, and\n      hyperparameters.\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nDeploy\n\n    \n\n      Deploy your models to Cloud ML or serve them locally as a REST\n      API.\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nVisualize\n\n    \n\n      Visualize results across multiple runs with TensorBoard and\n      Guild View.\n    \n\n    \n\n      \n\n    \n\n  \n\n\n  \n\n    \nPackage\n\n    \n\n      Package and distribute your models for others to use and learn\n      from.\n    \n\n    \n\n  \n\n\n  \n\n    \nDiscover\n\n    \n\n      Explore state-of-the-art models in Guild AI's ever-growing\n      ecosystem.\n    \n\n    \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\nQuick start\n\n\n\n  \n\n    \nStep 1. Install Guild AI\n\n    \nGuild AI is installed\n    using \npip\n. Select one of the installation methods below.\n    \n\n    \n\n      \n\n        \n\n          \nNative pip\n\n        \n\n        \n\n          \nVirtual env\n\n        \n\n        \n\n          \nConda\n\n        \n\n      \n\n      \n\n        \n\n          \nInstall Guild AI using pip by running:\n\n          \n\n            \npip install guildai\n\n          \n\n          \n\n            \nNote\n\n            \nIf you are unable to install Guild AI due to permission\n              errors, you may need to run the command\n              using \nsudo\n:\n            \n\n            \n\n              \nsudo pip install guildai\n\n            \n\n          \n\n        \n\n        \n\n          \nInstall Guild AI in a virtual environment\n          named \nguild\n by running:\n\n          \n\n            \n\n              virtualenv guild\n              . guild/bin/activate\n              pip install guildai\n            \n\n          \n\n          \nTo install in a different virtual env,\n          replace \nguild\n above with the\n          alternate location.\n\n        \n\n        \n\n          \nInstall Guild AI in a Conda environment\n          named \nguild\n by running:\n\n          \n\n            \n\n              conda create -n guild python=3.6\n              source activate guild\n              pip install guildai\n            \n\n          \n\n          \nTo install in a different virtual env,\n          replace \nguild\n above with the\n          alternate location.\n\n        \n\n      \n\n    \n\n    \nFor more information, see \nInstalling Guild AI\n.\n    \n\n    \nWhen Guild AI is installed, initialize the Guild environment by\n    running:\n\n    \n\n      \nguild init --env\n\n    \n\n    \n\n      This step verifies that your environment is setup correctly and\n      prompts you to install TensorFlow if it isn't already installed.\n    \n\n    \n\n      If you encounter errors at this stage,\n      see \nTroubleshooting\n for more\n      information or \nopen an issue on GitHub\n to get\n      help.\n    \n\n  \n\n  \n\n    \n\n    \nInstalling Guild AI\n\n  \n\n\n  \n\n    \nStep 2. Find and install models\n\n    \nGuild AI lets you find and install models in seconds. Search\n    for \nmnist\n by running:\n\n    \n\n      \nguild search mnist\n\n    \n\n    \nFor this quick start, we'll work with the\n    base \nmnist\n package. Install it by running:\n\n    \n\n      \nguild install mnist\n\n    \n\n    \nList the models you just installed by running:\n\n    \n\n      \nguild models\n\n    \n\n  \n\n  \n\n    \n\n    \nFinding and installing models\n\n  \n\n\n  \n\n    \nStep 3. Train the models\n\n    \nIn this step we'll train two models: \nmnist-softmax\n\n      and \nmnist-cnn\n.\n    \n\n    \nFirst, train the softmax version by running:\n\n    \n\n      \nguild train mnist-softmax\n\n    \n\n    \n\n      Review the default values and\n      press \nENTER\n. The \nmnist-softmax\n model\n      trains quickly even on systems that don't have a GPU!\n    \n\n    \n\n      Next we'll train the CNN. Choose a method based on your system\n      type.\n    \n\n    \n\n      \n\n        \n\n          \nGPU-accelerated\n\n        \n\n        \n\n          \nCPU only\n\n        \n\n      \n\n      \n\n        \n\n          \nMost GPU accelerated systems can train the CNN model in a\n          minute or two. If your system system has a GPU,\n          train \nmnist-cnn\n for the default number of\n          epochs by running:\n\n          \n\n            \nguild train mnist-cnn\n\n          \n\n        \n\n        \n\n          \nIf you system doesn't have a GPU,\n          the \nmnist-cnn\n model will train slowly. You can\n          complete this step faster by training fewer epochs. Train\n          for one epoch by running:\n\n          \n\n            \nguild train mnist-cnn epochs=1\n\n          \n\n        \n\n      \n\n    \n\n    \nWhen both models are trained, view the list of runs by running:\n\n    \n\n      \nguild runs\n\n    \n\n  \n\n  \n\n    \n\n    \nTraining MNIST\n\n  \n\n\n  \n\n    \nStep 4. Compare model performance\n\n    \n\n      In this step we'll view the training\n      results. \nOpen\n      a separate command line console\n and run:\n    \n\n    \n\n      \nguild view\n\n    \n\n    \n\n      \nGuild View\n is a visual\n      application that lets you explore runs, compare model\n      performance, and view generated files. Guild View opens\n      automatically in your browser when run the command.\n    \n\n    \n\n      \n\n        \n\n      \n\n    \n\n    \nStep 4.1. Compare runs in Guild View\n\n    \n\n      In your browser, click \n in the left\n      sidebar. This displays a table containing the results of your\n      two runs.\n    \n\n    \n\n      \n\n        \n\n      \n\n    \n\n    \n\n      Use this view to find the run with the best performance. In this\n      case, it's the CNN, and by quite a margin!\n    \n\n    \nStep 4.2. Compare runs in TensorBoard\n\n    \n\n      In your browser, click \n in the\n      left sidebar. This opens another tab running TensorBoard, which\n      lets you view detailed training data. Use the tabs at the top of\n      TensorBoard to view different types of data.\n    \n\n    \n\n      \n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \n\n    \n\n    \nResults in Guild View\n\n  \n\n\n  \n\n    \nStep 5. Serve locally\n\n    \n\n      In the previous step, we saw that the CNN model performs much\n      better than the softmax! Let's serve that model locally as a\n      REST prediction API.\n    \n\n    \nIn\n    a \nnew\n    command line console\n, serve the CNN model by running:\n\n    \n\n      \nguild serve -o mnist-cnn --host localhost --port 8083\n\n    \n\n    \n\n      This command opens a new browser window for Guild Serve, which\n      describes the REST endpoint for the trained MNIST CNN model. You\n      can build your application and test locally before deploying to\n      a production environment.\n    \n\n  \n\n  \n\n    \n\n      \n\n    \n\n    \nGuild Serve\n\n  \n\n\n  \n\n    \nStep 6. Deploy to Cloud ML\n\n    \n\n      \nNote\n\n      \n\n        This step requires a Google Cloud Machine Learning Engine\n        account. To setup your account and environment, follow the\n        steps\n        in \nCloud ML Engine - Getting\n        Started\n\n      \n\n    \n\n    \n\n      When you're ready to run your prediction service in production,\n      you can deploy to Cloud ML by running:\n    \n\n    \n\n      \nguild run mnist-cnn:cloudml-deploy bucket=$BUCKET_NAME\n\n    \n\n    \n\n      \n$BUCKET_NAME\n must refer to a Google\n      Cloud Storage bucket that you have write permission to.\n    \n\n    \n\n      For a deep dive into Guild AI's Cloud ML support,\n      see \nTrain\n      and predict with Cloud ML\n.\n    \n\n  \n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n\nNext steps\n\n\n\n\n\n\n\n\n\nGo deeper with Guild AI\n\n\n\n\nGo deeper into the Quick Start material above with a step-by-step\ntutorial on training and deploying with Guild AI and Cloud ML.\n\n\n\n\nGo deeper with Guild AI \n\n\n\n\n\n\n\n\n\n\n\nExplore models\n\n\n\n\nGuild AI supports an ever-growing ecosystem of TensorFlow and Keras\nmodels that you can install and train with a few simple commands.\n\n\n\n\nDiscover the models \n\n\n\n\n\n\n\n\n\n\n\nBrowse the docs\n\n\n\n\nIf you're interested in a complete picture of Guild AI, start by\nbrowsing its comprehensives documentation.\n\n\n\n\nBrowse the docs",
            "title": "Home"
        },
        {
            "location": "/#guild-ai-streamline-your-tensorflow-and-keras-development",
            "text": "Guild AI automates TensorFlow and Keras deep learning workflow,\nletting you focus on optimizing your models and getting them into\nproduction as quickly as possible. The command line toolset, supports\nyour deep learning work end-to-end \u2014 from model acquisition, through\ntraining and testing, to deployment. You can even publish your models\nfor your collaborators to use!",
            "title": "Guild AI &nbsp;streamline your TensorFlow and Keras development"
        },
        {
            "location": "/#quick-start",
            "text": "",
            "title": "Quick start"
        },
        {
            "location": "/#next-steps",
            "text": "",
            "title": "Next steps"
        },
        {
            "location": "/install/",
            "text": "Installing Guild AI\n\n\n\n\nRequirements\n\n\nInstall Guild AI\n\n\nInstall Guild AI with Virtualenv\n\n\nInstall Guild AI with Conda\n\n\nInstall Guild AI from source code\n\n\n\n\n\n\nInstall TensorFlow\n\n\nInstall optional libraries\n\n\nCUDA and cuDNN\n\n\nNVIDIA System Management Interface\n\n\n\n\n\n\nVerify your installation\n\n\nNext steps\n\n\n\n\nRequirements\n\n\nGuild AI has the following requirements:\n\n\n\n\nMax OS, Linux\n\n\nPython 2.7, Python 3\n\n\npip\n\n\n\n\nGuild is installed from PyPI using \npip\n. Refer to \nInstalling pip\n to ensure you have it\ninstalled.\n\n\nInstall Guild AI\n\n\nTo install Guild AI, run the following on the command line:\n\n\npip install guildai\n\n\n\n\nIf you need to run \ninstall\n with administrative privileges, run:\n\n\nsudo pip install guildai\n\n\n\n\nYou may alternatively install Guild AI in a Python virtual\nenvironment. Refer to the next sections for details.\n\n\nInstall Guild AI with Virtualenv\n\n\nIf you would like to install Guild within a Python \nvirtual\nenvironment\n, run the following:\n\n\nvirtualenv guild\nsource guild/bin/activate\npip install guildai\n\n\n\n\nThis has the advantage of isolating Guild AI within a single\ndirectory. For more information on the advantages and disadvantages of\nthis approach, see \nWorking with\nVirtualenv\n.\n\n\nInstall Guild AI with Conda\n\n\nGuild AI can be installed in \nConda\n environments using pip.\n\n\nconda create -n guild python=3.6\nsource activate guild\npip install guildai\n\n\n\n\nIf you\u2019d like to install Guild with a different Python version,\nreplace \n3.6\n above with the version you want.\n\n\nInstall Guild AI from source code\n\n\n\n\nNote\n\n\nThis step is an alternative to installing Guild AI with pip\ndescribed above. Install Guild AI from source code if you want a\nspecific version from GitHub (e.g. an early release or development\nbranch) or if you want to contribute to the project.\n\n\n\n\nAdditional required tools for installing from source code:\n\n\n\n\ngit\n\n\nnpm\n v5.8.0 or later\n\n\nPython development library and headers for your system\n\n\n\n\nTo install Guild from source, clone the repository by running:\n\n\ngit clone https://github.com/guildai/guild.git\n\n\n\n\nChange to the \nguild\n directory and install the required pip packages:\n\n\ncd guild\npip install -r requirements.txt\n\n\n\n\nBuild Guild by running:\n\n\npython setup.py build\n\n\n\n\nVerify Guild by running:\n\n\nguild/scripts/guild check\n\n\n\n\nIf see the message \nNOT INSTALLED (No module named 'tensorflow')\n\nthat\u2019s okay - you\u2019ll install TensorFlow in the steps below. If you see\nother errors, please \nopen an issue on GitHub\n and we\u2019ll help!\n\n\nYou can run the \nGUILD_SOURCE_DIR/guild/scripts/guild\n executable\ndirectly (where \nGUILD_SOURCE_DIR\n is the location of your cloned\nGuild AI source repository) or modify your environment to make \nguild\n\navailable on your PATH using one of these methods:\n\n\n\n\nAdd \nGUILD_SOURCE_DIR/guild/scripts\n directory to your \nPATH\n environment\n  variable, OR\n\n\nCreate a symlink to \nGUILD_SOURCE_DIR/guild/scripts/guild\n that is\n  available on your PATH\n\n\n\n\nInstall TensorFlow\n\n\nGuild requires TensorFlow but does not install it for\nyou. \n1\n You can use \npip\n to install TensorFlow by running:\n\n\npip install tensorflow\n\n\n\n\nIf your system has a GPU, install the GPU enabled package by running:\n\n\npip install tensorflow-gpu\n\n\n\n\nFor alternative installation methods, refer to \nInstalling TensorFlow\n.\n\n\nInstall optional libraries\n\n\nIf you system has a GPU or other accelerator supported by TensorFlow,\nyou will need to install and configure support for your hardware.\n\n\nCUDA and cuDNN\n\n\nIf you have an NVIDIA GPU and and want to use the GPU enabled\nTensorFlow package, you must install the NVIDIA CUDA and cuDNN\nlibraries for your system. Refer to the links below for help\ninstalling the libraries.\n\n\n\n\nCUDA Toolkit Download\n\n\nNVIDIA cuDNN\n\n\n\n\nNVIDIA System Management Interface\n\n\nGuild uses NVIDIA System Management Interface (\nnvidia-smi\n) on GPU\naccelerated systems to collect GPU metrics. This tool is optional and\nGuild will run without it. However, to collect GPU stats on systems\nwith one or more GPUs, ensure that \nnvidia-smi\n is installed.\n\n\n\n\nNote\n\n\nNVIDIA System Management Interface is typically installed with NVIDIA\nGPU drivers. Refer to \nNVIDIA System Management Interface\n\nfor more information.\n\n\n\n\nVerify your installation\n\n\nVerify that Guild is installed properly by running the\n\ncheck\n command:\n\n\nguild check\n\n\n\n\nIf there are problems with your installation, Guild will display the\ndetails and exit with an error. Refer to\n\nTroubleshooting\n for assistance.\n\n\nNext steps\n\n\nCongratulations, you\u2019ve installed Guild AI! We\u2019ve outlined some next\nsteps for you below.\n\n\n\n\n\n\n\n\nTrain your first model\n\n\n\n\nDive in and train your first model using Guild AI. This introductory\ntutorial will walk you through the basics of Guild and cover most of\nits features.\n\n\n\n\nTrain your first model \n\n\n\n\n\n\n\n\n\n\n\nDiscover Guild models\n\n\n\n\nGuild AI provides a catalog of state-of-the-art TensorFlow models that\ncan be used to build deep learning applications. Start here to see\nwhat developers are building.\n\n\n\n\nDiscover Guild models \n\n\n\n\n\n\n\n\n\n\n\nBrowse the docs\n\n\n\n\nIf you're interested in a complete picture of Guild AI, start by\nbrowsing its comprehensives documentation.\n\n\n\n\nBrowse the docs \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow is a rapidly evolving software library and is provided as\nboth CPU and GPU supported packages. Guild leaves the specific package\nand version of TensorFlow up to the user.\u00a0\n\u21a9",
            "title": "Install"
        },
        {
            "location": "/install/#installing-guild-ai",
            "text": "Requirements  Install Guild AI  Install Guild AI with Virtualenv  Install Guild AI with Conda  Install Guild AI from source code    Install TensorFlow  Install optional libraries  CUDA and cuDNN  NVIDIA System Management Interface    Verify your installation  Next steps",
            "title": "Installing Guild AI"
        },
        {
            "location": "/install/#requirements",
            "text": "Guild AI has the following requirements:   Max OS, Linux  Python 2.7, Python 3  pip   Guild is installed from PyPI using  pip . Refer to  Installing pip  to ensure you have it\ninstalled.",
            "title": "Requirements"
        },
        {
            "location": "/install/#install-guild-ai",
            "text": "To install Guild AI, run the following on the command line:  pip install guildai  If you need to run  install  with administrative privileges, run:  sudo pip install guildai  You may alternatively install Guild AI in a Python virtual\nenvironment. Refer to the next sections for details.",
            "title": "Install Guild AI"
        },
        {
            "location": "/install/#install-guild-ai-with-virtualenv",
            "text": "If you would like to install Guild within a Python  virtual\nenvironment , run the following:  virtualenv guild\nsource guild/bin/activate\npip install guildai  This has the advantage of isolating Guild AI within a single\ndirectory. For more information on the advantages and disadvantages of\nthis approach, see  Working with\nVirtualenv .",
            "title": "Install Guild AI with Virtualenv"
        },
        {
            "location": "/install/#install-guild-ai-with-conda",
            "text": "Guild AI can be installed in  Conda  environments using pip.  conda create -n guild python=3.6\nsource activate guild\npip install guildai  If you\u2019d like to install Guild with a different Python version,\nreplace  3.6  above with the version you want.",
            "title": "Install Guild AI with Conda"
        },
        {
            "location": "/install/#install-guild-ai-from-source-code",
            "text": "Note  This step is an alternative to installing Guild AI with pip\ndescribed above. Install Guild AI from source code if you want a\nspecific version from GitHub (e.g. an early release or development\nbranch) or if you want to contribute to the project.   Additional required tools for installing from source code:   git  npm  v5.8.0 or later  Python development library and headers for your system   To install Guild from source, clone the repository by running:  git clone https://github.com/guildai/guild.git  Change to the  guild  directory and install the required pip packages:  cd guild\npip install -r requirements.txt  Build Guild by running:  python setup.py build  Verify Guild by running:  guild/scripts/guild check  If see the message  NOT INSTALLED (No module named 'tensorflow') \nthat\u2019s okay - you\u2019ll install TensorFlow in the steps below. If you see\nother errors, please  open an issue on GitHub  and we\u2019ll help!  You can run the  GUILD_SOURCE_DIR/guild/scripts/guild  executable\ndirectly (where  GUILD_SOURCE_DIR  is the location of your cloned\nGuild AI source repository) or modify your environment to make  guild \navailable on your PATH using one of these methods:   Add  GUILD_SOURCE_DIR/guild/scripts  directory to your  PATH  environment\n  variable, OR  Create a symlink to  GUILD_SOURCE_DIR/guild/scripts/guild  that is\n  available on your PATH",
            "title": "Install Guild AI from source code"
        },
        {
            "location": "/install/#install-tensorflow",
            "text": "Guild requires TensorFlow but does not install it for\nyou.  1  You can use  pip  to install TensorFlow by running:  pip install tensorflow  If your system has a GPU, install the GPU enabled package by running:  pip install tensorflow-gpu  For alternative installation methods, refer to  Installing TensorFlow .",
            "title": "Install TensorFlow"
        },
        {
            "location": "/install/#install-optional-libraries",
            "text": "If you system has a GPU or other accelerator supported by TensorFlow,\nyou will need to install and configure support for your hardware.",
            "title": "Install optional libraries"
        },
        {
            "location": "/install/#cuda-and-cudnn",
            "text": "If you have an NVIDIA GPU and and want to use the GPU enabled\nTensorFlow package, you must install the NVIDIA CUDA and cuDNN\nlibraries for your system. Refer to the links below for help\ninstalling the libraries.   CUDA Toolkit Download  NVIDIA cuDNN",
            "title": "CUDA and cuDNN"
        },
        {
            "location": "/install/#nvidia-system-management-interface",
            "text": "Guild uses NVIDIA System Management Interface ( nvidia-smi ) on GPU\naccelerated systems to collect GPU metrics. This tool is optional and\nGuild will run without it. However, to collect GPU stats on systems\nwith one or more GPUs, ensure that  nvidia-smi  is installed.   Note  NVIDIA System Management Interface is typically installed with NVIDIA\nGPU drivers. Refer to  NVIDIA System Management Interface \nfor more information.",
            "title": "NVIDIA System Management Interface"
        },
        {
            "location": "/install/#verify-your-installation",
            "text": "Verify that Guild is installed properly by running the check  command:  guild check  If there are problems with your installation, Guild will display the\ndetails and exit with an error. Refer to Troubleshooting  for assistance.",
            "title": "Verify your installation"
        },
        {
            "location": "/install/#next-steps",
            "text": "Congratulations, you\u2019ve installed Guild AI! We\u2019ve outlined some next\nsteps for you below.",
            "title": "Next steps"
        },
        {
            "location": "/docs/",
            "text": "Documentation\n\n\n\n\n\n\nConcepts\n\n\nOverview\nModels\nOperations\nRuns\nResources\nPackages\n\n\n\n\nVisual tools\n\n\nGuild View\nTensorBoard\n\n\n\n\nReference\n\n\nCommands\nGuild file reference\nGuild home reference\n\n\n\n\nPopular\n\n\nGo deeper with Guild AI\nTrain and predict with Cloud ML\nDevelop a model from scratch",
            "title": "Documentation"
        },
        {
            "location": "/docs/#documentation",
            "text": "",
            "title": "Documentation"
        },
        {
            "location": "/docs/#concepts",
            "text": "Overview Models Operations Runs Resources Packages",
            "title": "Concepts"
        },
        {
            "location": "/docs/#visual-tools",
            "text": "Guild View TensorBoard",
            "title": "Visual tools"
        },
        {
            "location": "/docs/#reference",
            "text": "Commands Guild file reference Guild home reference",
            "title": "Reference"
        },
        {
            "location": "/docs/#popular",
            "text": "Go deeper with Guild AI Train and predict with Cloud ML Develop a model from scratch",
            "title": "Popular"
        },
        {
            "location": "/docs/overview/",
            "text": "Guild AI overview\n\n\n\n\nGuild files\n\n\nPackages\n\n\nModels\n\n\nOperations\n\n\nRuns\n\n\nResources\n\n\n\n\nGuild AI is a toolkit that streamlines TensorFlow and Keras model\ndevelopment. It provides features that let you train, evaluate, and\nrelease deep learning models with less time and with fewer errors.\n\n\nHere\u2019s a training operation in Guild AI:\n\n\nguild train resnet-50\n\n\n\n\nWhen you train a model using Guild, a number of important steps are\nperformed for you:\n\n\n\n\n\n\nGuild creates a unique run directory to capture the results of the\n  training experiment.\n\n\n\n\n\n\nGuild saves details about the operation, including the model,\n  hyperparameters, environment, and command.\n\n\n\n\n\n\nGuild directs files generated by the training operation, such as\n  TensorFlow event logs and saved models, to the run directory.\n\n\n\n\n\n\nAs a result, each time you perform a training operation on a model,\nyou retain the results as a unique experiment.\n\n\nGuild provides tools for evaluating your experiments:\n\n\n\n\n\n\nCompare training loss, validation accuracy, and other metrics across\n  runs.\n\n\n\n\n\n\nUse TensorBoard to evaluate metrics generated during the training\n  process.\n\n\n\n\n\n\nBrowse training artifacts.\n\n\n\n\n\n\nAnnotate runs to capture your ideas.\n\n\n\n\n\n\nWhen you\u2019re happy with a trained model, its artifacts are available to\ndeploy to production. If it\u2019s Cloud ML ready, you can deploy it to the\ncloud with a single command.\n\n\nGuild files\n\n\nGuild makes use of Guild files, provide information needed to run\noperations on a model.\n\n\nGuild files are always named \nguild.yml\n.\n\n\nHere\u2019s a simple example:\n\n\nmodel: cifar10-cnn\ndescription: CNN classifier for CIFAR10 dataset\noperations:\n  train:\n    description: Train the CNN\n    cmd: train_cnn\n    flags:\n      epochs: 10000\n      batch-size: 128\n\n\n\n\nThis tells Guild that the \ntrain\n operation is implemented by a Python\nscript named \ntrain_cnn.py\n and that it accepts two flags: \nepochs\n\nand \nbatch-size\n.\n\n\nFrom a command line, a user can train the CIFAR10 CNN classifier by\nrunning:\n\n\nguild train cifar10-cnn epochs=1000 batch-size=64\n\n\n\n\nThis command has the effect of running \ntrain_cnn.py\n with the\nspecified flag values as a distinct run, which can be evaluated during\nor after the operation.\n\n\nGuild\nfile reference\n\n\nPackages\n\n\nPackages are containers for \nmodels\n and required\n\nresources\n. They are the basis of deep learning model\ndistribution in Guild AI.\n\n\nYou can discover packages \nonline\n or by using Guild\u2019s\n\nsearch\n command.\n\n\nMore about packages\n\n\nModels\n\n\nGuild models correspond to machine learning models. They\u2019re defined in\nGuild files with their \noperations\n, which can be run to perform\ntraining, evaluation, and other model related work.\n\n\nMore about models\n\n\nOperations\n\n\nOperations are \nactions\n that you can perform on a model. The most\ncommon operation in deep learning is \ntrain\n, which optimizes a model\nusing data. But operations can be anything at all!\n\n\nExamples of operations that a model might provide include:\n\n\n\n\ntrain\n\n\nTrain a model \u2014 nearly every model supports it!\n\n\nfinetune\n\n\nfinetune a model using a pretrained model as a starting point.\n\n\nevaluate\n\n\nMeasure model performance using a test dataset.\n\n\noptimize\n\n\nOptimize a model for a target environment (e.g. compress the model\n  size for mobile applications)\n\n\ndeploy\n\n\nDeploy a model to a target environment.\n\n\n\n\nMore about\noperations\n\n\nRuns\n\n\nA \nrun\n is generated each time you start an operation with the\n\nrun\n command. Runs preserves the outcome of each experiment,\nallowing you to analyze your results and make informed decisions about\nnext steps.\n\n\nMore about runs \n\n\nResources\n\n\nResources are files and software libraries required by\noperations. Models and packages may provide resources, which Guild\nmakes available to operations during each run.\n\n\nMore about resources",
            "title": "Overview"
        },
        {
            "location": "/docs/overview/#guild-ai-overview",
            "text": "Guild files  Packages  Models  Operations  Runs  Resources   Guild AI is a toolkit that streamlines TensorFlow and Keras model\ndevelopment. It provides features that let you train, evaluate, and\nrelease deep learning models with less time and with fewer errors.  Here\u2019s a training operation in Guild AI:  guild train resnet-50  When you train a model using Guild, a number of important steps are\nperformed for you:    Guild creates a unique run directory to capture the results of the\n  training experiment.    Guild saves details about the operation, including the model,\n  hyperparameters, environment, and command.    Guild directs files generated by the training operation, such as\n  TensorFlow event logs and saved models, to the run directory.    As a result, each time you perform a training operation on a model,\nyou retain the results as a unique experiment.  Guild provides tools for evaluating your experiments:    Compare training loss, validation accuracy, and other metrics across\n  runs.    Use TensorBoard to evaluate metrics generated during the training\n  process.    Browse training artifacts.    Annotate runs to capture your ideas.    When you\u2019re happy with a trained model, its artifacts are available to\ndeploy to production. If it\u2019s Cloud ML ready, you can deploy it to the\ncloud with a single command.",
            "title": "Guild AI overview"
        },
        {
            "location": "/docs/overview/#guild-files",
            "text": "Guild makes use of Guild files, provide information needed to run\noperations on a model.  Guild files are always named  guild.yml .  Here\u2019s a simple example:  model: cifar10-cnn\ndescription: CNN classifier for CIFAR10 dataset\noperations:\n  train:\n    description: Train the CNN\n    cmd: train_cnn\n    flags:\n      epochs: 10000\n      batch-size: 128  This tells Guild that the  train  operation is implemented by a Python\nscript named  train_cnn.py  and that it accepts two flags:  epochs \nand  batch-size .  From a command line, a user can train the CIFAR10 CNN classifier by\nrunning:  guild train cifar10-cnn epochs=1000 batch-size=64  This command has the effect of running  train_cnn.py  with the\nspecified flag values as a distinct run, which can be evaluated during\nor after the operation.  Guild\nfile reference",
            "title": "Guild files"
        },
        {
            "location": "/docs/overview/#packages",
            "text": "Packages are containers for  models  and required resources . They are the basis of deep learning model\ndistribution in Guild AI.  You can discover packages  online  or by using Guild\u2019s search  command.  More about packages",
            "title": "Packages"
        },
        {
            "location": "/docs/overview/#models",
            "text": "Guild models correspond to machine learning models. They\u2019re defined in\nGuild files with their  operations , which can be run to perform\ntraining, evaluation, and other model related work.  More about models",
            "title": "Models"
        },
        {
            "location": "/docs/overview/#operations",
            "text": "Operations are  actions  that you can perform on a model. The most\ncommon operation in deep learning is  train , which optimizes a model\nusing data. But operations can be anything at all!  Examples of operations that a model might provide include:   train  Train a model \u2014 nearly every model supports it!  finetune  finetune a model using a pretrained model as a starting point.  evaluate  Measure model performance using a test dataset.  optimize  Optimize a model for a target environment (e.g. compress the model\n  size for mobile applications)  deploy  Deploy a model to a target environment.   More about\noperations",
            "title": "Operations"
        },
        {
            "location": "/docs/overview/#runs",
            "text": "A  run  is generated each time you start an operation with the run  command. Runs preserves the outcome of each experiment,\nallowing you to analyze your results and make informed decisions about\nnext steps.  More about runs",
            "title": "Runs"
        },
        {
            "location": "/docs/overview/#resources",
            "text": "Resources are files and software libraries required by\noperations. Models and packages may provide resources, which Guild\nmakes available to operations during each run.  More about resources",
            "title": "Resources"
        },
        {
            "location": "/docs/models/",
            "text": "Models\n\n\n\n\nCreate a model\n\n\nInstall models\n\n\nGet model help\n\n\nList models\n\n\nUninstall models\n\n\n\n\nA model in Guild AI is a representation of a machine learning model. A\nmodel may be as simple, like a linear regression, or complex, like a\ndeep neural network.\n\n\nModels in Guild are characterized by their support of\n\noperations\n, which are actions that can be performed\non the model. One of the most common model operations is \ntrain\n,\nwhich iteratively modifies model parameters to optimize the model\u2019s\npredictive or generative capability. Other common operations include\n\nevaluate\n, \npredict\n, \ndeploy\n, and \ngenerate\n.\n\n\nGuild does not implement model operations. That task is the\nresponsibility of the model developer. Guild instead manages the work\nflow associated with running operations and tracking results.\n\n\nConsider the following Guild command:\n\n\nguild train mnist\n\n\n\n\nWhen this command is run, Guild will look for a model named \nmnist\n\nand then for an operation associated with that model named\n\ntrain\n. With this information, Guild will start a \nrun\n,\nwhich is a Python session used to train the model. Guild will track\nsave run metadata and output so that you can access it later.\n\n\nGuild looks for models in \nGuild files\n, which are\nfiles named \nguild.yml\n that contain model definitions.\n\n\nHere\u2019s what a \nguild.yml\n that defines an \nmnist\n model might look\nlike:\n\n\nmodel: mnist\noperations:\n  train:\n    cmd: train_mnist\n\n\n\n\nThis file tells Guild that the model \nmnist\n has an operation \ntrain\n\nthat is performed by running a Python module \ntrain_mnist\n.\n\n\nHere\u2019s what \ntrain_mnist.py\n \u2014 the training module source file \u2014\nmight look like:\n\n\nimport mnist_data\nimport mnist_model\n\ndef train():\n    data = mnist_data.load()\n    model = mnist_model.init()\n    model.train(data)\n\nif __name__ == \"__main__\":\n    train()\n\n\n\n\nModels in Guild are very flexible and can be used to manage any type\nof operation associated with the model.\n\n\nCreate a model\n\n\nTo create a model, create a file named \nguild.yml\n in your model\nproject directory.\n\n\n\n\nTip\n\n\nIf you\u2019re starting from scratch, use the \ninit\n command\nto initialize a new project, including a sample training script.\n\n\n\n\nUsing a text editor, paste the following code sample into \nguild.yml\n:\n\n\n- model: my-model\n  description: My model\n  operations:\n    train:\n      cmd: train_my_model\n\n\n\n\n\n\n\n\nReplace the values for \nmodel\n and \ndescription\n to reflect the\n  attributes of your model.\n\n\n\n\n\n\nReplace \ntrain_my_model\n with the Python module used to train your\n  model.\n\n\n\n\n\n\nSave \nguild.yml\n.\n\n\n\n\n\n\nFor more information on defining models and operations, see \nGuile\nfile reference\n.\n\n\nFor a detailed step-by-step example of creating a Guild model, see\n\nDevelop a model from\nscratch\n.\n\n\nYou may experiment with training your model by running:\n\n\nguild train\n\n\n\n\nFor information on working with runs generated by operations, see\n\nRuns\n.\n\n\nInstall models\n\n\nGuild AI is a model packaging and distribution system that can be used\nto install models from the deep learning ecosystem.\n\n\nYou install models by installing Guild packages that contain the\nmodels.\n\n\nTo install a package, run:\n\n\nguild install PACKAGE\n\n\n\n\nYou can lookup packages by running:\n\n\nguild search TERM\n\n\n\n\nFor example, to install a package that contains ResNet models, search\nfor \nresnet\n by running:\n\n\nguild search resnet\n\n\n\n\nYou can also browse \nGuild AI models\n.\n\n\nAfter reviewing the list of available packages, if you decide to\ninstall \ntensorflow.reset\n, you can install it by running:\n\n\nguild install tensorflow.resnet\n\n\n\n\nFor more information, see \nInstall\npackages\n and the \ninstall\n\ncommand.\n\n\nGet model help\n\n\nTo view help for models defined in the current directory (i.e. models\ndefined in \n./guild.yml\n) run:\n\n\nguild help\n\n\n\n\nTo view help for models defined in another directory, run:\n\n\nguild help DIRECTORY\n\n\n\n\n\n\nNote\n\n\nDIRECTORY\n must contain a Guild file (i.e. \nguild.yml\n).\n\n\n\n\nTo view help for models defined in a package, run:\n\n\nguild help PACKAGE\n\n\n\n\nFor example, to view help for the models provided by\n\ntensorflow.resnet\n, run:\n\n\nguild help tensorflow.resnet\n\n\n\n\nFor more information, see the \nhelp\n command.\n\n\nList models\n\n\nList available models by running:\n\n\nguild models\n\n\n\n\nYou can filter the results using a term. For example, to list\navailable models that contain \ncnn\n, run:\n\n\nguild models cnn\n\n\n\n\nFor more information, see the \nmodels\n command.\n\n\nUninstall models\n\n\nYou can remove models defined in packages by uninstalling the\npackages.\n\n\nguild uninstall PACKAGE\n\n\n\n\nThe package that a model is associated with is displayed when you run\n\nguild models\n in the form \nPACKAGE/MODEL\n.\n\n\nFor more information, see \nUninstall\npackages\n and the\n\nuninstall\n command.",
            "title": "Models"
        },
        {
            "location": "/docs/models/#models",
            "text": "Create a model  Install models  Get model help  List models  Uninstall models   A model in Guild AI is a representation of a machine learning model. A\nmodel may be as simple, like a linear regression, or complex, like a\ndeep neural network.  Models in Guild are characterized by their support of operations , which are actions that can be performed\non the model. One of the most common model operations is  train ,\nwhich iteratively modifies model parameters to optimize the model\u2019s\npredictive or generative capability. Other common operations include evaluate ,  predict ,  deploy , and  generate .  Guild does not implement model operations. That task is the\nresponsibility of the model developer. Guild instead manages the work\nflow associated with running operations and tracking results.  Consider the following Guild command:  guild train mnist  When this command is run, Guild will look for a model named  mnist \nand then for an operation associated with that model named train . With this information, Guild will start a  run ,\nwhich is a Python session used to train the model. Guild will track\nsave run metadata and output so that you can access it later.  Guild looks for models in  Guild files , which are\nfiles named  guild.yml  that contain model definitions.  Here\u2019s what a  guild.yml  that defines an  mnist  model might look\nlike:  model: mnist\noperations:\n  train:\n    cmd: train_mnist  This file tells Guild that the model  mnist  has an operation  train \nthat is performed by running a Python module  train_mnist .  Here\u2019s what  train_mnist.py  \u2014 the training module source file \u2014\nmight look like:  import mnist_data\nimport mnist_model\n\ndef train():\n    data = mnist_data.load()\n    model = mnist_model.init()\n    model.train(data)\n\nif __name__ == \"__main__\":\n    train()  Models in Guild are very flexible and can be used to manage any type\nof operation associated with the model.",
            "title": "Models"
        },
        {
            "location": "/docs/models/#create-a-model",
            "text": "To create a model, create a file named  guild.yml  in your model\nproject directory.   Tip  If you\u2019re starting from scratch, use the  init  command\nto initialize a new project, including a sample training script.   Using a text editor, paste the following code sample into  guild.yml :  - model: my-model\n  description: My model\n  operations:\n    train:\n      cmd: train_my_model    Replace the values for  model  and  description  to reflect the\n  attributes of your model.    Replace  train_my_model  with the Python module used to train your\n  model.    Save  guild.yml .    For more information on defining models and operations, see  Guile\nfile reference .  For a detailed step-by-step example of creating a Guild model, see Develop a model from\nscratch .  You may experiment with training your model by running:  guild train  For information on working with runs generated by operations, see Runs .",
            "title": "Create a model"
        },
        {
            "location": "/docs/models/#install-models",
            "text": "Guild AI is a model packaging and distribution system that can be used\nto install models from the deep learning ecosystem.  You install models by installing Guild packages that contain the\nmodels.  To install a package, run:  guild install PACKAGE  You can lookup packages by running:  guild search TERM  For example, to install a package that contains ResNet models, search\nfor  resnet  by running:  guild search resnet  You can also browse  Guild AI models .  After reviewing the list of available packages, if you decide to\ninstall  tensorflow.reset , you can install it by running:  guild install tensorflow.resnet  For more information, see  Install\npackages  and the  install \ncommand.",
            "title": "Install models"
        },
        {
            "location": "/docs/models/#get-model-help",
            "text": "To view help for models defined in the current directory (i.e. models\ndefined in  ./guild.yml ) run:  guild help  To view help for models defined in another directory, run:  guild help DIRECTORY   Note  DIRECTORY  must contain a Guild file (i.e.  guild.yml ).   To view help for models defined in a package, run:  guild help PACKAGE  For example, to view help for the models provided by tensorflow.resnet , run:  guild help tensorflow.resnet  For more information, see the  help  command.",
            "title": "Get model help"
        },
        {
            "location": "/docs/models/#list-models",
            "text": "List available models by running:  guild models  You can filter the results using a term. For example, to list\navailable models that contain  cnn , run:  guild models cnn  For more information, see the  models  command.",
            "title": "List models"
        },
        {
            "location": "/docs/models/#uninstall-models",
            "text": "You can remove models defined in packages by uninstalling the\npackages.  guild uninstall PACKAGE  The package that a model is associated with is displayed when you run guild models  in the form  PACKAGE/MODEL .  For more information, see  Uninstall\npackages  and the uninstall  command.",
            "title": "Uninstall models"
        },
        {
            "location": "/docs/operations/",
            "text": "Operations\n\n\n\n\nRun an operation\n\n\nOperation aliases\n\n\n\n\n\n\nGet operation help\n\n\nList operations\n\n\nFlags\n\n\nRequired resources\n\n\nImplementing an operation\n\n\n\n\nAn operation is an action performed on a \nmodel\n. When run, an\noperation generates a \nrun\n, which is persistent record of the\noperation.\n\n\nExamples of model operations include:\n\n\n\n\ntrain\n\n\nTrain a model\n\n\nevaluate\n\n\nEvaluate a trained model\n\n\nfinetune\n\n\nFine tune a pretrained model\n\n\nprepare\n\n\nPrepare a dataset for use in training\n\n\ngenerate\n\n\nUse a model to generate content\n\n\n\n\nWhile these operations are commonly used, model developers are free to\ndefine different operations as needed. For example, if a model\nsupports compression (e.g. by using quantization), it might define a\n\ncompress\n operation.\n\n\nRun an operation\n\n\nTo run an operation, use the \nrun\n command:\n\n\nguild run OPERATION [ARG...]\n\n\n\n\nOPERATION\n must include the complete operation name and may also\ninclude package and model information to disambiguate the operation.\n\n\nTo specify the model along with the operation name, use\n\nMODEL:OPERATION\n. For example, to run the \nprepare\n operation on a\nmodel named \niris-dataset\n, you would run:\n\n\nguild run iris-dataset:prepare\n\n\n\n\nFor more information, see the \nrun\n command.\n\n\nOperation aliases\n\n\nSome operations are so common that Guild provides\n\naliases\n for them. Aliases let you run commands\nthis way:\n\n\nguild OPERATION_ALIAS [MODEL] [ARG...]\n\n\n\n\nThe following operation aliases are supported:\n\n\n\n\ntrain\n\n\n\n\nFor example, to run the \ntrain\n operation on a model, use:\n\n\nguild train MODEL\n\n\n\n\nThis command is equivalent to running:\n\n\nguild run MODEL:train\n\n\n\n\nGet operation help\n\n\nOperation help is displayed for model help when run \nguild\nhelp\n. See \nGet model help\n and the\n\nhelp\n command for more information on general model help.\n\n\nYou can get help for a specific operation using the \n--help-op\n\noption with the \nrun\n command (or an operation alias).\n\n\nOperation help includes the list of flags you can specify for an\noperation. This is useful when you have started to type a run command\nand want help on available or required flags.\n\n\nFor example, to view operation help for the \ntrain\n operation, run:\n\n\nguild train --help-op\n\n\n\n\nList operations\n\n\nTo list available operations, run:\n\n\nguild operations\n\n\n\n\nFor more information, see the \noperations\n command.\n\n\nFlags\n\n\nFlags are operation parameters and are used to specify the behavior of\nan operation for a run.\n\n\nFlags are defined by the model operation. For more information on flag\ndefinitions, see \nFlags\n in the\nGuild file reference.\n\n\nFlag values are specified using \nNAME=VALUE\n arguments to the\n\nrun\n command (or operation alias).\n\n\nFor example, consider the operation help for\n\nkeras.mnist/mnist-mlp:train\n, which we can show by running:\n\n\nguild run mlp:train --help-op\n\n\n\n\nUsage: guild run [OPTIONS] mnist-mlp:train [FLAG]...\n\nTrain the MLP\n\nUse 'guild run --help' for a list of options.\n\nFlags:\n  batch-size  Training batch size (default is 128)\n  epochs      Number of epochs to train (default is 20)\n\n\n\n\nAs described in the operation help, \nmnist-mlp:train\n supports two\nflags: \nbatch-size\n and \nepochs\n. If we wanted to train the model over\n\n10\n epochs using a batch size of \n64\n, we would use:\n\n\nguild train batch-size=64 epochs=10\n\n\n\n\nRequired resources\n\n\nOperations may require \nresources\n. Required resources\nare listed in the operation\u2019s \nrequires\n attribute.\n\n\nWhen Guild starts an operation, it first resolves each required\nresource. If a resource cannot be resolved, the operation fails with\nan error message.\n\n\nResources are resolved by acquiring them (e.g. download a file from\nthe Internet), verifying them, and finally creating links to resources\nfiles in the run directory. In this way, operations can easily express\n\u201cI need these files to run\u201d and ensure that the correct files are\navailable for each run.\n\n\nIn most cases resources are automatically resolved, but in some cases\nan operation may require that the user specify a resource. Resources\ncan be specified the same way flag values are specified \u2014 using\n\nNAME=VALUE\n. In the case of a resource, \nVALUE\n is the name of the\nrequired resource.\n\n\nRequired resources are described in operation help, if applicable.\n\n\nImplementing an operation\n\n\nOperations are implemented in Python modules. If \nmain\n is specified,\nthe module must execute when loaded, and should use this pattern:\n\n\ndef main():\n    \"Operation code here.\"\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\nOperations are executed in the context of the current run directory.\n\n\n\n\n\nOperations have access to a number of environment variables.\n\n\n\n\nCMD_DIR\n\n\nPath where the operation was run. This is the original working\n  directory that was changed to \nRUN_DIR\n for the operation.\n\n\nGUILD_HOME\n\n\nGuild install location.\n\n\nGUILD_OP\n\n\nName of the operation including the model.\n\n\nGUILD_PLUGINS\n\n\nComma separated list of active Guild plugins.\n\n\nLOG_LEVEL\n\n\nPython log level active for the run.\n\n\nMODEL_DIR\n\n\nThe directory containing the operation model definition. This is\n  where the Guild file is located and can be used to reference\n  relative files.\n\n\nRUN_DIR\n\n\nActive run directory path. This is the working directory during an\n  operation. See \nCMD_DIR\n for the original working directory -\n  i.e. where the operation was run from.\n\n\nRUN_ID\n\n\nActive run ID.",
            "title": "Operations"
        },
        {
            "location": "/docs/operations/#operations",
            "text": "Run an operation  Operation aliases    Get operation help  List operations  Flags  Required resources  Implementing an operation   An operation is an action performed on a  model . When run, an\noperation generates a  run , which is persistent record of the\noperation.  Examples of model operations include:   train  Train a model  evaluate  Evaluate a trained model  finetune  Fine tune a pretrained model  prepare  Prepare a dataset for use in training  generate  Use a model to generate content   While these operations are commonly used, model developers are free to\ndefine different operations as needed. For example, if a model\nsupports compression (e.g. by using quantization), it might define a compress  operation.",
            "title": "Operations"
        },
        {
            "location": "/docs/operations/#run-an-operation",
            "text": "To run an operation, use the  run  command:  guild run OPERATION [ARG...]  OPERATION  must include the complete operation name and may also\ninclude package and model information to disambiguate the operation.  To specify the model along with the operation name, use MODEL:OPERATION . For example, to run the  prepare  operation on a\nmodel named  iris-dataset , you would run:  guild run iris-dataset:prepare  For more information, see the  run  command.",
            "title": "Run an operation"
        },
        {
            "location": "/docs/operations/#operation-aliases",
            "text": "Some operations are so common that Guild provides aliases  for them. Aliases let you run commands\nthis way:  guild OPERATION_ALIAS [MODEL] [ARG...]  The following operation aliases are supported:   train   For example, to run the  train  operation on a model, use:  guild train MODEL  This command is equivalent to running:  guild run MODEL:train",
            "title": "Operation aliases"
        },
        {
            "location": "/docs/operations/#get-operation-help",
            "text": "Operation help is displayed for model help when run  guild\nhelp . See  Get model help  and the help  command for more information on general model help.  You can get help for a specific operation using the  --help-op \noption with the  run  command (or an operation alias).  Operation help includes the list of flags you can specify for an\noperation. This is useful when you have started to type a run command\nand want help on available or required flags.  For example, to view operation help for the  train  operation, run:  guild train --help-op",
            "title": "Get operation help"
        },
        {
            "location": "/docs/operations/#list-operations",
            "text": "To list available operations, run:  guild operations  For more information, see the  operations  command.",
            "title": "List operations"
        },
        {
            "location": "/docs/operations/#flags",
            "text": "Flags are operation parameters and are used to specify the behavior of\nan operation for a run.  Flags are defined by the model operation. For more information on flag\ndefinitions, see  Flags  in the\nGuild file reference.  Flag values are specified using  NAME=VALUE  arguments to the run  command (or operation alias).  For example, consider the operation help for keras.mnist/mnist-mlp:train , which we can show by running:  guild run mlp:train --help-op  Usage: guild run [OPTIONS] mnist-mlp:train [FLAG]...\n\nTrain the MLP\n\nUse 'guild run --help' for a list of options.\n\nFlags:\n  batch-size  Training batch size (default is 128)\n  epochs      Number of epochs to train (default is 20)  As described in the operation help,  mnist-mlp:train  supports two\nflags:  batch-size  and  epochs . If we wanted to train the model over 10  epochs using a batch size of  64 , we would use:  guild train batch-size=64 epochs=10",
            "title": "Flags"
        },
        {
            "location": "/docs/operations/#required-resources",
            "text": "Operations may require  resources . Required resources\nare listed in the operation\u2019s  requires  attribute.  When Guild starts an operation, it first resolves each required\nresource. If a resource cannot be resolved, the operation fails with\nan error message.  Resources are resolved by acquiring them (e.g. download a file from\nthe Internet), verifying them, and finally creating links to resources\nfiles in the run directory. In this way, operations can easily express\n\u201cI need these files to run\u201d and ensure that the correct files are\navailable for each run.  In most cases resources are automatically resolved, but in some cases\nan operation may require that the user specify a resource. Resources\ncan be specified the same way flag values are specified \u2014 using NAME=VALUE . In the case of a resource,  VALUE  is the name of the\nrequired resource.  Required resources are described in operation help, if applicable.",
            "title": "Required resources"
        },
        {
            "location": "/docs/operations/#implementing-an-operation",
            "text": "Operations are implemented in Python modules. If  main  is specified,\nthe module must execute when loaded, and should use this pattern:  def main():\n    \"Operation code here.\"\n\nif __name__ == \"__main__\":\n    main()  Operations are executed in the context of the current run directory.   Operations have access to a number of environment variables.   CMD_DIR  Path where the operation was run. This is the original working\n  directory that was changed to  RUN_DIR  for the operation.  GUILD_HOME  Guild install location.  GUILD_OP  Name of the operation including the model.  GUILD_PLUGINS  Comma separated list of active Guild plugins.  LOG_LEVEL  Python log level active for the run.  MODEL_DIR  The directory containing the operation model definition. This is\n  where the Guild file is located and can be used to reference\n  relative files.  RUN_DIR  Active run directory path. This is the working directory during an\n  operation. See  CMD_DIR  for the original working directory -\n  i.e. where the operation was run from.  RUN_ID  Active run ID.",
            "title": "Implementing an operation"
        },
        {
            "location": "/docs/runs/",
            "text": "Runs\n\n\n\n\nOverview\n\n\nConcepts\n\n\nRun directory\n\n\nLimiting runs\n\n\nRun scope\n\n\nRun filtering\n\n\n\n\n\n\nSelecting runs\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nStart a run\n\n\nOperation aliases\n\n\n\n\n\n\nFlag values\n\n\nList runs\n\n\nGet run information\n\n\nCompare runs\n\n\nLabel runs\n\n\nDelete runs\n\n\nFrequently used delete commands\n\n\n\n\n\n\nRestore deleted runs\n\n\nPurge deleted runs\n\n\n\n\nOverview\n\n\nRuns are generated in Guild AI by running an \noperation\n.\n\n\nWhen you train a model, you generate a \nrun\n, which contains the\ntrained model as well as training logs and other artifacts associated\nwith the operation.\n\n\nSimilarly, when you fine tune a model, you generate a run. When you\ntest a model, you generate a run. In fact, any operation that you run\ngenerates a distinct run. This is how Guild manages your work.\n\n\nHere is a common work flow:\n\n\n\n\nFind and install a model\n\n\nRun an operation on that model (e.g. \ntrain\n)\n\n\nMonitor the progress of the operation (e.g. \nview\n)\n\n\nRun another operation with different hyper-parameters (flags)\n\n\nCompare runs\n\n\nDelete runs that you\u2019re no longer interested in\n\n\nSelect successful runs for deployment or use in other operations\n\n\n\n\nThe work centers on \nruns\n \u2014 creating, comparing, and selecting.\n\n\nConcepts\n\n\nAs you work with runs in Guild it\u2019s important to understand some core\nconcepts. If you\u2019d prefer to skip this conceptual material, jump to\n\nStart a run\n below.\n\n\nRun directory\n\n\nA \nrun directory\n is a file system directory (or folder) that contains\nartifacts associated with a run. Guild creates a unique run directory\nfor every run. This directory contains a variety of important data:\n\n\n\n\nRun metadata\n\n\nRun sources such as datasets\n\n\nRun output such as event logs and saved models\n\n\n\n\nRun directories are located in \nGUILD_HOME/runs\n. For more information\nsee \nGuild home\n.\n\n\nRun related operations interact with run directories in various ways:\n\n\n\n\nguild run\n creates a new run directory\n\n\nguild runs info\n prints information read from a run directory\n\n\nguild runs list\n enumerates run directories\n\n\nguild runs delete\n deletes run directories\n\n\n\n\nLimiting runs\n\n\nOver time you\u2019ll generate a large number of runs. This list can become\nunwieldy, especially when you\u2019re interested in a small subset \u2014\ne.g. runs associated with a particular model you\u2019re working with. For\nthis reason, Guild provides two ways of limiting the runs that apply\nto run related commands:\n\n\n\n\nLimit to runs associated with a model defined in the current\n  directory\n\n\nLimit to runs that match a filter\n\n\n\n\nRun scope\n\n\nThe first limit is known as \nrun scope\n. Scope can be either \nlocal\n\nor \nglobal\n. By default, scope is local when the current directory\ncontains a \nmodel definition\n, otherwise scope is\nglobal. Local scope limits runs to those associated with models\ndefined in the current directory. Global scope displays all runs.\n\n\nGlobal scope can be applied using the \n--all\n (or \n-a\n) option.\n\n\nRun scope is applied based on the directory that Guild commands are\nrun in. Consider the following directory structure:\n\n\n\n\n\n\nHome\nDoes not contain a model definition \u2014 global scope applies\n\n\nModels\n \n\n \nmnist \nContains a model definition \u2014 local scope applies\n\n  \n\n  \nMODELS \nModel definition\n\n  \n\n \n\n \n\n\n\n\n\n\n\n\n\nCommands run the from \n/Home\n have \nglobal\n run scope because \n/Home\n\ndoesn\u2019t contain a model definition. Commands run from \n/Models/mnist\n\nhowever have \nlocal\n scope because that directory contains a model\ndefinition (\n/Models/mnist/MODELS\n).\n\n\nRun scope defaults to \nlocal\n when a model definition is exists\nbecause Guild assumes that the user is working on models defined at\nthat location and is not interested in other runs, at least by\ndefault. This follows the pattern of command line tools such as \ngit\n\nthat apply operations locally when they find a project, repository,\netc. in the current directory.\n\n\nWhen a command is run in local scope, Guild prints a message to\nindicate that results are limited:\n\n\nLimiting runs to the current directory (use --all to include all)\n\n\n\nRun filtering\n\n\nThe other limit is \nrun filtering\n. Filters are applied with command\nline options that specify run attributes, which may include:\n\n\n\n\nOperation\n\n\nRun status\n\n\nDeleted status\n\n\n\n\nRun filtering is applied \nafter\n run scope (see above).\n\n\nFor example, to view runs that are associated with the \ntrain\n\noperation, use the \n--op\n (or \n-o\n) option:\n\n\nguild runs --op train\n\n\n\n\nIf the command is in local scope, Guild will limit runs to those\nassociated with models in the current directory otherwise it will use\nall runs. It will then filter those runs, limiting the result to those\nassociated with operations containing the string \u201ctrain\u201d.\n\n\nSelecting runs\n\n\nSome run related commands let you select one or more runs:\n\n\n\n\nruns delete\nruns info\nruns purge\nruns restore\n\n\n\n\nFor these commands, runs can be specified in various ways:\n\n\n\n\nIndex as returned by \nguild runs\n or \nguild runs list\n\n\nRun ID (full or partial if unique)\n\n\n\n\nAdditionally, a range may be specified using run indexes in the form:\n\n\n[START]:[STOP]\n\n\n\nSTOP\n and \nSTART\n are inclusive \u2014 runs are selected beginning with\nthe \nSTOP\n index up to and including those with the \nSTART\n index.\n\n\nBoth \nSTOP\n and \nSTART\n are optional. If \nSTART\n is omitted it is\nassumed to be \n0\n (i.e. the first run in the list). If \nSTOP\n is\nomitted it is assumed to be the index of the last run.\n\n\n\n\nImportant\n\n\nRun indexes are relative to the list of runs returned by \nguild\nruns\n or \nguild runs list\n for a given scope and filter (see\n\nLimiting runs\n above). The run associated with index\n\n0\n for one listing may not be the same run for another\nlisting. Always verify the selected runs before proceeding with a\ncommand.\n\n\nWhen in doubt, use a run ID to select a run.\n\n\n\n\nExamples\n\n\nConsider this output from \nguild runs\n:\n\n\nLimiting runs to the current directory (use --all to include all)\n[0:9734f85e]   ./slim-resnet-101:train        2017-12-14 07:56:32  terminated\n[1:d8cde0fc]   ./slim-resnet-50:export        2017-12-13 13:14:31  completed\n[2:0df943ac]   ./slim-resnet-50:predict       2017-12-06 11:51:15  completed\n[3:e150e44a]   ./slim-resnet-50:predict       2017-12-06 11:50:00  completed\n\n\n\n\n\n\nNote\n\n\nThe \nrun scope\n in the above command is \nlocal\n. If\nthe user had run \nguild runs --all\n the scope would be \nglobal\n \u2014\nthe list and run indexes would likely be different.\n\n\n\n\nBelow are various operations with run selectors applied to this\nlist.\n\n\n\n\nguild runs rm 0\n\n\nDelete run \n9734f85e\n (you can always use index \n0\n to select the\n  most recently started run in the list)\n\n\nguild runs rm 1:2\n\n\nDelete runs \nd8cde0fc\n and \n0df943ac\n\n\nguild runs rm :\n\n\nDelete all runs\n\n\nguild runs rm 0df943ac e150e44a\n\n\nDelete runs \n0df943ac\n and \ne150e44a\n\n\n\n\n\n\nNote\n\n\nThe following assumptions must hold for the above examples that use\nrun indexes:\n\n\n\n\n\n\nCommands must be executed in the same directory as the command\n  that generated the list and without scope modifiers or filters\n\n\n\n\n\n\nThe runs themselves must not change \u2014 i.e. runs cannot be deleted\n  or started\n\n\n\n\n\n\n\n\nStart a run\n\n\nTo start a run, use the \nrun\n command. The basic format of a\n\nrun\n command looks like this:\n\n\nguild run OPERATION\nguild run MODEL:OPERATION\nguild run PACKAGE/MODEL:OPERATION\n\n\n\n\nYou can list available operations using the \noperations\n\ncommand.\n\n\nIn general, you can omit information about an operation name as long\nas Guild can uniquely identify the operation.\n\n\nFor example, if the output of \noperations\n looks like this:\n\n\niris/iris-cnn:train\niris/iris-cnn:finetune\niris/iris-cnn:test\n\n\n\n\nYou can start the \nfinetune\n operation by running:\n\n\nguild run finetune\n\n\n\n\nYou can always provide the model or package. For example, this form\nwill also start \nfinetune\n:\n\n\nguild run iris-cnn:finetune\n\n\n\n\nYou use part of the operation specification as long as Guild can\nuniquely identify the operation. For example, you can run the \ntest\n\non \niris-cnn\n using:\n\n\nguild run cnn:train\n\n\n\n\nOperation aliases\n\n\nSome operations are so common that Guild provides \nalias\n\ncommands. Aliases currently include:\n\n\n\n\ntrain\n\n\n\n\nAliases are used to start operation using these forms:\n\n\nguild ALIAS_CMD\nguild ALIAS_CMD MODEL\nguild ALIAS_CMD PACKAGE/MODEL\n\n\n\n\nThe \ntrain\n alias is used to run the \ntrain\n operation. In the example\nabove, the following commands can be used to train the iris model:\n\n\nguild train\nguild train iris-cnn\nguild train cnn\n\n\n\n\nFlag values\n\n\nSpecify operation flag values as \nNAME=VALUE\n arguments to \nrun\n.\n\n\nTo get help on available and required flags for an operation, run:\n\n\nguild run OPERATION --help-op\n\n\n\n\nYou can also view help for models defined in the current directory by\nrunning:\n\n\nguild help\n\n\n\n\nTo get help for a packaged model, run:\n\n\nguild help PACKAGE\n\n\n\n\nIf you omit a required flag, the \nrun\n command (or applicable alias)\nwill exit with an error message.\n\n\nList runs\n\n\nTo list Guild runs, use the \nruns\n or \nruns list\n\ncommand.\n\n\nguild runs\n is shorthand for \nguild runs list\n.\n\n\nWhen listing runs, be aware of \nrun scope\n and \nrun\nfiltering\n \u2014 these effect the runs that are\ndisplayed.\n\n\n\n\nguild runs\n\n\nList all runs with the run scope. If the current directory contain a\n  \nmodel definition\n the list is limited to runs\n  associated with the locally defined models, otherwise the list will\n  contain all runs.\n\n\nfoo\n\n\nAnother thing yo.\n\n\n\n\nThe command:\n\n\nguild runs\n\n\n\n\nwill display different lists depending on the directory it\u2019s run\nin. If the directory contains a model definition, runs will be limited\nto those associated with the locally defined models. If the directory\ndoes not contain a model definition, all runs are displayed.\n\n\nGet run information\n\n\nUse \nruns info\n to show information about a run.\n\n\nBy default, Guild shows information about the latest run:\n\n\nguild runs info\n\n\n\n\nYou can select a specific run by providing a run ID or index.\n\n\nRun indexes are displayed in run lists (see \nList runs\n\nabove).\n\n\nCompare runs\n\n\nCompare runs by running:\n\n\nguild compare\n\n\n\n\nGuild Compare is spreadsheet-like application that displays runs,\ntheir status, and metrics such as validation accuracy and training\nloss.\n\n\nTo display compare results as a table, use:\n\n\nguild compare --table\n\n\n\n\nTo display compare results in CSV format (e.g. for use in Excel), use:\n\n\nguild compare --csv\n\n\n\n\nFor more help, see the \ncompare\n command.\n\n\nLabel runs\n\n\nRuns can have \nlabels\n, which provide additional information about the\nrun. A label can used for filtering in the \nruns list\n\ncommand.\n\n\nUse \nruns label\n to set or clear a label for a run.\n\n\nUse \nguild runs list LABEL\n to list runs with the specified label.\n\n\nDelete runs\n\n\nDelete runs using \nguild runs delete\n or \nguild runs rm\n. See\n\nruns delete\n for command details.\n\n\nGuild will display the list of runs to be deleted and ask you to\nconfirm the operation. You must type \ny\n and then press \nENTER\n to\nconfirm.\n\n\nDeleted runs can be restored using the \nruns\nrestore\n command. Refer to \nRestoring deleted\nruns\n below for details.\n\n\nFrequently used delete commands\n\n\nTo delete all failed runs, use:\n\n\nguild runs rm -E\n\n\n\n\nTo permanently delete all failed runs, use:\n\n\nguild runs rm -Ep\n\n\n\n\n\n\nImportant\n\n\nPermanently deleted runs cannot be recovered!\n\n\n\n\nTo delete all failed and terminated runs, use:\n\n\nguild runs -ET\n\n\n\n\nRestore deleted runs\n\n\nDeleted runs can be recovered by running:\n\n\nguild runs restore [RUN...]\n\n\n\n\nFor more help, see the \nruns restore\n command.\n\n\nPurge deleted runs\n\n\nThe disk space used by deleted runs can be recovered by permanently\ndeleting them using \nruns purge\n.\n\n\n\n\nTip\n\n\nYou can show the list deleted runs using \nguild runs --deleted\n.\n\n\n\n\nFor example, to permanently delete all deleted runs, use:\n\n\nguild runs purge\n\n\n\n\nGuild will prompt you before proceeding.\n\n\n\n\nImportant\n\n\nPurging deleted runs will permanently delete them! Be certain that\nyou don\u2019t need a run before permanently deleting it.\n\n\n\n\nFor more help, see the \nruns purge\n command.",
            "title": "Runs"
        },
        {
            "location": "/docs/runs/#runs",
            "text": "Overview  Concepts  Run directory  Limiting runs  Run scope  Run filtering    Selecting runs  Examples      Start a run  Operation aliases    Flag values  List runs  Get run information  Compare runs  Label runs  Delete runs  Frequently used delete commands    Restore deleted runs  Purge deleted runs",
            "title": "Runs"
        },
        {
            "location": "/docs/runs/#overview",
            "text": "Runs are generated in Guild AI by running an  operation .  When you train a model, you generate a  run , which contains the\ntrained model as well as training logs and other artifacts associated\nwith the operation.  Similarly, when you fine tune a model, you generate a run. When you\ntest a model, you generate a run. In fact, any operation that you run\ngenerates a distinct run. This is how Guild manages your work.  Here is a common work flow:   Find and install a model  Run an operation on that model (e.g.  train )  Monitor the progress of the operation (e.g.  view )  Run another operation with different hyper-parameters (flags)  Compare runs  Delete runs that you\u2019re no longer interested in  Select successful runs for deployment or use in other operations   The work centers on  runs  \u2014 creating, comparing, and selecting.",
            "title": "Overview"
        },
        {
            "location": "/docs/runs/#concepts",
            "text": "As you work with runs in Guild it\u2019s important to understand some core\nconcepts. If you\u2019d prefer to skip this conceptual material, jump to Start a run  below.",
            "title": "Concepts"
        },
        {
            "location": "/docs/runs/#run-directory",
            "text": "A  run directory  is a file system directory (or folder) that contains\nartifacts associated with a run. Guild creates a unique run directory\nfor every run. This directory contains a variety of important data:   Run metadata  Run sources such as datasets  Run output such as event logs and saved models   Run directories are located in  GUILD_HOME/runs . For more information\nsee  Guild home .  Run related operations interact with run directories in various ways:   guild run  creates a new run directory  guild runs info  prints information read from a run directory  guild runs list  enumerates run directories  guild runs delete  deletes run directories",
            "title": "Run directory"
        },
        {
            "location": "/docs/runs/#limiting-runs",
            "text": "Over time you\u2019ll generate a large number of runs. This list can become\nunwieldy, especially when you\u2019re interested in a small subset \u2014\ne.g. runs associated with a particular model you\u2019re working with. For\nthis reason, Guild provides two ways of limiting the runs that apply\nto run related commands:   Limit to runs associated with a model defined in the current\n  directory  Limit to runs that match a filter",
            "title": "Limiting runs"
        },
        {
            "location": "/docs/runs/#run-scope",
            "text": "The first limit is known as  run scope . Scope can be either  local \nor  global . By default, scope is local when the current directory\ncontains a  model definition , otherwise scope is\nglobal. Local scope limits runs to those associated with models\ndefined in the current directory. Global scope displays all runs.  Global scope can be applied using the  --all  (or  -a ) option.  Run scope is applied based on the directory that Guild commands are\nrun in. Consider the following directory structure:    Home Does not contain a model definition \u2014 global scope applies  Models\n  \n  mnist  Contains a model definition \u2014 local scope applies \n   \n   MODELS  Model definition \n   \n  \n      Commands run the from  /Home  have  global  run scope because  /Home \ndoesn\u2019t contain a model definition. Commands run from  /Models/mnist \nhowever have  local  scope because that directory contains a model\ndefinition ( /Models/mnist/MODELS ).  Run scope defaults to  local  when a model definition is exists\nbecause Guild assumes that the user is working on models defined at\nthat location and is not interested in other runs, at least by\ndefault. This follows the pattern of command line tools such as  git \nthat apply operations locally when they find a project, repository,\netc. in the current directory.  When a command is run in local scope, Guild prints a message to\nindicate that results are limited:  Limiting runs to the current directory (use --all to include all)",
            "title": "Run scope"
        },
        {
            "location": "/docs/runs/#run-filtering",
            "text": "The other limit is  run filtering . Filters are applied with command\nline options that specify run attributes, which may include:   Operation  Run status  Deleted status   Run filtering is applied  after  run scope (see above).  For example, to view runs that are associated with the  train \noperation, use the  --op  (or  -o ) option:  guild runs --op train  If the command is in local scope, Guild will limit runs to those\nassociated with models in the current directory otherwise it will use\nall runs. It will then filter those runs, limiting the result to those\nassociated with operations containing the string \u201ctrain\u201d.",
            "title": "Run filtering"
        },
        {
            "location": "/docs/runs/#selecting-runs",
            "text": "Some run related commands let you select one or more runs:   runs delete runs info runs purge runs restore   For these commands, runs can be specified in various ways:   Index as returned by  guild runs  or  guild runs list  Run ID (full or partial if unique)   Additionally, a range may be specified using run indexes in the form:  [START]:[STOP]  STOP  and  START  are inclusive \u2014 runs are selected beginning with\nthe  STOP  index up to and including those with the  START  index.  Both  STOP  and  START  are optional. If  START  is omitted it is\nassumed to be  0  (i.e. the first run in the list). If  STOP  is\nomitted it is assumed to be the index of the last run.   Important  Run indexes are relative to the list of runs returned by  guild\nruns  or  guild runs list  for a given scope and filter (see Limiting runs  above). The run associated with index 0  for one listing may not be the same run for another\nlisting. Always verify the selected runs before proceeding with a\ncommand.  When in doubt, use a run ID to select a run.",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/runs/#examples",
            "text": "Consider this output from  guild runs :  Limiting runs to the current directory (use --all to include all)\n[0:9734f85e]   ./slim-resnet-101:train        2017-12-14 07:56:32  terminated\n[1:d8cde0fc]   ./slim-resnet-50:export        2017-12-13 13:14:31  completed\n[2:0df943ac]   ./slim-resnet-50:predict       2017-12-06 11:51:15  completed\n[3:e150e44a]   ./slim-resnet-50:predict       2017-12-06 11:50:00  completed   Note  The  run scope  in the above command is  local . If\nthe user had run  guild runs --all  the scope would be  global  \u2014\nthe list and run indexes would likely be different.   Below are various operations with run selectors applied to this\nlist.   guild runs rm 0  Delete run  9734f85e  (you can always use index  0  to select the\n  most recently started run in the list)  guild runs rm 1:2  Delete runs  d8cde0fc  and  0df943ac  guild runs rm :  Delete all runs  guild runs rm 0df943ac e150e44a  Delete runs  0df943ac  and  e150e44a    Note  The following assumptions must hold for the above examples that use\nrun indexes:    Commands must be executed in the same directory as the command\n  that generated the list and without scope modifiers or filters    The runs themselves must not change \u2014 i.e. runs cannot be deleted\n  or started",
            "title": "Examples"
        },
        {
            "location": "/docs/runs/#start-a-run",
            "text": "To start a run, use the  run  command. The basic format of a run  command looks like this:  guild run OPERATION\nguild run MODEL:OPERATION\nguild run PACKAGE/MODEL:OPERATION  You can list available operations using the  operations \ncommand.  In general, you can omit information about an operation name as long\nas Guild can uniquely identify the operation.  For example, if the output of  operations  looks like this:  iris/iris-cnn:train\niris/iris-cnn:finetune\niris/iris-cnn:test  You can start the  finetune  operation by running:  guild run finetune  You can always provide the model or package. For example, this form\nwill also start  finetune :  guild run iris-cnn:finetune  You use part of the operation specification as long as Guild can\nuniquely identify the operation. For example, you can run the  test \non  iris-cnn  using:  guild run cnn:train",
            "title": "Start a run"
        },
        {
            "location": "/docs/runs/#operation-aliases",
            "text": "Some operations are so common that Guild provides  alias \ncommands. Aliases currently include:   train   Aliases are used to start operation using these forms:  guild ALIAS_CMD\nguild ALIAS_CMD MODEL\nguild ALIAS_CMD PACKAGE/MODEL  The  train  alias is used to run the  train  operation. In the example\nabove, the following commands can be used to train the iris model:  guild train\nguild train iris-cnn\nguild train cnn",
            "title": "Operation aliases"
        },
        {
            "location": "/docs/runs/#flag-values",
            "text": "Specify operation flag values as  NAME=VALUE  arguments to  run .  To get help on available and required flags for an operation, run:  guild run OPERATION --help-op  You can also view help for models defined in the current directory by\nrunning:  guild help  To get help for a packaged model, run:  guild help PACKAGE  If you omit a required flag, the  run  command (or applicable alias)\nwill exit with an error message.",
            "title": "Flag values"
        },
        {
            "location": "/docs/runs/#list-runs",
            "text": "To list Guild runs, use the  runs  or  runs list \ncommand.  guild runs  is shorthand for  guild runs list .  When listing runs, be aware of  run scope  and  run\nfiltering  \u2014 these effect the runs that are\ndisplayed.   guild runs  List all runs with the run scope. If the current directory contain a\n   model definition  the list is limited to runs\n  associated with the locally defined models, otherwise the list will\n  contain all runs.  foo  Another thing yo.   The command:  guild runs  will display different lists depending on the directory it\u2019s run\nin. If the directory contains a model definition, runs will be limited\nto those associated with the locally defined models. If the directory\ndoes not contain a model definition, all runs are displayed.",
            "title": "List runs"
        },
        {
            "location": "/docs/runs/#get-run-information",
            "text": "Use  runs info  to show information about a run.  By default, Guild shows information about the latest run:  guild runs info  You can select a specific run by providing a run ID or index.  Run indexes are displayed in run lists (see  List runs \nabove).",
            "title": "Get run information"
        },
        {
            "location": "/docs/runs/#compare-runs",
            "text": "Compare runs by running:  guild compare  Guild Compare is spreadsheet-like application that displays runs,\ntheir status, and metrics such as validation accuracy and training\nloss.  To display compare results as a table, use:  guild compare --table  To display compare results in CSV format (e.g. for use in Excel), use:  guild compare --csv  For more help, see the  compare  command.",
            "title": "Compare runs"
        },
        {
            "location": "/docs/runs/#label-runs",
            "text": "Runs can have  labels , which provide additional information about the\nrun. A label can used for filtering in the  runs list \ncommand.  Use  runs label  to set or clear a label for a run.  Use  guild runs list LABEL  to list runs with the specified label.",
            "title": "Label runs"
        },
        {
            "location": "/docs/runs/#delete-runs",
            "text": "Delete runs using  guild runs delete  or  guild runs rm . See runs delete  for command details.  Guild will display the list of runs to be deleted and ask you to\nconfirm the operation. You must type  y  and then press  ENTER  to\nconfirm.  Deleted runs can be restored using the  runs\nrestore  command. Refer to  Restoring deleted\nruns  below for details.",
            "title": "Delete runs"
        },
        {
            "location": "/docs/runs/#frequently-used-delete-commands",
            "text": "To delete all failed runs, use:  guild runs rm -E  To permanently delete all failed runs, use:  guild runs rm -Ep   Important  Permanently deleted runs cannot be recovered!   To delete all failed and terminated runs, use:  guild runs -ET",
            "title": "Frequently used delete commands"
        },
        {
            "location": "/docs/runs/#restore-deleted-runs",
            "text": "Deleted runs can be recovered by running:  guild runs restore [RUN...]  For more help, see the  runs restore  command.",
            "title": "Restore deleted runs"
        },
        {
            "location": "/docs/runs/#purge-deleted-runs",
            "text": "The disk space used by deleted runs can be recovered by permanently\ndeleting them using  runs purge .   Tip  You can show the list deleted runs using  guild runs --deleted .   For example, to permanently delete all deleted runs, use:  guild runs purge  Guild will prompt you before proceeding.   Important  Purging deleted runs will permanently delete them! Be certain that\nyou don\u2019t need a run before permanently deleting it.   For more help, see the  runs purge  command.",
            "title": "Purge deleted runs"
        },
        {
            "location": "/docs/resources/",
            "text": "Resources\n\n\n\n\nResource sources\n\n\nResolving resources\n\n\nUnpacking sources\n\n\nSelecting source files\n\n\nOperation output\n\n\n\n\nA resource is named set of files that are used by model\n\noperations\n.\n\n\nResources may be defined at two levels:\n\n\n\n\nModel resource\n\n\nPackage resource\n\n\n\n\nModel resources are defined by a model. Here\u2019s an example:\n\n\n# MODEL\nname: simple-model\noperations:\n  train: train\n  requires: data\nresources:\n  data: data.csv\n\n\n\n\nResource sources\n\n\nResources are comprised of one or more \nsources\n. A source may be a\nstring or an object.\n\n\nA source string is equivalent to a source object with the value used\nfor the \nfile\n attribute (see below).\n\n\nA source object may define these attributes:\n\n\n\n\nurl\n\n\nSource is located on a remote server and is accessible via a\n  URL. The protocols \nhttp\n and \nhttps\n are supported. \nurl\n cannot be\n  used with either \nfile\n or \noperation\n \u2014 they are mutually\n  exclusive.\n\n\nfile\n\n\nSource is a file or directory located relative to the model or\n  package file. \nfile\n cannot be used with \nurl\n or \noperation\n \u2014\n  they are mutually exclusive.\n\n\noperation\n\n\nSource is a file generated by a model operation. Operations must be\n  specified as \n[[PACKAGE/]MODEL:]OPERATION\n. Multiple operations may\n  be specified by separating them with a comma. For more information\n  see \nOperation output\n below. \noperation\n cannot\n  be used with \nurl\n or \nfile\n \u2014 they are mutually exclusive.\n\n\nselect\n\n\nA regular expression used to select files from a local directory,\n  archive, or a run directory if the source is an operation. For more\n  information see \nSelecting source files\n\n  below. \nselect\n is required if \noperation\n is used.\n\n\nsha256\n\n\nA \nSHA-256\n hash of the\n  resource source. If specified, the source SHA-256 hash must match\n  this value for the resource to resolve.\n\n\nunpack\n\n\nA boolean flag (\ntrue\n or \nfalse\n) indicating whether or not to\n  unpack the source. If unspecified, sources are unpacked if they are\n  archives. For more information see \nUnpacking\n  sources\n below.\n\n\n\n\nThe attributes \nurl\n, \nfile\n, and \noperation\n collectively represent\nthe source \ntype\n. One and only one of these attributes must be\nspecified for each source object.\n\n\nResolving resources\n\n\nBefore an operation is started, each required resource must be\n\nresolved\n. Resource resolution consists of these steps for each\nresource source:\n\n\n\n\nAcquire the source\n\n\nIf a SHA-256 hash is available, verify the source\n\n\nIf the source is an archive and \nunpack\n is not \nno\n, unpack the\n  archive\n\n\nCreate link to the source within the operation run directory or, if\n  \nselect\n is specified, create a link for each matching path within\n  the source\n\n\n\n\nWhen all required resources are resolved, Guild will start the\noperation.\n\n\nURL sources are stored in Guild\u2019s \nresource\ncache\n. If a source\n\n\nUnpacking sources\n\n\nSource archives may be unpacked to access their constituent files. A\nfile is considered an archive if it has one of the following\nextensions: \n.zip\n, \n.tar\n, \n.tgz\n, \n.tar.*\n.\n\n\nBy default, archives are unpacked. You can explicitly disable\nunpacking by setting the \nunpack\n source attribute to \nno\n.\n\n\nSelecting source files\n\n\nA source may indicate that files within a directory or archive should\nbe \nselected\n for use by specifying the source \nselect\n attribute. The\nvalue of \nselect\n must be a value regular expression. When specified,\nGuild will create links to each matching path within the source\ndirectory or archive.\n\n\nArchives must be unpacked to select source files.\n\n\nLinks use the basename of each matching file and do not contain parent\npaths. To illustrate, consider this structure, which may apply to\neither a file system directory or the contents of an archive:\n\n\n\n\n\n\nmodels-master\n \n\n \nsrc\n  \n\n  \nmnist\n  \n\n \n\n \n\n\n\n\n\n\n\n\n\nTo create a link to \nmnist\n in the operation run directory, use a\n\nselect\n value of \nmodels-master/src/mnist\n.\n\n\nHere\u2019s a model definition that illustrates this scheme.\n\n\nname: example\noperations:\n  train:\n    cmd: train\n    requires: mnist\nresources:\n  mnist:\n    sources:\n    - url: https://github.com/acme/models/archive/master.zip\n      select: models-master/src/mnist\n\n\n\n\nOperation output\n\n\nIt\u2019s common for an operation to require the output of another\noperation. Examples include:\n\n\n\n\nModel training requires a prepared dataset\n\n\nModel compression requires a trained model\n\n\nModel deployment requires a compressed model\n\n\n\n\nBy using required resources with \noperation\n sources, model developers\ncan effectively link operations together in a pipeline.\n\n\nGuild resolves operations using these steps:\n\n\n\n\n\n\nIf the user specifies a run ID as an argument to the \nrun\n\n  command in the form \nRESOURCE_NAME=RUN_ID\n Guild will resolve the\n  operation source using the target run directory.\n\n\n\n\n\n\nIf the user does not specify a run ID, Guild uses the latest\n  non-error run for any of the specified operations. Multiple\n  operations may be specified by separating the operation specs with a\n  comma.\n\n\n\n\n\n\nConsider the following model definition:\n\n\nname: example\noperations:\n  prepare:\n    cmd: prepare\n  train:\n    cmd: train\n    requires: data\nresources:\n  data:\n    sources:\n    - operation: prepare\n      select: data.csv\n\n\n\n\nIn this example, the \ntrain\n operation requires output from the\n\nprepare\n operation. This requirement is expressed using the\n\nrequires\n operation attribute, which references the named resource\n\ndata\n. The \ndata\n resource consists of a single source:\n\ndata.csv\n, which is generated by the \nprepare\n operation.",
            "title": "Resources"
        },
        {
            "location": "/docs/resources/#resources",
            "text": "Resource sources  Resolving resources  Unpacking sources  Selecting source files  Operation output   A resource is named set of files that are used by model operations .  Resources may be defined at two levels:   Model resource  Package resource   Model resources are defined by a model. Here\u2019s an example:  # MODEL\nname: simple-model\noperations:\n  train: train\n  requires: data\nresources:\n  data: data.csv",
            "title": "Resources"
        },
        {
            "location": "/docs/resources/#resource-sources",
            "text": "Resources are comprised of one or more  sources . A source may be a\nstring or an object.  A source string is equivalent to a source object with the value used\nfor the  file  attribute (see below).  A source object may define these attributes:   url  Source is located on a remote server and is accessible via a\n  URL. The protocols  http  and  https  are supported.  url  cannot be\n  used with either  file  or  operation  \u2014 they are mutually\n  exclusive.  file  Source is a file or directory located relative to the model or\n  package file.  file  cannot be used with  url  or  operation  \u2014\n  they are mutually exclusive.  operation  Source is a file generated by a model operation. Operations must be\n  specified as  [[PACKAGE/]MODEL:]OPERATION . Multiple operations may\n  be specified by separating them with a comma. For more information\n  see  Operation output  below.  operation  cannot\n  be used with  url  or  file  \u2014 they are mutually exclusive.  select  A regular expression used to select files from a local directory,\n  archive, or a run directory if the source is an operation. For more\n  information see  Selecting source files \n  below.  select  is required if  operation  is used.  sha256  A  SHA-256  hash of the\n  resource source. If specified, the source SHA-256 hash must match\n  this value for the resource to resolve.  unpack  A boolean flag ( true  or  false ) indicating whether or not to\n  unpack the source. If unspecified, sources are unpacked if they are\n  archives. For more information see  Unpacking\n  sources  below.   The attributes  url ,  file , and  operation  collectively represent\nthe source  type . One and only one of these attributes must be\nspecified for each source object.",
            "title": "Resource sources"
        },
        {
            "location": "/docs/resources/#resolving-resources",
            "text": "Before an operation is started, each required resource must be resolved . Resource resolution consists of these steps for each\nresource source:   Acquire the source  If a SHA-256 hash is available, verify the source  If the source is an archive and  unpack  is not  no , unpack the\n  archive  Create link to the source within the operation run directory or, if\n   select  is specified, create a link for each matching path within\n  the source   When all required resources are resolved, Guild will start the\noperation.  URL sources are stored in Guild\u2019s  resource\ncache . If a source",
            "title": "Resolving resources"
        },
        {
            "location": "/docs/resources/#unpacking-sources",
            "text": "Source archives may be unpacked to access their constituent files. A\nfile is considered an archive if it has one of the following\nextensions:  .zip ,  .tar ,  .tgz ,  .tar.* .  By default, archives are unpacked. You can explicitly disable\nunpacking by setting the  unpack  source attribute to  no .",
            "title": "Unpacking sources"
        },
        {
            "location": "/docs/resources/#selecting-source-files",
            "text": "A source may indicate that files within a directory or archive should\nbe  selected  for use by specifying the source  select  attribute. The\nvalue of  select  must be a value regular expression. When specified,\nGuild will create links to each matching path within the source\ndirectory or archive.  Archives must be unpacked to select source files.  Links use the basename of each matching file and do not contain parent\npaths. To illustrate, consider this structure, which may apply to\neither a file system directory or the contents of an archive:    models-master\n  \n  src\n   \n   mnist\n   \n  \n      To create a link to  mnist  in the operation run directory, use a select  value of  models-master/src/mnist .  Here\u2019s a model definition that illustrates this scheme.  name: example\noperations:\n  train:\n    cmd: train\n    requires: mnist\nresources:\n  mnist:\n    sources:\n    - url: https://github.com/acme/models/archive/master.zip\n      select: models-master/src/mnist",
            "title": "Selecting source files"
        },
        {
            "location": "/docs/resources/#operation-output",
            "text": "It\u2019s common for an operation to require the output of another\noperation. Examples include:   Model training requires a prepared dataset  Model compression requires a trained model  Model deployment requires a compressed model   By using required resources with  operation  sources, model developers\ncan effectively link operations together in a pipeline.  Guild resolves operations using these steps:    If the user specifies a run ID as an argument to the  run \n  command in the form  RESOURCE_NAME=RUN_ID  Guild will resolve the\n  operation source using the target run directory.    If the user does not specify a run ID, Guild uses the latest\n  non-error run for any of the specified operations. Multiple\n  operations may be specified by separating the operation specs with a\n  comma.    Consider the following model definition:  name: example\noperations:\n  prepare:\n    cmd: prepare\n  train:\n    cmd: train\n    requires: data\nresources:\n  data:\n    sources:\n    - operation: prepare\n      select: data.csv  In this example, the  train  operation requires output from the prepare  operation. This requirement is expressed using the requires  operation attribute, which references the named resource data . The  data  resource consists of a single source: data.csv , which is generated by the  prepare  operation.",
            "title": "Operation output"
        },
        {
            "location": "/docs/packages/",
            "text": "Packages\n\n\n\n\nFind packages\n\n\nInstall packages\n\n\nList installed packages\n\n\nUninstall packages\n\n\nCreate package\n\n\n\n\nA Guild AI \npackage\n is a container for \nmodels\n and\n\nresources\n. Packages let developers easily publish\ntheir work for users to discover. They let users easily find, install,\nand use models. Packages are the central feature of Guild\u2019s support\nfor model collaboration, sharing and reuse.\n\n\nFind packages\n\n\nYou can find Guild packages in various ways:\n\n\n\n\nVisit Guild\u2019s \nmodel repository\n\n\nSearch for a model using the \nsearch\n command\n\n\n\n\nFor example, to find models that support the ImageNet dataset, simply run:\n\n\nguild search imagenet\n\n\n\n\nNew models are being published all the time so if you don\u2019t find what\nyou\u2019re looking for, let the community know by \nsubmitting a request on\nGuild\u2019s issue tracker\n.\n\n\nIt\u2019s also easy to create and publish your own models.\n\n\nInstall packages\n\n\nInstall a package by running:\n\n\nguild install PACKAGE\n\n\n\n\nYou can find package names using \nsearch\n (see \nFind\npackages\n above).\n\n\nYou can also browse \nGuild AI models\n.\n\n\nList installed packages\n\n\nTo list installed Guild AI packages, run:\n\n\nguild packages\n\n\n\n\nYou can list specific packages using \npackages\nlist\n:\n\n\nguild packages list FILTER\n\n\n\n\nFor example, to list installed packages containing \nmagenta\n, run:\n\n\nguild packages list magenta\n\n\n\n\nUninstall packages\n\n\nUninstall a package by running:\n\n\nguild uninstall PACKAGE\n\n\n\n\nGuild will prompt you before deleting package files.\n\n\nIf you\u2019d prefer to skip the prompt, use the \n-y\n option:\n\n\nguild uninstall PACKAGE -y\n\n\n\n\nCreate package\n\n\nCreating packages is an advanced topic that is not currently covered\nin this documentation.\n\n\nYou may however review the package definitions at\n\nhttps://github.com/guildai/packages\n for examples of packages.\n\n\nIf you need help creating a package, drop us a line at\n\nniceperson@guild.ai\n and we\u2019ll be happy\nto help!",
            "title": "Packages"
        },
        {
            "location": "/docs/packages/#packages",
            "text": "Find packages  Install packages  List installed packages  Uninstall packages  Create package   A Guild AI  package  is a container for  models  and resources . Packages let developers easily publish\ntheir work for users to discover. They let users easily find, install,\nand use models. Packages are the central feature of Guild\u2019s support\nfor model collaboration, sharing and reuse.",
            "title": "Packages"
        },
        {
            "location": "/docs/packages/#find-packages",
            "text": "You can find Guild packages in various ways:   Visit Guild\u2019s  model repository  Search for a model using the  search  command   For example, to find models that support the ImageNet dataset, simply run:  guild search imagenet  New models are being published all the time so if you don\u2019t find what\nyou\u2019re looking for, let the community know by  submitting a request on\nGuild\u2019s issue tracker .  It\u2019s also easy to create and publish your own models.",
            "title": "Find packages"
        },
        {
            "location": "/docs/packages/#install-packages",
            "text": "Install a package by running:  guild install PACKAGE  You can find package names using  search  (see  Find\npackages  above).  You can also browse  Guild AI models .",
            "title": "Install packages"
        },
        {
            "location": "/docs/packages/#list-installed-packages",
            "text": "To list installed Guild AI packages, run:  guild packages  You can list specific packages using  packages\nlist :  guild packages list FILTER  For example, to list installed packages containing  magenta , run:  guild packages list magenta",
            "title": "List installed packages"
        },
        {
            "location": "/docs/packages/#uninstall-packages",
            "text": "Uninstall a package by running:  guild uninstall PACKAGE  Guild will prompt you before deleting package files.  If you\u2019d prefer to skip the prompt, use the  -y  option:  guild uninstall PACKAGE -y",
            "title": "Uninstall packages"
        },
        {
            "location": "/docs/packages/#create-package",
            "text": "Creating packages is an advanced topic that is not currently covered\nin this documentation.  You may however review the package definitions at https://github.com/guildai/packages  for examples of packages.  If you need help creating a package, drop us a line at niceperson@guild.ai  and we\u2019ll be happy\nto help!",
            "title": "Create package"
        },
        {
            "location": "/docs/visual/guild-view/",
            "text": "Guild View\n\n\nGuild View is a visual application provided with the Guild AI\ntoolkit. It can be used to view run results and browse run\nartifacts. It also integrates with TensorBoard to let you view\nTensorFlow event logs generated for runs.\n\n\n\n\nGuild View\n\n\n\n\nStarting Guild View\n\n\nViewing specific runs\n\n\nFiltering runs within Guild View\n\n\nViewing run files\n\n\nView runs in TensorBoard\n\n\nAutomatic updates\n\n\nStopping Guild View\n\n\n\n\nStarting Guild View\n\n\nStart Guild View by running:\n\n\nguild view\n\n\n\n\nThis will start Guild View an a randomly selected port and open it in\nyour browser.\n\n\n\n\nImportant\n\n\nIf you\u2019re running Guild View on a remote server, Guild will not be\nable to open it on your workstation. Instead, manually open the link\ndisplayed by the \nguild view\n command.\n\n\n\n\nIf you\u2019d like to run Guild View on a specific port, use:\n\n\nguild view --port PORT\n\n\n\n\nIf you\u2019d like to start Guild View without opening a new browser\nwindow, use:\n\n\nguild view --no-open\n\n\n\n\nFor more help, see the \nview\n command.\n\n\nViewing specific runs\n\n\nYou can limit the runs that you view using various Guild View options.\n\n\nFor example, to view only runs for operations associated with a\nparticular model, use the \n-o\n (or \n--operation\n) option and specify\nthe model:\n\n\nguild view -o MODEL\n\n\n\n\nFor example, if you\u2019re working with the \nresnet-50\n model, you can\ntell Guild View to only show runs for that model by running:\n\n\nguild view -o resnet-50\n\n\n\n\nFor a complete list of options, see the \nview\n command.\n\n\nFiltering runs within Guild View\n\n\nYou can further filter runs in Guild View using the \nFilter\n input\nin the upper left of the window.\n\n\nRuns can be filtered using:\n\n\n\n\nPackage\n\n\nModel\n\n\nOperation name\n\n\nRun ID\n\n\n\n\nViewing run files\n\n\nYou can browse run files \u2014 both those used as run input and those\ngenerated as output \u2014 by clicking on the \nFILES\n tab.\n\n\n\n\nGuild View - \nFILES\n tab\n\n\nUse the \nFilter\n input at the top of the files list to view only\nfiles you\u2019re interested in.\n\n\nGuild View provides a built-in file viewer for some file types:\n\n\n\n\nImages\n\n\nMusic\n\n\n\n\nIf a file can be opened in Guild View, its name will appear as a light\ngrey button. When you click the button, Guild will open a file viewer.\n\n\n\n\nMedia file names have have grey buttons, which can be clicked to\n  view the file.\n\n\nWhen you\u2019re viewing files in Guild View, you can navigate through them\nusing \nNEXT\n and \nPREV\n buttons.\n\n\n\n\nUse Guild View to view generated artifacts like images and music\n\n\nView runs in TensorBoard\n\n\nGuild View provides integration with TensorBoard. To view TensorFlow\nevent logs, click \n in the upper left of\nthe window.\n\n\n\n\nImportant\n\n\nIf you\u2019re running Guild View on a remote server, the \nView in\nTensorBoard\n feature will work. This is a known issue and will be\nfixed in upcoming releases of Guild AI. To view runs in\nTensorBoard on a remote server, use the\n\ntensorboard\n command from the remote server\ninstead of Guild View.\n\n\n\n\nAutomatic updates\n\n\nGuild View and TensorBoard will automatically update to show the\nlatest runs and run files.\n\n\nStopping Guild View\n\n\nGuild View will run until it\u2019s stopped. To stop Guild View, type\n\nCTRL-C\n in the console window where Guild View is running.\n\n\n\n\nNote\n\n\nWhen you\u2019ve stopped Guild View, the associated browser window will\nno longer update or respond.",
            "title": "Guild View"
        },
        {
            "location": "/docs/visual/guild-view/#guild-view",
            "text": "Guild View is a visual application provided with the Guild AI\ntoolkit. It can be used to view run results and browse run\nartifacts. It also integrates with TensorBoard to let you view\nTensorFlow event logs generated for runs.   Guild View   Starting Guild View  Viewing specific runs  Filtering runs within Guild View  Viewing run files  View runs in TensorBoard  Automatic updates  Stopping Guild View",
            "title": "Guild View"
        },
        {
            "location": "/docs/visual/guild-view/#starting-guild-view",
            "text": "Start Guild View by running:  guild view  This will start Guild View an a randomly selected port and open it in\nyour browser.   Important  If you\u2019re running Guild View on a remote server, Guild will not be\nable to open it on your workstation. Instead, manually open the link\ndisplayed by the  guild view  command.   If you\u2019d like to run Guild View on a specific port, use:  guild view --port PORT  If you\u2019d like to start Guild View without opening a new browser\nwindow, use:  guild view --no-open  For more help, see the  view  command.",
            "title": "Starting Guild View"
        },
        {
            "location": "/docs/visual/guild-view/#viewing-specific-runs",
            "text": "You can limit the runs that you view using various Guild View options.  For example, to view only runs for operations associated with a\nparticular model, use the  -o  (or  --operation ) option and specify\nthe model:  guild view -o MODEL  For example, if you\u2019re working with the  resnet-50  model, you can\ntell Guild View to only show runs for that model by running:  guild view -o resnet-50  For a complete list of options, see the  view  command.",
            "title": "Viewing specific runs"
        },
        {
            "location": "/docs/visual/guild-view/#filtering-runs-within-guild-view",
            "text": "You can further filter runs in Guild View using the  Filter  input\nin the upper left of the window.  Runs can be filtered using:   Package  Model  Operation name  Run ID",
            "title": "Filtering runs within Guild View"
        },
        {
            "location": "/docs/visual/guild-view/#viewing-run-files",
            "text": "You can browse run files \u2014 both those used as run input and those\ngenerated as output \u2014 by clicking on the  FILES  tab.   Guild View -  FILES  tab  Use the  Filter  input at the top of the files list to view only\nfiles you\u2019re interested in.  Guild View provides a built-in file viewer for some file types:   Images  Music   If a file can be opened in Guild View, its name will appear as a light\ngrey button. When you click the button, Guild will open a file viewer.   Media file names have have grey buttons, which can be clicked to\n  view the file.  When you\u2019re viewing files in Guild View, you can navigate through them\nusing  NEXT  and  PREV  buttons.   Use Guild View to view generated artifacts like images and music",
            "title": "Viewing run files"
        },
        {
            "location": "/docs/visual/guild-view/#view-runs-in-tensorboard",
            "text": "Guild View provides integration with TensorBoard. To view TensorFlow\nevent logs, click   in the upper left of\nthe window.   Important  If you\u2019re running Guild View on a remote server, the  View in\nTensorBoard  feature will work. This is a known issue and will be\nfixed in upcoming releases of Guild AI. To view runs in\nTensorBoard on a remote server, use the tensorboard  command from the remote server\ninstead of Guild View.",
            "title": "View runs in TensorBoard"
        },
        {
            "location": "/docs/visual/guild-view/#automatic-updates",
            "text": "Guild View and TensorBoard will automatically update to show the\nlatest runs and run files.",
            "title": "Automatic updates"
        },
        {
            "location": "/docs/visual/guild-view/#stopping-guild-view",
            "text": "Guild View will run until it\u2019s stopped. To stop Guild View, type CTRL-C  in the console window where Guild View is running.   Note  When you\u2019ve stopped Guild View, the associated browser window will\nno longer update or respond.",
            "title": "Stopping Guild View"
        },
        {
            "location": "/docs/visual/tensorboard/",
            "text": "TensorBoard\n\n\n\n\nTensorBoard\n\n\nTensorBoard is a visualization tool from the TensorFlow team. It\u2019s a\nweb based application that lets you view TensorFlow event logs, which\ncontain a variety of useful information associated with a run:\n\n\n\n\nMetrics (scalars)\n\n\nImages, audio, and text generated during training\n\n\nModel graph\n\n\nModel statistics\n\n\n\n\nFor more information, see \nTensorBoard: Visualizing\nLearning\n.\n\n\nTensorBoard and Guild\n\n\nGuild integrates TensorBoard to make it easy to visualize TensorFlow\nevent logs. To visualize events for a set of runs, you can launch\nTensorBoard by running:\n\n\nguild tensorboard\n\n\n\n\nFor more information, see \ntensorboard\ncommand\n.\n\n\nTensorBoard is also integrated with \nGuild\nView\n. You can launch TensorBoard from Guild\nView by clicking \n which is located in\nthe upper left of the screen.\n\n\nRun synchronization\n\n\nWhen you run TensorBoard from Guild, by either the \ntensorboard\n\ncommand or from Guild View, the list of runs is automatically\nsynchronized with the current \nrun view\n.",
            "title": "TensorBoard"
        },
        {
            "location": "/docs/visual/tensorboard/#tensorboard",
            "text": "TensorBoard  TensorBoard is a visualization tool from the TensorFlow team. It\u2019s a\nweb based application that lets you view TensorFlow event logs, which\ncontain a variety of useful information associated with a run:   Metrics (scalars)  Images, audio, and text generated during training  Model graph  Model statistics   For more information, see  TensorBoard: Visualizing\nLearning .",
            "title": "TensorBoard"
        },
        {
            "location": "/docs/visual/tensorboard/#tensorboard-and-guild",
            "text": "Guild integrates TensorBoard to make it easy to visualize TensorFlow\nevent logs. To visualize events for a set of runs, you can launch\nTensorBoard by running:  guild tensorboard  For more information, see  tensorboard\ncommand .  TensorBoard is also integrated with  Guild\nView . You can launch TensorBoard from Guild\nView by clicking   which is located in\nthe upper left of the screen.",
            "title": "TensorBoard and Guild"
        },
        {
            "location": "/docs/visual/tensorboard/#run-synchronization",
            "text": "When you run TensorBoard from Guild, by either the  tensorboard \ncommand or from Guild View, the list of runs is automatically\nsynchronized with the current  run view .",
            "title": "Run synchronization"
        },
        {
            "location": "/docs/commands/",
            "text": "Commands\n\n\n\n\nGeneral\n\n\nModel support\n\n\nRun support\n\n\nPackage support\n\n\nVisualization\n\n\nUtilities\n\n\nRunning commands\n\n\nCommand options\n\n\nRunning commands in a separate console\n\n\n\n\n\n\nCommand help\n\n\n\n\n\n\n\n\nGeneral\n\n\ncheck\nhelp\ninit\n\n\n\n\nModel support\n\n\nhelp\nmodels\noperations\n\n\n\n\nRun support\n\n\ncompare\nlabel\nrun\nruns\nruns delete\nruns info\nruns label\nruns list\nruns purge\nruns restore\nruns stop\nstop\nsync\ntrain\n\n\n\n\nPackage support\n\n\ninstall\npackage\npackages\npackages delete\npackages info\npackages list\nresources\nsearch\nuninstall\n\n\n\n\nVisualization\n\n\ntensorboard\nview\n\n\n\n\nUtilities\n\n\nindex\ntensorflow inspect\n\n\n\n\nRunning commands\n\n\nGuild commands must be executed on a command line. If you\u2019re\nunfamiliar with running commands on your system, refer to \nGetting to\nKnow the Command Line\n for a\nprimer.\n\n\nAt a command line, run a Guild command using this convention:\n\n\nguild COMMAND\n\n\n\n\nwhere \nCOMMAND\n is one of the commands above. For a list of commands,\nrefer to this page or run \nguild --help\n on the command line. For\nmore information on getting see \nCommand help\n below.\n\n\nAll Guild commands run in the foreground and terminate when the\ncommand succeeds or an error occurs. You can stop a command at any\ntime by typing \nCTRL-c\n (i.e. hold down the \ncontrol\nkey\n for your system and press \nc\n).\n\n\nCommand options\n\n\nCommands accept \noptions\n, which may be provided as command line\narguments in the format \n--OPTION [VALUE]\n or \n-OPTION_CHAR\n[VALUE]\n where \nOPTION\n is the full name of the option and\n\nOPTION_CHAR\n is the single character option shortcut. \nVALUE\n may be\nrequired, optional, or considered invalid depending on the specific\ncommand option.\n\n\nOptions for each command are printed when you run \nguild COMMAND\n--help\n. They are also listed and described in more details in this\nguide.\n\n\nRunning commands in a separate console\n\n\nThere are some commands that will not terminate until you explicitly\nstop them:\n\n\n\n\ncompare\n\n\nview\n\n\ntensorboard\n\n\n\n\nTo run another command while one of these is still running, run the\nnew command in a separate console. There are various strategies for\nmanaging separate consoles:\n\n\n\n\nOpen another console/terminal\n\n\nUse a console/terminal application that supports multiple tabs\n\n\nUse an integrated developer environment (IDE) that supports running\n  commands in different terminals\n\n\nUse a multiplexer like \ntmux\n\n  (advanced)\n\n\n\n\nCommand help\n\n\nThe Guild CLI provides two levels of help:\n\n\n\n\nGeneral help\n\n\nCommand specific help\n\n\n\n\nGeneral help is available by running:\n\n\nguild --help\n\n\n\n\nThis will print Guild\u2019s global options as well as available commands.\n\n\nGlobal options may be specified for any command but must be specified\nbefore the command.\n\n\nCommand help is available by running:\n\n\nguild COMMAND --help\n\n\n\n\nThis will print details about what the command does and how it can be\nconfigured including details about its options.",
            "title": "Commands"
        },
        {
            "location": "/docs/commands/#commands",
            "text": "General  Model support  Run support  Package support  Visualization  Utilities  Running commands  Command options  Running commands in a separate console    Command help",
            "title": "Commands"
        },
        {
            "location": "/docs/commands/#general",
            "text": "check help init",
            "title": "General"
        },
        {
            "location": "/docs/commands/#model-support",
            "text": "help models operations",
            "title": "Model support"
        },
        {
            "location": "/docs/commands/#run-support",
            "text": "compare label run runs runs delete runs info runs label runs list runs purge runs restore runs stop stop sync train",
            "title": "Run support"
        },
        {
            "location": "/docs/commands/#package-support",
            "text": "install package packages packages delete packages info packages list resources search uninstall",
            "title": "Package support"
        },
        {
            "location": "/docs/commands/#visualization",
            "text": "tensorboard view",
            "title": "Visualization"
        },
        {
            "location": "/docs/commands/#utilities",
            "text": "index tensorflow inspect",
            "title": "Utilities"
        },
        {
            "location": "/docs/commands/#running-commands",
            "text": "Guild commands must be executed on a command line. If you\u2019re\nunfamiliar with running commands on your system, refer to  Getting to\nKnow the Command Line  for a\nprimer.  At a command line, run a Guild command using this convention:  guild COMMAND  where  COMMAND  is one of the commands above. For a list of commands,\nrefer to this page or run  guild --help  on the command line. For\nmore information on getting see  Command help  below.  All Guild commands run in the foreground and terminate when the\ncommand succeeds or an error occurs. You can stop a command at any\ntime by typing  CTRL-c  (i.e. hold down the  control\nkey  for your system and press  c ).",
            "title": "Running commands"
        },
        {
            "location": "/docs/commands/#command-options",
            "text": "Commands accept  options , which may be provided as command line\narguments in the format  --OPTION [VALUE]  or  -OPTION_CHAR\n[VALUE]  where  OPTION  is the full name of the option and OPTION_CHAR  is the single character option shortcut.  VALUE  may be\nrequired, optional, or considered invalid depending on the specific\ncommand option.  Options for each command are printed when you run  guild COMMAND\n--help . They are also listed and described in more details in this\nguide.",
            "title": "Command options"
        },
        {
            "location": "/docs/commands/#running-commands-in-a-separate-console",
            "text": "There are some commands that will not terminate until you explicitly\nstop them:   compare  view  tensorboard   To run another command while one of these is still running, run the\nnew command in a separate console. There are various strategies for\nmanaging separate consoles:   Open another console/terminal  Use a console/terminal application that supports multiple tabs  Use an integrated developer environment (IDE) that supports running\n  commands in different terminals  Use a multiplexer like  tmux \n  (advanced)",
            "title": "Running commands in a separate console"
        },
        {
            "location": "/docs/commands/#command-help",
            "text": "The Guild CLI provides two levels of help:   General help  Command specific help   General help is available by running:  guild --help  This will print Guild\u2019s global options as well as available commands.  Global options may be specified for any command but must be specified\nbefore the command.  Command help is available by running:  guild COMMAND --help  This will print details about what the command does and how it can be\nconfigured including details about its options.",
            "title": "Command help"
        },
        {
            "location": "/docs/commands/check-cmd/",
            "text": "check command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild check [OPTIONS]\n\n  \n\n    \nCheck the Guild setup.\n\n\nThis command performs a number of checks and prints information\nabout the Guild setup.\n\n\nYou can also run the Guild test suite by specifying the \n--tests\n\noption.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-T, --tests\n\n        \nRun Guild test suite.\n\n      \n\n      \n      \n\n        \n-t, --test TEST\n\n        \nRun \nTEST\n (may be used multiple times).\n\n      \n\n      \n      \n\n        \n-n, --no-info\n\n        \nDon't print info (useful when just running tests).\n\n      \n\n      \n      \n\n        \n-s, --skip TEST\n\n        \nSkip \nTEST\n when running Guild test suite. Ignored otherwise.\n\n      \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow more information.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "check command"
        },
        {
            "location": "/docs/commands/check-cmd/#check-command",
            "text": "Usage  Options",
            "title": "check command"
        },
        {
            "location": "/docs/commands/check-cmd/#usage",
            "text": "guild check [OPTIONS] \n   \n     Check the Guild setup.  This command performs a number of checks and prints information\nabout the Guild setup.  You can also run the Guild test suite by specifying the  --tests \noption.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/check-cmd/#options",
            "text": "-T, --tests \n         Run Guild test suite. \n       \n      \n       \n         -t, --test TEST \n         Run  TEST  (may be used multiple times). \n       \n      \n       \n         -n, --no-info \n         Don't print info (useful when just running tests). \n       \n      \n       \n         -s, --skip TEST \n         Skip  TEST  when running Guild test suite. Ignored otherwise. \n       \n      \n       \n         -v, --verbose \n         Show more information. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/compare-cmd/",
            "text": "compare command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild compare [OPTIONS] [RUN...]\n\n  \n\n    \nCompare run results.\n\n\nGuild Compare is a console based application that displays a table\nof runs with their current accuracy and loss. The application will\ncontinue to run until you exit it by pressing \nq\n (for quit).\n\n\nGuild Compare supports a number of commands. Commands are\nactivated by pressing a letter. To view the list of commands,\npress \n?\n.\n\n\nGuild Compare does not automatically update to display the latest\navailable data. If you want to update the list of runs and their\nstatus, press \nr\n (for refresh).\n\n\nYou may alternative use this command to generate CSV output for\nrun. Use the \n--csv\n option to print data to standard output\ninstead of running as an application. You can redirect this output\nto a file using:\n\n\nguild compare --csv > RUNS.csv\n\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-t, --table\n\n        \nGenerate comparison data as a table.\n\n      \n\n      \n      \n\n        \n-c, --csv\n\n        \nGenerate comparison data as a CSV file.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "compare command"
        },
        {
            "location": "/docs/commands/compare-cmd/#compare-command",
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status    Options",
            "title": "compare command"
        },
        {
            "location": "/docs/commands/compare-cmd/#usage",
            "text": "guild compare [OPTIONS] [RUN...] \n   \n     Compare run results.  Guild Compare is a console based application that displays a table\nof runs with their current accuracy and loss. The application will\ncontinue to run until you exit it by pressing  q  (for quit).  Guild Compare supports a number of commands. Commands are\nactivated by pressing a letter. To view the list of commands,\npress  ? .  Guild Compare does not automatically update to display the latest\navailable data. If you want to update the list of runs and their\nstatus, press  r  (for refresh).  You may alternative use this command to generate CSV output for\nrun. Use the  --csv  option to print data to standard output\ninstead of running as an application. You can redirect this output\nto a file using:  guild compare --csv > RUNS.csv",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/compare-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/compare-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/compare-cmd/#filtering-by-run-status",
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.",
            "title": "Filtering by run status"
        },
        {
            "location": "/docs/commands/compare-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -t, --table \n         Generate comparison data as a table. \n       \n      \n       \n         -c, --csv \n         Generate comparison data as a CSV file. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/help-cmd/",
            "text": "help command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild help [OPTIONS] [PATH_OR_PACKAGE]\n\n  \n\n    \nShow help for a path or package.\n\n\nBy default shows information about the models defined in the\nproject.\n\n\nTo display the description for distributions generated using the\npackage command, specify the \n--package-description\n option.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n--package-description\n\n        \nShow the package description.\n\n      \n\n      \n      \n\n        \n-n, --no-pager\n\n        \nDo not use a pager when showing help.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "help command"
        },
        {
            "location": "/docs/commands/help-cmd/#help-command",
            "text": "Usage  Options",
            "title": "help command"
        },
        {
            "location": "/docs/commands/help-cmd/#usage",
            "text": "guild help [OPTIONS] [PATH_OR_PACKAGE] \n   \n     Show help for a path or package.  By default shows information about the models defined in the\nproject.  To display the description for distributions generated using the\npackage command, specify the  --package-description  option.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/help-cmd/#options",
            "text": "--package-description \n         Show the package description. \n       \n      \n       \n         -n, --no-pager \n         Do not use a pager when showing help. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/index-cmd/",
            "text": "index command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild index [OPTIONS]\n\n  \n\n    \nMange the run index.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-s, --sync\n\n        \nSynchronize index with current runs.\n\n      \n\n      \n      \n\n        \n-r, --raw-fields\n\n        \nShow raw (unformatted) index fields as JSON.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "index command"
        },
        {
            "location": "/docs/commands/index-cmd/#index-command",
            "text": "Usage  Options",
            "title": "index command"
        },
        {
            "location": "/docs/commands/index-cmd/#usage",
            "text": "guild index [OPTIONS] \n   \n     Mange the run index.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/index-cmd/#options",
            "text": "-s, --sync \n         Synchronize index with current runs. \n       \n      \n       \n         -r, --raw-fields \n         Show raw (unformatted) index fields as JSON. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/init-cmd/",
            "text": "init command\n\n\n\n\nUsage\n\n\nEnvironments\n\n\nProjects\n\n\nTemplates and parameters\n\n\nShared resource cache\n\n\nInstalling TensorFlow\n\n\nExamples\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild init [OPTIONS] [DIR] [PARAM=VALUE...]\n\n  \n\n    \nInitialize an environment or project.\n\n\nYou must specify either \n--env\n or \n--prject\n. Refer to the\nsections below for more information.\n\n\nEnvironments\n\n\nTo initialize a Guild environment in \nDIR\n, use \n--env\n. If \nDIR\n\nis not specified, its value is determined by whether or not the\n\ninit\n command is executed in a Python virtual environment. If\n\ninit\n is executed within a virtual environment, \nDIR\n is the\nenvironment variable root directory. If \ninit\n is not executed in\na virtual environment, \nDIR\n is the user's home directory.\n\n\nEnvironment files are always created in a \n.guild\n subdirectory of\n\nDIR\n and no other system files will be modified.\n\n\nIf \n--project\n is used, \ninit\n will not initialize a Guild\nenvironment but will instead initialize a project. See PROJECTS\nbelow for more information.\n\n\nProjects\n\n\nTo initialize a project in \nDIR\n, use \n--project\n. If \nDIR\n\ncontains any files that would otherwise be overwritten by \ninit\n,\nthe command will exit with an error.\n\n\nProjects may be generated using templates. See TEMPLATES below for\nmore information.\n\n\nProjects may alternatively be generated from installed packages by\nusing \n--from-package\n. When using \n--from-package\n the use of\n\n--project\n is implied. If \nPACKAGE\n is not installed, the command\nwill exit with an error.\n\n\nTemplates and parameters\n\n\nBy default, the template named \ndefault\n is used to initialize\nprojects. To use a different template, specify it using\n\n--template\n.\n\n\n--template\n may not be used with \n--from-package\n for\ninitializing a project.\n\n\nUse \n--list-templates\n to list available templates.\n\n\nTemplates support parameters, some of which may be required when\ninitializing a project. Use \n--help-template\n to list template\nparameters.\n\n\nSpecify parameter values using \nPARAM=VALUE\n arguments. If the\nfirst argument, which would otherwise be consideted \nDIR\n, is in\nthe form \nPARAM=VALUE\n, it will be treated as a paramater and not\nas `DIR.\n\n\nShared resource cache\n\n\nBy default, when initializing an environment outside the user's\nhome directory, the new environment will share cached resources\nwith the user level environment (i.e. the environment in the\nuser's home directory). If you want to completely isolate a new\nenvironment and ensure that cached resource are not shared, use\n\n--local-resource-cache\n.\n\n\nInstalling TensorFlow\n\n\nWhen initializing an environment, \ninit\n will check if TensorFlow\nis installed and attempt to install it if it is not. This check\ncan be skipped using \n--skip-checks\n. \ninit\n will prompt before\ninstalling unless \n--yes\n is specified.\n\n\nExamples\n\n\nInitialize the default Guild AI environment:\n\n\n$ guild init\n\n\n\nInitialize a new default project in the current directory:\n\n\n$ guild init --project\n\n\n\nCreate a new project from the installed package \nguild.mnist\n:\n\n\n$ guild init --from-package guild.mnist\n\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-e, --env\n\n        \nInitialize an environment.\n\n      \n\n      \n      \n\n        \n-p, --project\n\n        \nInitialize a new project.\n\n      \n\n      \n      \n\n        \n-t, --template TEMPLATE\n\n        \nUse a template when initializing a project.\n\n      \n\n      \n      \n\n        \n--from-package PACKAGE\n\n        \nIf initializing a project, use installed \nPACKAGE\n as a template.\n\n      \n\n      \n      \n\n        \n--local-resource-cache\n\n        \nUse a local cache when initializing an environment.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nAnswer yes to all prompts.\n\n      \n\n      \n      \n\n        \n-n, --no-check\n\n        \nDon't check the environment after initialization.\n\n      \n\n      \n      \n\n        \n--list-templates\n\n        \nShow available templates and exit.\n\n      \n\n      \n      \n\n        \n--help-template TEMPLATE\n\n        \nShow help for \nTEMPLATE\n and exit.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "init command"
        },
        {
            "location": "/docs/commands/init-cmd/#init-command",
            "text": "Usage  Environments  Projects  Templates and parameters  Shared resource cache  Installing TensorFlow  Examples    Options",
            "title": "init command"
        },
        {
            "location": "/docs/commands/init-cmd/#usage",
            "text": "guild init [OPTIONS] [DIR] [PARAM=VALUE...] \n   \n     Initialize an environment or project.  You must specify either  --env  or  --prject . Refer to the\nsections below for more information.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/init-cmd/#environments",
            "text": "To initialize a Guild environment in  DIR , use  --env . If  DIR \nis not specified, its value is determined by whether or not the init  command is executed in a Python virtual environment. If init  is executed within a virtual environment,  DIR  is the\nenvironment variable root directory. If  init  is not executed in\na virtual environment,  DIR  is the user's home directory.  Environment files are always created in a  .guild  subdirectory of DIR  and no other system files will be modified.  If  --project  is used,  init  will not initialize a Guild\nenvironment but will instead initialize a project. See PROJECTS\nbelow for more information.",
            "title": "Environments"
        },
        {
            "location": "/docs/commands/init-cmd/#projects",
            "text": "To initialize a project in  DIR , use  --project . If  DIR \ncontains any files that would otherwise be overwritten by  init ,\nthe command will exit with an error.  Projects may be generated using templates. See TEMPLATES below for\nmore information.  Projects may alternatively be generated from installed packages by\nusing  --from-package . When using  --from-package  the use of --project  is implied. If  PACKAGE  is not installed, the command\nwill exit with an error.",
            "title": "Projects"
        },
        {
            "location": "/docs/commands/init-cmd/#templates-and-parameters",
            "text": "By default, the template named  default  is used to initialize\nprojects. To use a different template, specify it using --template .  --template  may not be used with  --from-package  for\ninitializing a project.  Use  --list-templates  to list available templates.  Templates support parameters, some of which may be required when\ninitializing a project. Use  --help-template  to list template\nparameters.  Specify parameter values using  PARAM=VALUE  arguments. If the\nfirst argument, which would otherwise be consideted  DIR , is in\nthe form  PARAM=VALUE , it will be treated as a paramater and not\nas `DIR.",
            "title": "Templates and parameters"
        },
        {
            "location": "/docs/commands/init-cmd/#shared-resource-cache",
            "text": "By default, when initializing an environment outside the user's\nhome directory, the new environment will share cached resources\nwith the user level environment (i.e. the environment in the\nuser's home directory). If you want to completely isolate a new\nenvironment and ensure that cached resource are not shared, use --local-resource-cache .",
            "title": "Shared resource cache"
        },
        {
            "location": "/docs/commands/init-cmd/#installing-tensorflow",
            "text": "When initializing an environment,  init  will check if TensorFlow\nis installed and attempt to install it if it is not. This check\ncan be skipped using  --skip-checks .  init  will prompt before\ninstalling unless  --yes  is specified.",
            "title": "Installing TensorFlow"
        },
        {
            "location": "/docs/commands/init-cmd/#examples",
            "text": "Initialize the default Guild AI environment:  $ guild init  Initialize a new default project in the current directory:  $ guild init --project  Create a new project from the installed package  guild.mnist :  $ guild init --from-package guild.mnist",
            "title": "Examples"
        },
        {
            "location": "/docs/commands/init-cmd/#options",
            "text": "-e, --env \n         Initialize an environment. \n       \n      \n       \n         -p, --project \n         Initialize a new project. \n       \n      \n       \n         -t, --template TEMPLATE \n         Use a template when initializing a project. \n       \n      \n       \n         --from-package PACKAGE \n         If initializing a project, use installed  PACKAGE  as a template. \n       \n      \n       \n         --local-resource-cache \n         Use a local cache when initializing an environment. \n       \n      \n       \n         -y, --yes \n         Answer yes to all prompts. \n       \n      \n       \n         -n, --no-check \n         Don't check the environment after initialization. \n       \n      \n       \n         --list-templates \n         Show available templates and exit. \n       \n      \n       \n         --help-template TEMPLATE \n         Show help for  TEMPLATE  and exit. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/install-cmd/",
            "text": "install command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild install [OPTIONS] PACKAGE...\n\n  \n\n    \nInstall one or more packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-U, --upgrade\n\n        \nUpgrade specified packages to the newest available version.\n\n      \n\n      \n      \n\n        \n--reinstall\n\n        \nResinstall the package if it's already installed. Implies --upgrade.\n\n      \n\n      \n      \n\n        \n--no-cache\n\n        \nDon't use cached packages.\n\n      \n\n      \n      \n\n        \n--no-deps\n\n        \nDon't install dependencies.\n\n      \n\n      \n      \n\n        \n--pre\n\n        \nInstall pre-release versions.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "install command"
        },
        {
            "location": "/docs/commands/install-cmd/#install-command",
            "text": "Usage  Options",
            "title": "install command"
        },
        {
            "location": "/docs/commands/install-cmd/#usage",
            "text": "guild install [OPTIONS] PACKAGE... \n   \n     Install one or more packages.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/install-cmd/#options",
            "text": "-U, --upgrade \n         Upgrade specified packages to the newest available version. \n       \n      \n       \n         --reinstall \n         Resinstall the package if it's already installed. Implies --upgrade. \n       \n      \n       \n         --no-cache \n         Don't use cached packages. \n       \n      \n       \n         --no-deps \n         Don't install dependencies. \n       \n      \n       \n         --pre \n         Install pre-release versions. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/label-cmd/",
            "text": "label command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild label [OPTIONS] [RUN...] [LABEL]\n\n  \n\n    \nSet run labels.\n\n\nIf \nLABEL\n is provided, the command will label the selected\nruns. To clear a run label, use the \n--clear\n option.\n\n\nSelect runs to modify using one or more \nRUN\n arguments. See\nSELECT RUNS below for information on selecting runs.\n\n\nIf \nRUN\n isn't specified, the most recent run is selected.\n\n\nBy default Guild will prompt you before making any changes. If you\nwant to apply the changes without being prompted, use the\n\n--yes\n option.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n0\n is assumed (the most\nrecent run).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-c, --clear\n\n        \nClear the run's label.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before modifying labels.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "label command"
        },
        {
            "location": "/docs/commands/label-cmd/#label-command",
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status    Options",
            "title": "label command"
        },
        {
            "location": "/docs/commands/label-cmd/#usage",
            "text": "guild label [OPTIONS] [RUN...] [LABEL] \n   \n     Set run labels.  If  LABEL  is provided, the command will label the selected\nruns. To clear a run label, use the  --clear  option.  Select runs to modify using one or more  RUN  arguments. See\nSELECT RUNS below for information on selecting runs.  If  RUN  isn't specified, the most recent run is selected.  By default Guild will prompt you before making any changes. If you\nwant to apply the changes without being prompted, use the --yes  option.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/label-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  0  is assumed (the most\nrecent run).",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/label-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/label-cmd/#filtering-by-run-status",
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.",
            "title": "Filtering by run status"
        },
        {
            "location": "/docs/commands/label-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -c, --clear \n         Clear the run's label. \n       \n      \n       \n         -y, --yes \n         Do not prompt before modifying labels. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/models-cmd/",
            "text": "models command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild models [OPTIONS] [FILTER]...\n\n  \n\n    \nShow available models.\n\n\nUse one or more \nFILTER\n arguments to show only models that match\nthe specified values.\n\n\nFILTER\n may a directory to indicate that only models defined in\nthat location are included in the list.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow model details.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "models command"
        },
        {
            "location": "/docs/commands/models-cmd/#models-command",
            "text": "Usage  Options",
            "title": "models command"
        },
        {
            "location": "/docs/commands/models-cmd/#usage",
            "text": "guild models [OPTIONS] [FILTER]... \n   \n     Show available models.  Use one or more  FILTER  arguments to show only models that match\nthe specified values.  FILTER  may a directory to indicate that only models defined in\nthat location are included in the list.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/models-cmd/#options",
            "text": "-v, --verbose \n         Show model details. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/operations-cmd/",
            "text": "operations command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild operations [OPTIONS] [FILTER]...\n\n  \n\n    \nShow model operations.\n\n\nUse one or more \nFILTER\n arguments to show only operations whose\nnames or models match the specified values.\n\n\nFILTER\n may a directory to indicate that only operations of\nmodels defined in that location are included in the list.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow operation details.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "operations command"
        },
        {
            "location": "/docs/commands/operations-cmd/#operations-command",
            "text": "Usage  Options",
            "title": "operations command"
        },
        {
            "location": "/docs/commands/operations-cmd/#usage",
            "text": "guild operations [OPTIONS] [FILTER]... \n   \n     Show model operations.  Use one or more  FILTER  arguments to show only operations whose\nnames or models match the specified values.  FILTER  may a directory to indicate that only operations of\nmodels defined in that location are included in the list.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/operations-cmd/#options",
            "text": "-v, --verbose \n         Show operation details. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/package-cmd/",
            "text": "package command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild package [OPTIONS]\n\n  \n\n    \nCreate a package for distribution.\n\n\nPackages are built from projects that contain guildfile with a\npackage definition, which describes the package to be built.\n\n\nYou may upload the generated package distribution to a PyPI\nrepository by using the \n--upload\n option or to the PyPI test site\nby using \n--upload-test\n.\n\n\nYou may upload to an alternative repository using\n\n--upload-repo\n. \nREPO\n may be a URL or the name of a section in\n\n~/.pypirc\n. For more information on the \n.pypirc\n file, see:\n\n\nhttps://docs.python.org/2/distutils/packageindex.html#pypirc\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-d, --dist-dir DIR\n\n        \nDirectory to create the package distribution in.\n\n      \n\n      \n      \n\n        \n--upload\n\n        \nUpload to PyPI after creating the package.\n\n      \n\n      \n      \n\n        \n--upload-test\n\n        \nUpload to the PyPI test site after creating the package.\n\n      \n\n      \n      \n\n        \n--repo REPO\n\n        \nUpload to \nREPO\n after creating the package.\n\n      \n\n      \n      \n\n        \n-s, --sign\n\n        \nSign a package distribution upload with gpg.\n\n      \n\n      \n      \n\n        \n-i, --identity IDENTITY\n\n        \nGPG identity used to sign upload.\n\n      \n\n      \n      \n\n        \n-u, --user USERNAME\n\n        \nPyPI user name for upload.\n\n      \n\n      \n      \n\n        \n-p, --password PASSWORD\n\n        \nPyPI password for upload.\n\n      \n\n      \n      \n\n        \n-e, --skip-existing\n\n        \nDon't upload if package already exists.\n\n      \n\n      \n      \n\n        \n-c, --comment COMMENT\n\n        \nComment to include with upload.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "package command"
        },
        {
            "location": "/docs/commands/package-cmd/#package-command",
            "text": "Usage  Options",
            "title": "package command"
        },
        {
            "location": "/docs/commands/package-cmd/#usage",
            "text": "guild package [OPTIONS] \n   \n     Create a package for distribution.  Packages are built from projects that contain guildfile with a\npackage definition, which describes the package to be built.  You may upload the generated package distribution to a PyPI\nrepository by using the  --upload  option or to the PyPI test site\nby using  --upload-test .  You may upload to an alternative repository using --upload-repo .  REPO  may be a URL or the name of a section in ~/.pypirc . For more information on the  .pypirc  file, see:  https://docs.python.org/2/distutils/packageindex.html#pypirc",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/package-cmd/#options",
            "text": "-d, --dist-dir DIR \n         Directory to create the package distribution in. \n       \n      \n       \n         --upload \n         Upload to PyPI after creating the package. \n       \n      \n       \n         --upload-test \n         Upload to the PyPI test site after creating the package. \n       \n      \n       \n         --repo REPO \n         Upload to  REPO  after creating the package. \n       \n      \n       \n         -s, --sign \n         Sign a package distribution upload with gpg. \n       \n      \n       \n         -i, --identity IDENTITY \n         GPG identity used to sign upload. \n       \n      \n       \n         -u, --user USERNAME \n         PyPI user name for upload. \n       \n      \n       \n         -p, --password PASSWORD \n         PyPI password for upload. \n       \n      \n       \n         -e, --skip-existing \n         Don't upload if package already exists. \n       \n      \n       \n         -c, --comment COMMENT \n         Comment to include with upload. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/packages-cmd/",
            "text": "packages command\n\n\n\n\nUsage\n\n\nOptions\n\n\nSubcommands\n\n\n\n\n\n  \nUsage\n\n  \nguild packages [OPTIONS] COMMAND [ARGS]...\n\n  \n\n    \nShow or manage packages.\n\n\nIf \nCOMMAND\n is not specified, lists packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-a, --all\n\n        \nShow all packages.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nSubcommands\n\n  \n\n    \n\n      \n      \n\n        \ndelete, rm\n\n        \nUninstall one or more packages.\n\n      \n\n      \n      \n\n        \ninfo\n\n        \nShow information for one or more packages.\n\n      \n\n      \n      \n\n        \nlist, ls\n\n        \nList installed packages.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "packages command"
        },
        {
            "location": "/docs/commands/packages-cmd/#packages-command",
            "text": "Usage  Options  Subcommands",
            "title": "packages command"
        },
        {
            "location": "/docs/commands/packages-cmd/#usage",
            "text": "guild packages [OPTIONS] COMMAND [ARGS]... \n   \n     Show or manage packages.  If  COMMAND  is not specified, lists packages.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/packages-cmd/#options",
            "text": "-a, --all \n         Show all packages. \n       \n      \n       \n         --help \n         Show command help and exit.",
            "title": "Options"
        },
        {
            "location": "/docs/commands/packages-cmd/#subcommands",
            "text": "delete, rm \n         Uninstall one or more packages. \n       \n      \n       \n         info \n         Show information for one or more packages. \n       \n      \n       \n         list, ls \n         List installed packages. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Subcommands"
        },
        {
            "location": "/docs/commands/packages-delete-cmd/",
            "text": "packages delete command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild packages delete [OPTIONS] PACKAGE...\n\n  \n\n    \nUninstall one or more packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before uninstalling.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "packages delete command"
        },
        {
            "location": "/docs/commands/packages-delete-cmd/#packages-delete-command",
            "text": "Usage  Options",
            "title": "packages delete command"
        },
        {
            "location": "/docs/commands/packages-delete-cmd/#usage",
            "text": "guild packages delete [OPTIONS] PACKAGE... \n   \n     Uninstall one or more packages.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/packages-delete-cmd/#options",
            "text": "-y, --yes \n         Do not prompt before uninstalling. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/packages-info-cmd/",
            "text": "packages info command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild packages info [OPTIONS] PACKAGE...\n\n  \n\n    \nShow information for one or more packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow more information.\n\n      \n\n      \n      \n\n        \n-f, --files\n\n        \nShow packages files.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "packages info command"
        },
        {
            "location": "/docs/commands/packages-info-cmd/#packages-info-command",
            "text": "Usage  Options",
            "title": "packages info command"
        },
        {
            "location": "/docs/commands/packages-info-cmd/#usage",
            "text": "guild packages info [OPTIONS] PACKAGE... \n   \n     Show information for one or more packages.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/packages-info-cmd/#options",
            "text": "-v, --verbose \n         Show more information. \n       \n      \n       \n         -f, --files \n         Show packages files. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/packages-list-cmd/",
            "text": "packages list command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild packages list [OPTIONS] [TERM]...\n\n  \n\n    \nList installed packages.\n\n\nSpecify one or more \nTERM\n arguments to show packages matching any\nof the specified values.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-a, --all\n\n        \nShow all packages.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "packages list command"
        },
        {
            "location": "/docs/commands/packages-list-cmd/#packages-list-command",
            "text": "Usage  Options",
            "title": "packages list command"
        },
        {
            "location": "/docs/commands/packages-list-cmd/#usage",
            "text": "guild packages list [OPTIONS] [TERM]... \n   \n     List installed packages.  Specify one or more  TERM  arguments to show packages matching any\nof the specified values.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/packages-list-cmd/#options",
            "text": "-a, --all \n         Show all packages. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/resources-cmd/",
            "text": "resources command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild resources [OPTIONS] [FILTER]...\n\n  \n\n    \nShow available resources.\n\n\nUse one or more \nFILTER\n arguments to show only resources that\nmatch the specified values.\n\n\nFILTER\n may a directory to indicate that only resources defined\nin that location are included in the list.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow resource details.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "resources command"
        },
        {
            "location": "/docs/commands/resources-cmd/#resources-command",
            "text": "Usage  Options",
            "title": "resources command"
        },
        {
            "location": "/docs/commands/resources-cmd/#usage",
            "text": "guild resources [OPTIONS] [FILTER]... \n   \n     Show available resources.  Use one or more  FILTER  arguments to show only resources that\nmatch the specified values.  FILTER  may a directory to indicate that only resources defined\nin that location are included in the list.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/resources-cmd/#options",
            "text": "-v, --verbose \n         Show resource details. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/run-cmd/",
            "text": "run command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild run [OPTIONS] [[MODEL:]OPERATION] [ARG...]\n\n  \n\n    \nRun a model operation.\n\n\nBy default Guild will try to run \nOPERATION\n for the default model\ndefined in a project. If a project location is not specified (see\n\n--project\n option below), Guild looks for a project in the\ncurrent directory.\n\n\nIf \nMODEL\n is specified, Guild will use it instead of the default\nmodel defined in a project.\n\n\n[MODEL]:OPERATION\n may be omitted if \n--rerun\n is specified, in\nwhich case the operation used in \nRUN\n will be used.\n\n\nIf \n--rerun\n is specified, the operation and flags used in \nRUN\n\nwill be applied to the new operation. You may add or redefine\nflags in the new operation. You may also use an alternative\noperation, in which case only the flag values from \nRUN\n will be\napplied. \nRUN\n must be a run ID or unique run ID prefix or the\nspecial value \n0\n, which indicates the latest run.\n\n\nIf \n--restart\n is specified, the specified \nRUN\n is restarted\nin-place using its operation and flags. Unlike rerun, restart does\nnot create a new run, but instead reuses the run directory of\n\nRUN\n. Like a rerun, a restart may specify a different operation\nand additional flags and may use \n0\n for the value of \nRUN\n to\nrestart the latest run. \n--run-dir\n may not be used with\n\n--restart\n.\n\n\n--rerun\n and \n--restart\n may not both be used.\n\n\nTo run an operation outside of Guild's run management facility,\nuse \n--run-dir\n or \n--stage\n to specify an alternative run\ndirectory. These options are useful when developing or debugging\nan operation. Use \n--stage\n to prepare a run directory for an\noperation without running the operation itself. This is useful\nwhen you want to verify dependency resolution and pre-processing\nor manually run an operation in a prepared directory.\n\n\nNOTE:\n Runs started with \n--run-dir\n are not visible to Guild\nand will not appear in run listings.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-l, --label LABEL\n\n        \nSet a label for the run.\n\n      \n\n      \n      \n\n        \n--run-dir DIR\n\n        \nUse alternative run directory DIR. Cannot be used with --stage.\n\n      \n\n      \n      \n\n        \n--stage DIR\n\n        \nStage an operation in DIR but do not run. Cannot be used with --run-dir.\n\n      \n\n      \n      \n\n        \n-r, --rerun RUN\n\n        \nUse the operation and flags from RUN. Flags may be added or redefined in this operation. Cannot be used with --restart.\n\n      \n\n      \n      \n\n        \n-s, --restart RUN\n\n        \nRestart RUN in-place without creating a new run. Cannot be used with --rerun or --run-dir.\n\n      \n\n      \n      \n\n        \n--disable-plugins LIST\n\n        \nA comma separated list of plugin names to disable. Use 'all' to disable all plugins.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before running operation.\n\n      \n\n      \n      \n\n        \n-n, --no-wait\n\n        \nDon't wait for a remote operation to complete. Ignored if run is local.\n\n      \n\n      \n      \n\n        \n--set-trace\n\n        \nEnter the Python debugger at the operation entry point.\n\n      \n\n      \n      \n\n        \n--print-cmd\n\n        \nShow operation command and exit.\n\n      \n\n      \n      \n\n        \n--print-env\n\n        \nShow operation environment and exit.\n\n      \n\n      \n      \n\n        \n--help-model\n\n        \nShow model help and exit.\n\n      \n\n      \n      \n\n        \n--help-op\n\n        \nShow operation help and exit.\n\n      \n\n      \n      \n\n        \n-w, --workflow\n\n        \nExperimental support for workflow.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "run command"
        },
        {
            "location": "/docs/commands/run-cmd/#run-command",
            "text": "Usage  Options",
            "title": "run command"
        },
        {
            "location": "/docs/commands/run-cmd/#usage",
            "text": "guild run [OPTIONS] [[MODEL:]OPERATION] [ARG...] \n   \n     Run a model operation.  By default Guild will try to run  OPERATION  for the default model\ndefined in a project. If a project location is not specified (see --project  option below), Guild looks for a project in the\ncurrent directory.  If  MODEL  is specified, Guild will use it instead of the default\nmodel defined in a project.  [MODEL]:OPERATION  may be omitted if  --rerun  is specified, in\nwhich case the operation used in  RUN  will be used.  If  --rerun  is specified, the operation and flags used in  RUN \nwill be applied to the new operation. You may add or redefine\nflags in the new operation. You may also use an alternative\noperation, in which case only the flag values from  RUN  will be\napplied.  RUN  must be a run ID or unique run ID prefix or the\nspecial value  0 , which indicates the latest run.  If  --restart  is specified, the specified  RUN  is restarted\nin-place using its operation and flags. Unlike rerun, restart does\nnot create a new run, but instead reuses the run directory of RUN . Like a rerun, a restart may specify a different operation\nand additional flags and may use  0  for the value of  RUN  to\nrestart the latest run.  --run-dir  may not be used with --restart .  --rerun  and  --restart  may not both be used.  To run an operation outside of Guild's run management facility,\nuse  --run-dir  or  --stage  to specify an alternative run\ndirectory. These options are useful when developing or debugging\nan operation. Use  --stage  to prepare a run directory for an\noperation without running the operation itself. This is useful\nwhen you want to verify dependency resolution and pre-processing\nor manually run an operation in a prepared directory.  NOTE:  Runs started with  --run-dir  are not visible to Guild\nand will not appear in run listings.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/run-cmd/#options",
            "text": "-l, --label LABEL \n         Set a label for the run. \n       \n      \n       \n         --run-dir DIR \n         Use alternative run directory DIR. Cannot be used with --stage. \n       \n      \n       \n         --stage DIR \n         Stage an operation in DIR but do not run. Cannot be used with --run-dir. \n       \n      \n       \n         -r, --rerun RUN \n         Use the operation and flags from RUN. Flags may be added or redefined in this operation. Cannot be used with --restart. \n       \n      \n       \n         -s, --restart RUN \n         Restart RUN in-place without creating a new run. Cannot be used with --rerun or --run-dir. \n       \n      \n       \n         --disable-plugins LIST \n         A comma separated list of plugin names to disable. Use 'all' to disable all plugins. \n       \n      \n       \n         -y, --yes \n         Do not prompt before running operation. \n       \n      \n       \n         -n, --no-wait \n         Don't wait for a remote operation to complete. Ignored if run is local. \n       \n      \n       \n         --set-trace \n         Enter the Python debugger at the operation entry point. \n       \n      \n       \n         --print-cmd \n         Show operation command and exit. \n       \n      \n       \n         --print-env \n         Show operation environment and exit. \n       \n      \n       \n         --help-model \n         Show model help and exit. \n       \n      \n       \n         --help-op \n         Show operation help and exit. \n       \n      \n       \n         -w, --workflow \n         Experimental support for workflow. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/runs-cmd/",
            "text": "runs command\n\n\n\n\nUsage\n\n\nOptions\n\n\nSubcommands\n\n\n\n\n\n  \nUsage\n\n  \nguild runs [OPTIONS] COMMAND [ARGS]...\n\n  \n\n    \nShow or manage runs.\n\n\nIf \nCOMMAND\n is omitted, lists runs. Refer to \nruns list\n for more information on the \nlist\n command.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-d, --deleted\n\n        \nShow deleted runs.\n\n      \n\n      \n      \n\n        \n--archive DIR\n\n        \nShow archived runs in DIR\n\n      \n\n      \n      \n\n        \n-a, --all\n\n        \nShow all runs (by default only the last 20 runs are shown)\n\n      \n\n      \n      \n\n        \n-m, --more\n\n        \nShow 20 more runs. Maybe used multiple times.\n\n      \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow run details.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nSubcommands\n\n  \n\n    \n\n      \n      \n\n        \ndelete, rm\n\n        \nDelete one or more runs.\n\n      \n\n      \n      \n\n        \nexport\n\n        \nExport one or more runs.\n\n      \n\n      \n      \n\n        \nimport\n\n        \nImport one or more runs from \nARCHIVE\n.\n\n      \n\n      \n      \n\n        \ninfo\n\n        \nShow run information.\n\n      \n\n      \n      \n\n        \nlabel\n\n        \nSet run labels.\n\n      \n\n      \n      \n\n        \nlist, ls\n\n        \nList runs.\n\n      \n\n      \n      \n\n        \npull\n\n        \nCopy one or more runs from a remote location.\n\n      \n\n      \n      \n\n        \npurge\n\n        \nPermanentaly delete one or more deleted runs.\n\n      \n\n      \n      \n\n        \npush\n\n        \nCopy one or more runs to a remote location.\n\n      \n\n      \n      \n\n        \nrestore\n\n        \nRestore one or more deleted runs.\n\n      \n\n      \n      \n\n        \nstop\n\n        \nStop one or more runs.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "runs command"
        },
        {
            "location": "/docs/commands/runs-cmd/#runs-command",
            "text": "Usage  Options  Subcommands",
            "title": "runs command"
        },
        {
            "location": "/docs/commands/runs-cmd/#usage",
            "text": "guild runs [OPTIONS] COMMAND [ARGS]... \n   \n     Show or manage runs.  If  COMMAND  is omitted, lists runs. Refer to  runs list  for more information on the  list  command.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/runs-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -d, --deleted \n         Show deleted runs. \n       \n      \n       \n         --archive DIR \n         Show archived runs in DIR \n       \n      \n       \n         -a, --all \n         Show all runs (by default only the last 20 runs are shown) \n       \n      \n       \n         -m, --more \n         Show 20 more runs. Maybe used multiple times. \n       \n      \n       \n         -v, --verbose \n         Show run details. \n       \n      \n       \n         --help \n         Show command help and exit.",
            "title": "Options"
        },
        {
            "location": "/docs/commands/runs-cmd/#subcommands",
            "text": "delete, rm \n         Delete one or more runs. \n       \n      \n       \n         export \n         Export one or more runs. \n       \n      \n       \n         import \n         Import one or more runs from  ARCHIVE . \n       \n      \n       \n         info \n         Show run information. \n       \n      \n       \n         label \n         Set run labels. \n       \n      \n       \n         list, ls \n         List runs. \n       \n      \n       \n         pull \n         Copy one or more runs from a remote location. \n       \n      \n       \n         purge \n         Permanentaly delete one or more deleted runs. \n       \n      \n       \n         push \n         Copy one or more runs to a remote location. \n       \n      \n       \n         restore \n         Restore one or more deleted runs. \n       \n      \n       \n         stop \n         Stop one or more runs. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Subcommands"
        },
        {
            "location": "/docs/commands/runs-delete-cmd/",
            "text": "runs delete command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs delete [OPTIONS] [RUN...]\n\n  \n\n    \nDelete one or more runs.\n\n\nRuns are deleting by selecting them with \nRUN\n arguments. If a\n\nRUN\n argument is not specified, all runs matching the filter\ncriteria are deleted. See SELECTING RUNS and FILTERING topics\nbelow for more information on how runs are selected.\n\n\nBy default, Guild will display the list of runs to be deleted and\nask you to confirm the operation. If you want to delete the runs\nwithout being prompted, use the \n--yes\n option.\n\n\nWARNING\n: Take care when deleting runs using indexes as the\nruns selected with indexes can change. Review the list of runs\ncarefully before confirming a delete operation.\n\n\nIf a run is still running, Guild will stop it first before\ndeleting it.\n\n\nIf you delete a run by mistake, provided you didn't use the\n\n--permanent\n option, you can restore it using \nguild runs\nrestore\n.\n\n\nIf you want to permanently delete runs, use the \n--permanent\n\noption.\n\n\nWARNING\n: Permanentaly deleted runs cannot be restored.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before deleting.\n\n      \n\n      \n      \n\n        \n-p, --permanent\n\n        \nPermanentaly delete runs so they cannot be recovered.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "runs delete command"
        },
        {
            "location": "/docs/commands/runs-delete-cmd/#runs-delete-command",
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status    Options",
            "title": "runs delete command"
        },
        {
            "location": "/docs/commands/runs-delete-cmd/#usage",
            "text": "guild runs delete [OPTIONS] [RUN...] \n   \n     Delete one or more runs.  Runs are deleting by selecting them with  RUN  arguments. If a RUN  argument is not specified, all runs matching the filter\ncriteria are deleted. See SELECTING RUNS and FILTERING topics\nbelow for more information on how runs are selected.  By default, Guild will display the list of runs to be deleted and\nask you to confirm the operation. If you want to delete the runs\nwithout being prompted, use the  --yes  option.  WARNING : Take care when deleting runs using indexes as the\nruns selected with indexes can change. Review the list of runs\ncarefully before confirming a delete operation.  If a run is still running, Guild will stop it first before\ndeleting it.  If you delete a run by mistake, provided you didn't use the --permanent  option, you can restore it using  guild runs\nrestore .  If you want to permanently delete runs, use the  --permanent \noption.  WARNING : Permanentaly deleted runs cannot be restored.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/runs-delete-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/runs-delete-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/runs-delete-cmd/#filtering-by-run-status",
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.",
            "title": "Filtering by run status"
        },
        {
            "location": "/docs/commands/runs-delete-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -y, --yes \n         Do not prompt before deleting. \n       \n      \n       \n         -p, --permanent \n         Permanentaly delete runs so they cannot be recovered. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/runs-info-cmd/",
            "text": "runs info command\n\n\n\n\nUsage\n\n\nSelecting a run\n\n\nAdditional information\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs info [OPTIONS] [RUN]\n\n  \n\n    \nShow run information.\n\n\nThis command shows information for a single run.\n\n\nSelecting a run\n\n\nYou may specify a run using a run ID, a run ID prefix, or a\nzero-based index corresponding to a run returned by the list\ncommand.\n\n\nIf RUN isn't specified, the latest run is selected.\n\n\nAdditional information\n\n\nYou can show additional run information by specifying option\nflags. You may use multiple flags to show more information. Refer\nto the options below for what additional information is available.\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-e, --env\n\n        \nShow run environment.\n\n      \n\n      \n      \n\n        \n-f, --flags\n\n        \nShow run flags.\n\n      \n\n      \n      \n\n        \n-d, --deps\n\n        \nShow resolved dependencies.\n\n      \n\n      \n      \n\n        \n-O, --output\n\n        \nShow run output.\n\n      \n\n      \n      \n\n        \n-p, --page-output\n\n        \nShow only run output in a pager\n\n      \n\n      \n      \n\n        \n-F, --files\n\n        \nShow run files.\n\n      \n\n      \n      \n\n        \n-L, --follow-links\n\n        \nFollow links when showing files.\n\n      \n\n      \n      \n\n        \n-A, --all-files\n\n        \nShow all run files including those generated by Guild.\n\n      \n\n      \n      \n\n        \n-P, --full-path\n\n        \nDisplay full path when showing files.\n\n      \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "runs info command"
        },
        {
            "location": "/docs/commands/runs-info-cmd/#runs-info-command",
            "text": "Usage  Selecting a run  Additional information  Filtering by operation and label  Filtering by run status    Options",
            "title": "runs info command"
        },
        {
            "location": "/docs/commands/runs-info-cmd/#usage",
            "text": "guild runs info [OPTIONS] [RUN] \n   \n     Show run information.  This command shows information for a single run.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/runs-info-cmd/#selecting-a-run",
            "text": "You may specify a run using a run ID, a run ID prefix, or a\nzero-based index corresponding to a run returned by the list\ncommand.  If RUN isn't specified, the latest run is selected.",
            "title": "Selecting a run"
        },
        {
            "location": "/docs/commands/runs-info-cmd/#additional-information",
            "text": "You can show additional run information by specifying option\nflags. You may use multiple flags to show more information. Refer\nto the options below for what additional information is available.",
            "title": "Additional information"
        },
        {
            "location": "/docs/commands/runs-info-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/runs-info-cmd/#filtering-by-run-status",
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.",
            "title": "Filtering by run status"
        },
        {
            "location": "/docs/commands/runs-info-cmd/#options",
            "text": "-e, --env \n         Show run environment. \n       \n      \n       \n         -f, --flags \n         Show run flags. \n       \n      \n       \n         -d, --deps \n         Show resolved dependencies. \n       \n      \n       \n         -O, --output \n         Show run output. \n       \n      \n       \n         -p, --page-output \n         Show only run output in a pager \n       \n      \n       \n         -F, --files \n         Show run files. \n       \n      \n       \n         -L, --follow-links \n         Follow links when showing files. \n       \n      \n       \n         -A, --all-files \n         Show all run files including those generated by Guild. \n       \n      \n       \n         -P, --full-path \n         Display full path when showing files. \n       \n      \n       \n         -o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/runs-label-cmd/",
            "text": "runs label command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs label [OPTIONS] [RUN...] [LABEL]\n\n  \n\n    \nSet run labels.\n\n\nIf \nLABEL\n is provided, the command will label the selected\nruns. To clear a run label, use the \n--clear\n option.\n\n\nSelect runs to modify using one or more \nRUN\n arguments. See\nSELECT RUNS below for information on selecting runs.\n\n\nIf \nRUN\n isn't specified, the most recent run is selected.\n\n\nBy default Guild will prompt you before making any changes. If you\nwant to apply the changes without being prompted, use the\n\n--yes\n option.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n0\n is assumed (the most\nrecent run).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-c, --clear\n\n        \nClear the run's label.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before modifying labels.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "runs label command"
        },
        {
            "location": "/docs/commands/runs-label-cmd/#runs-label-command",
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status    Options",
            "title": "runs label command"
        },
        {
            "location": "/docs/commands/runs-label-cmd/#usage",
            "text": "guild runs label [OPTIONS] [RUN...] [LABEL] \n   \n     Set run labels.  If  LABEL  is provided, the command will label the selected\nruns. To clear a run label, use the  --clear  option.  Select runs to modify using one or more  RUN  arguments. See\nSELECT RUNS below for information on selecting runs.  If  RUN  isn't specified, the most recent run is selected.  By default Guild will prompt you before making any changes. If you\nwant to apply the changes without being prompted, use the --yes  option.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/runs-label-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  0  is assumed (the most\nrecent run).",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/runs-label-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/runs-label-cmd/#filtering-by-run-status",
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.",
            "title": "Filtering by run status"
        },
        {
            "location": "/docs/commands/runs-label-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -c, --clear \n         Clear the run's label. \n       \n      \n       \n         -y, --yes \n         Do not prompt before modifying labels. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/runs-list-cmd/",
            "text": "runs list command\n\n\n\n\nUsage\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\nShow deleted runs\n\n\nShow archives runs\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs list [OPTIONS]\n\n  \n\n    \nList runs.\n\n\nRun lists may be filtered using a variety of options. See below\nfor details.\n\n\nBy default, the last 20 runs are shown. Use \n-a, --all\n to show\nall runs, or \n-m\n to show more 20 more runs. You may use \n-m\n\nmultiple times.\n\n\nRun indexes are included in list output for a specific listing,\nwhich is based on the available runs, their states, and the\nspecified filters. You may use the indexes in run selection\ncommands (e.g. \nruns delete\n, \ncompare\n, etc.) but note that\nthese indexes will change as runs are started, deleted, or run\nstatus changes.\n\n\nTo show run detail, use \n--verbose\n.\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n\nShow deleted runs\n\n\nUse \n--deleted\n to show deleted runs. You can use the listing for\nrun IDs and indexes to use in \nruns restore\n (restore runs) and\n\nruns purge\n (permanently delete runs).\n\n\nShow archives runs\n\n\nUse \n--archive\n to show runs in an archive directory. This option\nmay not be used with \n--delete\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-d, --deleted\n\n        \nShow deleted runs.\n\n      \n\n      \n      \n\n        \n--archive DIR\n\n        \nShow archived runs in DIR\n\n      \n\n      \n      \n\n        \n-a, --all\n\n        \nShow all runs (by default only the last 20 runs are shown)\n\n      \n\n      \n      \n\n        \n-m, --more\n\n        \nShow 20 more runs. Maybe used multiple times.\n\n      \n\n      \n      \n\n        \n-v, --verbose\n\n        \nShow run details.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "runs list command"
        },
        {
            "location": "/docs/commands/runs-list-cmd/#runs-list-command",
            "text": "Usage  Filtering by operation and label  Filtering by run status  Show deleted runs  Show archives runs    Options",
            "title": "runs list command"
        },
        {
            "location": "/docs/commands/runs-list-cmd/#usage",
            "text": "guild runs list [OPTIONS] \n   \n     List runs.  Run lists may be filtered using a variety of options. See below\nfor details.  By default, the last 20 runs are shown. Use  -a, --all  to show\nall runs, or  -m  to show more 20 more runs. You may use  -m \nmultiple times.  Run indexes are included in list output for a specific listing,\nwhich is based on the available runs, their states, and the\nspecified filters. You may use the indexes in run selection\ncommands (e.g.  runs delete ,  compare , etc.) but note that\nthese indexes will change as runs are started, deleted, or run\nstatus changes.  To show run detail, use  --verbose .",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/runs-list-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/runs-list-cmd/#filtering-by-run-status",
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.",
            "title": "Filtering by run status"
        },
        {
            "location": "/docs/commands/runs-list-cmd/#show-deleted-runs",
            "text": "Use  --deleted  to show deleted runs. You can use the listing for\nrun IDs and indexes to use in  runs restore  (restore runs) and runs purge  (permanently delete runs).",
            "title": "Show deleted runs"
        },
        {
            "location": "/docs/commands/runs-list-cmd/#show-archives-runs",
            "text": "Use  --archive  to show runs in an archive directory. This option\nmay not be used with  --delete .",
            "title": "Show archives runs"
        },
        {
            "location": "/docs/commands/runs-list-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -d, --deleted \n         Show deleted runs. \n       \n      \n       \n         --archive DIR \n         Show archived runs in DIR \n       \n      \n       \n         -a, --all \n         Show all runs (by default only the last 20 runs are shown) \n       \n      \n       \n         -m, --more \n         Show 20 more runs. Maybe used multiple times. \n       \n      \n       \n         -v, --verbose \n         Show run details. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/runs-purge-cmd/",
            "text": "runs purge command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs purge [OPTIONS] [RUN...]\n\n  \n\n    \nPermanentaly delete one or more deleted runs.\n\n\nWARNING\n: Purged runs cannot be recovered!\n\n\nRuns are purged (i.e. permanently deleted) by selecting them with\n\nRUN\n arguments. If a \nRUN\n argument is not specified, all runs\nmatching the filter criteria are purged. See SELECTING RUNS and\nFILTERING topics below for more information on how runs are\nselected.\n\n\nUse \nguild runs list --deleted\n for a list of runs that can be\npurged.\n\n\nBy default, Guild will display the list of runs to be purged and\nask you to confirm the operation. If you want to purge the runs\nwithout being prompted, use the \n--yes\n option.\n\n\nWARNING\n: Take care when purging runs using indexes as the runs\nselected with indexes can change. Review the list of runs\ncarefully before confirming a purge operation.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before purging.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "runs purge command"
        },
        {
            "location": "/docs/commands/runs-purge-cmd/#runs-purge-command",
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status    Options",
            "title": "runs purge command"
        },
        {
            "location": "/docs/commands/runs-purge-cmd/#usage",
            "text": "guild runs purge [OPTIONS] [RUN...] \n   \n     Permanentaly delete one or more deleted runs.  WARNING : Purged runs cannot be recovered!  Runs are purged (i.e. permanently deleted) by selecting them with RUN  arguments. If a  RUN  argument is not specified, all runs\nmatching the filter criteria are purged. See SELECTING RUNS and\nFILTERING topics below for more information on how runs are\nselected.  Use  guild runs list --deleted  for a list of runs that can be\npurged.  By default, Guild will display the list of runs to be purged and\nask you to confirm the operation. If you want to purge the runs\nwithout being prompted, use the  --yes  option.  WARNING : Take care when purging runs using indexes as the runs\nselected with indexes can change. Review the list of runs\ncarefully before confirming a purge operation.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/runs-purge-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/runs-purge-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/runs-purge-cmd/#filtering-by-run-status",
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.",
            "title": "Filtering by run status"
        },
        {
            "location": "/docs/commands/runs-purge-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -y, --yes \n         Do not prompt before purging. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/runs-restore-cmd/",
            "text": "runs restore command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\nFiltering by run status\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs restore [OPTIONS] [RUN...]\n\n  \n\n    \nRestore one or more deleted runs.\n\n\nRuns are restored by selecting them with \nRUN\n arguments. If a\n\nRUN\n argument is not specified, all runs matching the filter\ncriteria are restored. See SELECTING RUNS and FILTERING topics\nbelow for more information on how runs are selected.\n\n\nUse \nguild runs list --deleted\n for a list of runs that can be\nrestored.\n\n\nBy default, Guild will display the list of runs to be restored and\nask you to confirm the operation. If you want to restore the runs\nwithout being prompted, use the \n--yes\n option.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n\nFiltering by run status\n\n\nRuns may also be filtered by specifying one or more status\nfilters: \n--running\n, \n--completed\n, \n--error\n, and\n\n--terminated\n. These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use \n--terminated\n--error\n, or the short form \n-ET\n.\n\n\nStatus filters are applied before \nRUN\n indexes are resolved. For\nexample, a run index of \n0\n is the latest run that matches the\nstatus filters.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before restoring.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "runs restore command"
        },
        {
            "location": "/docs/commands/runs-restore-cmd/#runs-restore-command",
            "text": "Usage  Selecting runs  Filtering by operation and label  Filtering by run status    Options",
            "title": "runs restore command"
        },
        {
            "location": "/docs/commands/runs-restore-cmd/#usage",
            "text": "guild runs restore [OPTIONS] [RUN...] \n   \n     Restore one or more deleted runs.  Runs are restored by selecting them with  RUN  arguments. If a RUN  argument is not specified, all runs matching the filter\ncriteria are restored. See SELECTING RUNS and FILTERING topics\nbelow for more information on how runs are selected.  Use  guild runs list --deleted  for a list of runs that can be\nrestored.  By default, Guild will display the list of runs to be restored and\nask you to confirm the operation. If you want to restore the runs\nwithout being prompted, use the  --yes  option.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/runs-restore-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/runs-restore-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/runs-restore-cmd/#filtering-by-run-status",
            "text": "Runs may also be filtered by specifying one or more status\nfilters:  --running ,  --completed ,  --error , and --terminated . These may be used together to include runs that\nmatch any of the filters. For example to only include runs that\nwere either terminated or exited with an error, use  --terminated\n--error , or the short form  -ET .  Status filters are applied before  RUN  indexes are resolved. For\nexample, a run index of  0  is the latest run that matches the\nstatus filters.",
            "title": "Filtering by run status"
        },
        {
            "location": "/docs/commands/runs-restore-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         -y, --yes \n         Do not prompt before restoring. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/runs-stop-cmd/",
            "text": "runs stop command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild runs stop [OPTIONS] [RUN...]\n\n  \n\n    \nStop one or more runs.\n\n\nRuns are stopped by specifying one or more RUN arguments. See\nSELECTING RUNS and FILTER topics below for information on\nselecting runs to be stopped.\n\n\nOnly runs with status of 'running' are considered for this\noperation.\n\n\nIf a \nRUN\n is not specified, the latest selected run is stopped.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n0\n is assumed (the most\nrecent run with status 'running').\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before stopping.\n\n      \n\n      \n      \n\n        \n-n, --no-wait\n\n        \nDon't wait for remote runs to stop.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "runs stop command"
        },
        {
            "location": "/docs/commands/runs-stop-cmd/#runs-stop-command",
            "text": "Usage  Selecting runs  Filtering by operation and label    Options",
            "title": "runs stop command"
        },
        {
            "location": "/docs/commands/runs-stop-cmd/#usage",
            "text": "guild runs stop [OPTIONS] [RUN...] \n   \n     Stop one or more runs.  Runs are stopped by specifying one or more RUN arguments. See\nSELECTING RUNS and FILTER topics below for information on\nselecting runs to be stopped.  Only runs with status of 'running' are considered for this\noperation.  If a  RUN  is not specified, the latest selected run is stopped.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/runs-stop-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  0  is assumed (the most\nrecent run with status 'running').",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/runs-stop-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/runs-stop-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -y, --yes \n         Do not prompt before stopping. \n       \n      \n       \n         -n, --no-wait \n         Don't wait for remote runs to stop. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/search-cmd/",
            "text": "search command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild search [OPTIONS] TERM...\n\n  \n\n    \nSearch for a package.\n\n\nSpecify one or more \nTERM\n arguments to search for.\n\n\nBy default, only Guild packages are returned. To search all\npackages, use the \n--all\n option.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-a, --all\n\n        \nSearch all packages.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "search command"
        },
        {
            "location": "/docs/commands/search-cmd/#search-command",
            "text": "Usage  Options",
            "title": "search command"
        },
        {
            "location": "/docs/commands/search-cmd/#usage",
            "text": "guild search [OPTIONS] TERM... \n   \n     Search for a package.  Specify one or more  TERM  arguments to search for.  By default, only Guild packages are returned. To search all\npackages, use the  --all  option.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/search-cmd/#options",
            "text": "-a, --all \n         Search all packages. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/shell-cmd/",
            "text": "shell command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild shell [OPTIONS]\n\n  \n\n    \nStart a Python shell for API use.\n\n\nNOTE:\n This is a developer feature.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "shell command"
        },
        {
            "location": "/docs/commands/shell-cmd/#shell-command",
            "text": "Usage  Options",
            "title": "shell command"
        },
        {
            "location": "/docs/commands/shell-cmd/#usage",
            "text": "guild shell [OPTIONS] \n   \n     Start a Python shell for API use.  NOTE:  This is a developer feature.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/shell-cmd/#options",
            "text": "--help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/stop-cmd/",
            "text": "stop command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild stop [OPTIONS] [RUN...]\n\n  \n\n    \nStop one or more runs.\n\n\nRuns are stopped by specifying one or more RUN arguments. See\nSELECTING RUNS and FILTER topics below for information on\nselecting runs to be stopped.\n\n\nOnly runs with status of 'running' are considered for this\noperation.\n\n\nIf a \nRUN\n is not specified, the latest selected run is stopped.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n0\n is assumed (the most\nrecent run with status 'running').\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before stopping.\n\n      \n\n      \n      \n\n        \n-n, --no-wait\n\n        \nDon't wait for remote runs to stop.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "stop command"
        },
        {
            "location": "/docs/commands/stop-cmd/#stop-command",
            "text": "Usage  Selecting runs  Filtering by operation and label    Options",
            "title": "stop command"
        },
        {
            "location": "/docs/commands/stop-cmd/#usage",
            "text": "guild stop [OPTIONS] [RUN...] \n   \n     Stop one or more runs.  Runs are stopped by specifying one or more RUN arguments. See\nSELECTING RUNS and FILTER topics below for information on\nselecting runs to be stopped.  Only runs with status of 'running' are considered for this\noperation.  If a  RUN  is not specified, the latest selected run is stopped.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/stop-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  0  is assumed (the most\nrecent run with status 'running').",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/stop-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/stop-cmd/#options",
            "text": "-o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -y, --yes \n         Do not prompt before stopping. \n       \n      \n       \n         -n, --no-wait \n         Don't wait for remote runs to stop. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/sync-cmd/",
            "text": "sync command\n\n\n\n\nUsage\n\n\nSelecting runs\n\n\nFiltering by operation and label\n\n\n\n\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild sync [OPTIONS] [RUN...]\n\n  \n\n    \nSynchronize remote runs.\n\n\nA remote run is an operation that runs on another system. Guild\nkeeps track of where each remote run is located and can\nsynchronize with it. This includes downloading files generated by\nthe run as well as updating run status.\n\n\nBy default, Guild synchronizes once with the remote run and\nexits. If you want to automatically synchronize with the run while\nwatching its output, use the \n--watch\n option.\n\n\nYou can only watch one running operation at a time. If you don't\nspecify a RUN with the \n--watch\n option, Guild will watch the most\nrecently started running operation.\n\n\nWhen a remote status stops (it finished successfully, is\nterminated, or exits with an error), Guild will no longer\nsynchronize with it.\n\n\nYou can synchronize specific runs by selecting them using \nRUN\n\narguments. For more information, see SELECTING RUNS and FILTERING\ntopics below.\n\n\nSelecting runs\n\n\nYou may use one or more \nRUN\n arguments to limit the runs that are\nselected. \nRUN\n may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.\n\n\nIndexes may also be specified in ranges in the form \nSTART:END\n\nwhere \nSTART\n is the start index and \nEND\n is the end\nindex. Either \nSTART\n or \nEND\n may be omitted. If \nSTART\n is\nomitted, all runs up to \nEND\n are selected. If \nEND\n id omitted,\nall runs from \nSTART\n on are selected. If both \nSTART\n and \nEND\n\nare omitted (i.e. the \n:\n char is used by itself) all runs are\nselected.\n\n\nIf a \nRUN\n argument is not specified, \n:\n is assumed (all runs\nare selected).\n\n\nFiltering by operation and label\n\n\nRuns may be filtered by operation using \n--operation\n.  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.\n\n\nUse \n--label\n to only include runs with labels matching a\nspecified value.\n\n\n--operation\n and \n--label\n may be used multiple times to expand\nthe runs that are included.\n\n\nUse \n--unlabeled\n to only include runs without labels. This option\nmay not be used with \n--label\n.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-w, --watch\n\n        \nWatch a remote run and synchronize in the background.\n\n      \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "sync command"
        },
        {
            "location": "/docs/commands/sync-cmd/#sync-command",
            "text": "Usage  Selecting runs  Filtering by operation and label    Options",
            "title": "sync command"
        },
        {
            "location": "/docs/commands/sync-cmd/#usage",
            "text": "guild sync [OPTIONS] [RUN...] \n   \n     Synchronize remote runs.  A remote run is an operation that runs on another system. Guild\nkeeps track of where each remote run is located and can\nsynchronize with it. This includes downloading files generated by\nthe run as well as updating run status.  By default, Guild synchronizes once with the remote run and\nexits. If you want to automatically synchronize with the run while\nwatching its output, use the  --watch  option.  You can only watch one running operation at a time. If you don't\nspecify a RUN with the  --watch  option, Guild will watch the most\nrecently started running operation.  When a remote status stops (it finished successfully, is\nterminated, or exits with an error), Guild will no longer\nsynchronize with it.  You can synchronize specific runs by selecting them using  RUN \narguments. For more information, see SELECTING RUNS and FILTERING\ntopics below.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/sync-cmd/#selecting-runs",
            "text": "You may use one or more  RUN  arguments to limit the runs that are\nselected.  RUN  may be a run ID, a run ID prefix, or a zero-based\nindex corresponding to a run returned by the list command.  Indexes may also be specified in ranges in the form  START:END \nwhere  START  is the start index and  END  is the end\nindex. Either  START  or  END  may be omitted. If  START  is\nomitted, all runs up to  END  are selected. If  END  id omitted,\nall runs from  START  on are selected. If both  START  and  END \nare omitted (i.e. the  :  char is used by itself) all runs are\nselected.  If a  RUN  argument is not specified,  :  is assumed (all runs\nare selected).",
            "title": "Selecting runs"
        },
        {
            "location": "/docs/commands/sync-cmd/#filtering-by-operation-and-label",
            "text": "Runs may be filtered by operation using  --operation .  A run is\nonly included if any part of its full operation name, including\nthe package and model name, matches the value.  Use  --label  to only include runs with labels matching a\nspecified value.  --operation  and  --label  may be used multiple times to expand\nthe runs that are included.  Use  --unlabeled  to only include runs without labels. This option\nmay not be used with  --label .",
            "title": "Filtering by operation and label"
        },
        {
            "location": "/docs/commands/sync-cmd/#options",
            "text": "-w, --watch \n         Watch a remote run and synchronize in the background. \n       \n      \n       \n         -o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/tensorboard-cmd/",
            "text": "tensorboard command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild tensorboard [OPTIONS] [RUN...]\n\n  \n\n    \nVisualize runs with TensorBoard.\n\n\nThis command will start a TensorBoard process and open a browser\nwindow for you. TensorBoard will show the views that are selected\nusing the commands filters. This list corresponds to the the runs\nshown when running \nguild runs\n.\n\n\nThis command will not exit until you type \nCTRL-c\n to stop it.\n\n\nIf you'd like to change the filters used to select runs, stop the\ncommand and re-run it with a different set of filters. You may\nalternatively start another instance of TensorBoard in a separate\nconsole.\n\n\nTensorBoard will automatically refresh with the current run data.\n\n\nIf you're prefer that Guild not open a browser window, run the\ncommand with the \n--no-open\n option.\n\n\nBy default, Guild will start the TensorBoard process on a randomly\nselected free port. If you'd like to specify the port that\nTensorBoard runs on, use the \n--port\n option.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-h, --host HOST\n\n        \nName of host interface to listen on.\n\n      \n\n      \n      \n\n        \n-p, --port PORT\n\n        \nPort to listen on.\n\n      \n\n      \n      \n\n        \n--refresh-interval SECONDS\n\n        \nRefresh interval (defaults to 5 seconds).\n\n      \n\n      \n      \n\n        \n-n, --no-open\n\n        \nDon't open TensorBoard in a browser.\n\n      \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "tensorboard command"
        },
        {
            "location": "/docs/commands/tensorboard-cmd/#tensorboard-command",
            "text": "Usage  Options",
            "title": "tensorboard command"
        },
        {
            "location": "/docs/commands/tensorboard-cmd/#usage",
            "text": "guild tensorboard [OPTIONS] [RUN...] \n   \n     Visualize runs with TensorBoard.  This command will start a TensorBoard process and open a browser\nwindow for you. TensorBoard will show the views that are selected\nusing the commands filters. This list corresponds to the the runs\nshown when running  guild runs .  This command will not exit until you type  CTRL-c  to stop it.  If you'd like to change the filters used to select runs, stop the\ncommand and re-run it with a different set of filters. You may\nalternatively start another instance of TensorBoard in a separate\nconsole.  TensorBoard will automatically refresh with the current run data.  If you're prefer that Guild not open a browser window, run the\ncommand with the  --no-open  option.  By default, Guild will start the TensorBoard process on a randomly\nselected free port. If you'd like to specify the port that\nTensorBoard runs on, use the  --port  option.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/tensorboard-cmd/#options",
            "text": "-h, --host HOST \n         Name of host interface to listen on. \n       \n      \n       \n         -p, --port PORT \n         Port to listen on. \n       \n      \n       \n         --refresh-interval SECONDS \n         Refresh interval (defaults to 5 seconds). \n       \n      \n       \n         -n, --no-open \n         Don't open TensorBoard in a browser. \n       \n      \n       \n         -o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/tensorflow-cmd/",
            "text": "tensorflow command\n\n\n\n\nUsage\n\n\nOptions\n\n\nSubcommands\n\n\n\n\n\n  \nUsage\n\n  \nguild tensorflow [OPTIONS] COMMAND [ARGS]...\n\n  \n\n    \nCollection of TensorFlow tools.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nSubcommands\n\n  \n\n    \n\n      \n      \n\n        \ninspect\n\n        \nInspect a TensorFlow checkpoint file.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "tensorflow command"
        },
        {
            "location": "/docs/commands/tensorflow-cmd/#tensorflow-command",
            "text": "Usage  Options  Subcommands",
            "title": "tensorflow command"
        },
        {
            "location": "/docs/commands/tensorflow-cmd/#usage",
            "text": "guild tensorflow [OPTIONS] COMMAND [ARGS]... \n   \n     Collection of TensorFlow tools.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/tensorflow-cmd/#options",
            "text": "--help \n         Show command help and exit.",
            "title": "Options"
        },
        {
            "location": "/docs/commands/tensorflow-cmd/#subcommands",
            "text": "inspect \n         Inspect a TensorFlow checkpoint file. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Subcommands"
        },
        {
            "location": "/docs/commands/tensorflow-inspect-cmd/",
            "text": "tensorflow inspect command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild tensorflow inspect [OPTIONS] PATH\n\n  \n\n    \nInspect a TensorFlow checkpoint file.\n\n\nPATH\n is the path to the checkpoint file (usually ending in\n\n.ckpt\n).\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n--tensor-name NAME\n\n        \nName of the tensor to inspect.\n\n      \n\n      \n      \n\n        \n--all-tensors\n\n        \nPrint the values of all the tensors.\n\n      \n\n      \n      \n\n        \n--all-tensor-names\n\n        \nPrint the name of all the tensors\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "tensorflow inspect command"
        },
        {
            "location": "/docs/commands/tensorflow-inspect-cmd/#tensorflow-inspect-command",
            "text": "Usage  Options",
            "title": "tensorflow inspect command"
        },
        {
            "location": "/docs/commands/tensorflow-inspect-cmd/#usage",
            "text": "guild tensorflow inspect [OPTIONS] PATH \n   \n     Inspect a TensorFlow checkpoint file.  PATH  is the path to the checkpoint file (usually ending in .ckpt ).",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/tensorflow-inspect-cmd/#options",
            "text": "--tensor-name NAME \n         Name of the tensor to inspect. \n       \n      \n       \n         --all-tensors \n         Print the values of all the tensors. \n       \n      \n       \n         --all-tensor-names \n         Print the name of all the tensors \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/train-cmd/",
            "text": "train command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild train [OPTIONS] [MODEL] [ARG...]\n\n  \n\n    \nTrain a model.\n\n\nEquivalent to running \nguild run [MODEL:]train [ARG...]\n.\n\n\nBy default \nMODEL\n is the default model defined in the directory.\n\n\nMODEL\n may be a partial name of a matching model provided it\nmatches only one model.\n\n\nYou may omit \nMODEL\n (i.e. for training the default model) while\nproviding one or more \nARG\n values provided the first \nARG\n value\ncontains an equals sign (\n=\n). When specifying a switch (i.e. an\nargument that doesn't accept a value) as the first \nARG\n, you must\nprovide \nMODEL\n.\n\n\nRefer to help for the run command (\nrun\n) for more\ninformation.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-l, --label LABEL\n\n        \nSet a label for the run.\n\n      \n\n      \n      \n\n        \n--run-dir DIR\n\n        \nUse alternative run directory DIR. Cannot be used with --stage.\n\n      \n\n      \n      \n\n        \n--stage DIR\n\n        \nStage an operation in DIR but do not run. Cannot be used with --run-dir.\n\n      \n\n      \n      \n\n        \n-r, --rerun RUN\n\n        \nUse the operation and flags from RUN. Flags may be added or redefined in this operation. Cannot be used with --restart.\n\n      \n\n      \n      \n\n        \n-s, --restart RUN\n\n        \nRestart RUN in-place without creating a new run. Cannot be used with --rerun or --run-dir.\n\n      \n\n      \n      \n\n        \n--disable-plugins LIST\n\n        \nA comma separated list of plugin names to disable. Use 'all' to disable all plugins.\n\n      \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before running operation.\n\n      \n\n      \n      \n\n        \n-n, --no-wait\n\n        \nDon't wait for a remote operation to complete. Ignored if run is local.\n\n      \n\n      \n      \n\n        \n--set-trace\n\n        \nEnter the Python debugger at the operation entry point.\n\n      \n\n      \n      \n\n        \n--print-cmd\n\n        \nShow operation command and exit.\n\n      \n\n      \n      \n\n        \n--print-env\n\n        \nShow operation environment and exit.\n\n      \n\n      \n      \n\n        \n--help-model\n\n        \nShow model help and exit.\n\n      \n\n      \n      \n\n        \n--help-op\n\n        \nShow operation help and exit.\n\n      \n\n      \n      \n\n        \n-w, --workflow\n\n        \nExperimental support for workflow.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "train command"
        },
        {
            "location": "/docs/commands/train-cmd/#train-command",
            "text": "Usage  Options",
            "title": "train command"
        },
        {
            "location": "/docs/commands/train-cmd/#usage",
            "text": "guild train [OPTIONS] [MODEL] [ARG...] \n   \n     Train a model.  Equivalent to running  guild run [MODEL:]train [ARG...] .  By default  MODEL  is the default model defined in the directory.  MODEL  may be a partial name of a matching model provided it\nmatches only one model.  You may omit  MODEL  (i.e. for training the default model) while\nproviding one or more  ARG  values provided the first  ARG  value\ncontains an equals sign ( = ). When specifying a switch (i.e. an\nargument that doesn't accept a value) as the first  ARG , you must\nprovide  MODEL .  Refer to help for the run command ( run ) for more\ninformation.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/train-cmd/#options",
            "text": "-l, --label LABEL \n         Set a label for the run. \n       \n      \n       \n         --run-dir DIR \n         Use alternative run directory DIR. Cannot be used with --stage. \n       \n      \n       \n         --stage DIR \n         Stage an operation in DIR but do not run. Cannot be used with --run-dir. \n       \n      \n       \n         -r, --rerun RUN \n         Use the operation and flags from RUN. Flags may be added or redefined in this operation. Cannot be used with --restart. \n       \n      \n       \n         -s, --restart RUN \n         Restart RUN in-place without creating a new run. Cannot be used with --rerun or --run-dir. \n       \n      \n       \n         --disable-plugins LIST \n         A comma separated list of plugin names to disable. Use 'all' to disable all plugins. \n       \n      \n       \n         -y, --yes \n         Do not prompt before running operation. \n       \n      \n       \n         -n, --no-wait \n         Don't wait for a remote operation to complete. Ignored if run is local. \n       \n      \n       \n         --set-trace \n         Enter the Python debugger at the operation entry point. \n       \n      \n       \n         --print-cmd \n         Show operation command and exit. \n       \n      \n       \n         --print-env \n         Show operation environment and exit. \n       \n      \n       \n         --help-model \n         Show model help and exit. \n       \n      \n       \n         --help-op \n         Show operation help and exit. \n       \n      \n       \n         -w, --workflow \n         Experimental support for workflow. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/uninstall-cmd/",
            "text": "uninstall command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild uninstall [OPTIONS] PACKAGE...\n\n  \n\n    \nUninstall one or more packages.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-y, --yes\n\n        \nDo not prompt before uninstalling.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "uninstall command"
        },
        {
            "location": "/docs/commands/uninstall-cmd/#uninstall-command",
            "text": "Usage  Options",
            "title": "uninstall command"
        },
        {
            "location": "/docs/commands/uninstall-cmd/#usage",
            "text": "guild uninstall [OPTIONS] PACKAGE... \n   \n     Uninstall one or more packages.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/uninstall-cmd/#options",
            "text": "-y, --yes \n         Do not prompt before uninstalling. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/commands/view-cmd/",
            "text": "view command\n\n\n\n\nUsage\n\n\nOptions\n\n\n\n\n\n  \nUsage\n\n  \nguild view [OPTIONS] [RUN...]\n\n  \n\n    \nVisualize runs.\n\n  \n\n  \nOptions\n\n  \n\n    \n\n      \n      \n\n        \n-h, --host HOST\n\n        \nName of host interface to listen on.\n\n      \n\n      \n      \n\n        \n-p, --port PORT\n\n        \nPort to listen on.\n\n      \n\n      \n      \n\n        \n-n, --no-open\n\n        \nDon't open Guild View in a browser.\n\n      \n\n      \n      \n\n        \n--logging\n\n        \nLog requests.\n\n      \n\n      \n      \n\n        \n--files\n\n        \nView run files using file browser rather than start Guild View. Guild View related options (\n--no-open\n, \n--logging\n) are ignored.\n\n      \n\n      \n      \n\n        \n-o, --operation VAL\n\n        \nInclude runs with operations matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-l, --label VAL\n\n        \nInclude runs with labels matching \nVAL\n.\n\n      \n\n      \n      \n\n        \n-u, --unlabeled\n\n        \nInclude only runs without labels.\n\n      \n\n      \n      \n\n        \n-R, --running\n\n        \nInclude only runs that are still running.\n\n      \n\n      \n      \n\n        \n-C, --completed\n\n        \nInclude only completed runs.\n\n      \n\n      \n      \n\n        \n-E, --error\n\n        \nInclude only runs that exited with an error.\n\n      \n\n      \n      \n\n        \n-T, --terminated\n\n        \nInclude only runs terminated by the user.\n\n      \n\n      \n      \n\n        \n--help\n\n        \nShow command help and exit.\n\n      \n\n      \n    \n\n  \n\n  \n  \nGuild AI version 0.4.0",
            "title": "view command"
        },
        {
            "location": "/docs/commands/view-cmd/#view-command",
            "text": "Usage  Options",
            "title": "view command"
        },
        {
            "location": "/docs/commands/view-cmd/#usage",
            "text": "guild view [OPTIONS] [RUN...] \n   \n     Visualize runs.",
            "title": "Usage"
        },
        {
            "location": "/docs/commands/view-cmd/#options",
            "text": "-h, --host HOST \n         Name of host interface to listen on. \n       \n      \n       \n         -p, --port PORT \n         Port to listen on. \n       \n      \n       \n         -n, --no-open \n         Don't open Guild View in a browser. \n       \n      \n       \n         --logging \n         Log requests. \n       \n      \n       \n         --files \n         View run files using file browser rather than start Guild View. Guild View related options ( --no-open ,  --logging ) are ignored. \n       \n      \n       \n         -o, --operation VAL \n         Include runs with operations matching  VAL . \n       \n      \n       \n         -l, --label VAL \n         Include runs with labels matching  VAL . \n       \n      \n       \n         -u, --unlabeled \n         Include only runs without labels. \n       \n      \n       \n         -R, --running \n         Include only runs that are still running. \n       \n      \n       \n         -C, --completed \n         Include only completed runs. \n       \n      \n       \n         -E, --error \n         Include only runs that exited with an error. \n       \n      \n       \n         -T, --terminated \n         Include only runs terminated by the user. \n       \n      \n       \n         --help \n         Show command help and exit. \n       \n      \n     \n   \n  \n   Guild AI version 0.4.0",
            "title": "Options"
        },
        {
            "location": "/docs/reference/guild-file/",
            "text": "Guild file reference\n\n\n\n\nTop level objects\n\n\nExamples\n\n\n\n\n\n\nInheritance\n\n\nExamples\n\n\n\n\n\n\nPackages\n\n\nAttributes\n\n\nExamples\n\n\n\n\n\n\nModels\n\n\nAttributes\n\n\nExamples\n\n\n\n\n\n\nOperations\n\n\nAttributes\n\n\n\n\n\n\nFlags\n\n\nAttributes\n\n\n\n\n\n\nFlag choices\n\n\nAttributes\n\n\nExamples\n\n\n\n\n\n\nResources\n\n\nAttributes\n\n\n\n\n\n\nResource sources\n\n\nSource type\n\n\nAttributes\n\n\nExamples\n\n\n\n\n\n\n\n\nGuild files are files named \nguild.yml\n that contain information that\nGuild needs to perform an operation.\n\n\nGuild files are authored in \nYAML\n.\n\n\nTop level objects\n\n\nGuild files may contain a single top-level object or a list of\ntop-level objects.\n\n\nA top-level object may one of:\n\n\n\n\npackage\n\n\nmodel\n\n\nconfig\n\n\n\n\nTop-level objects are identified by the presence of an identifying\nattribute: \npackage\n, \nmodel\n, or \nconfig\n. A Guild file may contain\ntop-level objects that do not have one of these identifying\nattributes, but these objects are ignored by Guild.\n\n\nThe value of identifying attributes are used as object identifiers.\n\n\nA top-level object must only contain one and only one identifying\nattribute.\n\n\nExamples\n\n\nTop-level package:\n\n\npackage: my-package\ndescription: My package\n\n\n\n\nTop-level model:\n\n\nmodel: my-model\ndescription: My model\n\n\n\n\nTop-level config:\n\n\nconfig: my-config\ndescription: My config\n\n\n\n\nList of top-level objects:\n\n\n- package: my-package\n  description: My package\n\n- model: model-a\n  description: Model A\n\n- model: model-b\n  description: Model B\n\n- config: shared\n  description: Share config\n\n\n\n\nIllegal top-level object (multiple identifiers):\n\n\nmodel: my-model\npackage: my-package\n\n\n\n\nInheritance\n\n\nTop-level objects may extend other top-level objects by specifying an\n\nextends\n attribute. The value of \nextends\n may be a string, which\nidentifies a single object to extend, or a list of strings, which\nidentifies multiple objects to extend.\n\n\nWhen an object extends another object, it inherits its\nattributes. Extending objects may redefine attributes of the objects\nthey extend.\n\n\nWhen more than one object is extended, attributes of objects later in\nthe list take precedence of those higher in the list.\n\n\nConfig objects are used exclusively for inheritance and are not\notherwise used by Guild.\n\n\nExamples\n\n\nIn the following example, two models extend a base config. The first\nmodel redefines its description while the second does not.\n\n\n- config: base\n  description: A base config\n\n- model: model-a\n  extends: base\n  description: My model\n\n- model: model-b\n  extends: base\n\n\n\n\nPackages\n\n\nA Guild file must contain at most one \npackage\n object.\n\n\nPackages contain information used by Guild to generate Guild packages.\n\n\nAttributes\n\n\n\n\npackage\n\n\nPackage name (required string)\n\n\ndescription\n\n\nProject description (string)\n  \n\n  This may be a multi-line description.\n\n\nversion\n\n\nProject version (required string)\n\n\nurl\n\n\nURL to package website (URL)\n\n\nmaintainer\n\n\nName of individual or organization package maintainer (string)\n\n\nmaintainer-email\n\n\nEmail of package maintainer (email address)\n\n\nlicense\n\n\nName of package license (string)\n\n\ntags\n\n\nList of packages tags (list of strings)\n\n\npython-tag\n\n\nValue used as the Python tag when generating the package (string)\n\n\ndata_files\n\n\nList of additional files to include in the package (list of strings)\n\n\nresources\n\n\nList of package resources (list of \nresources\n)\n\n\npython-requires\n\n\nVersion of Python required by the package (string)\n\n\nrequires\n\n\nList of other packages required by the package (list of strings)\n\n\n\n\nExamples\n\n\nPackage definition for \nslim.resnet\n:\n\n\npackage: slim.resnet\nversion: 0.3.0\ndescription:\n  TF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2)\nurl: https://github.com/guildai/index/tree/master/slim/resnet\nmaintainer: Guild AI\nmaintainer-email: packages@guild.ai\nrequires:\n  - slim>=0.3.0.dev11\n  - slim.datasets>=0.3.0.dev3\nlicense: Apache 2.0\ntags: [resnet, images, model]\n\n\n\n\nModels\n\n\nModels may may be defined as top-level Guild file objects using the\n\nmodel\n identifying attribute.\n\n\nHere\u2019s a Guild file that defines two models:\n\n\n- model: model-a\n- model: model-b\n\n\n\n\nModels define operations, which can be run using the \nrun\n\ncommand.\n\n\nModels may also define resources that operations require.\n\n\nAttributes\n\n\n\n\nmodel\n\n\nModel name (required string)\n\n\ndescription\n\n\nModel description (string)\n  \n\n  This may be a multi-line description.\n\n\noperations\n\n\nModel operations (list of \noperations\n)\n\n\nresources\n\n\nModel resources (list of \nresources\n)\n\n\nreferences\n\n\nModel references (list of URLs)\n  \n\n  References are displayed in help text.\n\n\nextra\n\n\nAdditional information used by Guild and Guild plugins\n\n\n\n\nExamples\n\n\nComplete example of \nmnist-layers\n (from\n\ntensorflow.mnist\n):\n\n\n- model: mnist-layers\n  description: CNN estimator for MNIST using tf.layers\n  operations:\n    train:\n      description: Train the CNN\n      main: mnist/mnist --data_dir mnist-idx-data --model_dir . --export_dir .\n      requires:\n        - mnist-lib\n        - mnist/dataset\n      flags:\n        batch-size:\n          description: Number of images to process in a batch\n          default: 100\n        epochs:\n          description: Number of epochs to train\n          default: 40\n          arg-name: train_epochs\n  resources:\n    mnist-lib:\n      description: Python library for tensorflow.mnist\n      private: yes\n      sources:\n        - url: https://github.com/tensorflow/models/archive/v.1.6.0.zip\n          sha256: ed8fd7066bb014feccaed2cd2a46e516468ef24c40be8ef21a96a09849db7ff5\n          select: models-v.1.6.0/official/mnist\n  references:\n    - https://github.com/tensorflow/models/tree/v.1.6.0/official/mnist\n\n\n\n\nOperations\n\n\nOperations define commands that are run for a model. Operations are\ndefined as named objects under the \noperations\n model attribute.\n\n\nHere\u2019s model with two operations, \ntrain\n and \ntest\n:\n\n\nmodel: my-model\noperations:\n  train:\n    main: train --epochs 1\n  test:\n    main: test --data .\n\n\n\n\nAttributes\n\n\n\n\ndescription\n\n\nOperation description (string)\n  \n\n  This may be a multi-line description.\n\n\nhandle-keyboard-interrupt\n\n\nHandle keyboard interrupts from the user (boolean)\n  \n\n  By default, an operation must explicitly handle keyboard interrupts,\n  which are generated when the user types \nCtrl-C\n, by catching\n  Python\u2019s \nKeyboardInterrupt\n or the process will terminate with an\n  error and a Python traceback. Set \nhandle-keyboard-interrupt\n to\n  \nyes\n to indicate that Guild should handle \nKeyboardInterrupt\n\n  and exit without printing an error message.\n  \n\n  If the operation is run with \n--debug\n, Guild will print the full\n  traceback as a log message.\n  \n\n  Note that an operation terminated with \nCtrl-C\n will still have a\n  status of \nterminated\n even if the interrupt is handled by\n  Guild. To indicate that the operation should be considered\n  \ncompleted\n, set the operation\u2019s \nstoppable\n attribute to \nyes\n.\n\n\nmain\n\n\nMain command module (required string unless \nplugin-op\n is used)\n  \n\n  Operation commands must be in the form \n[MODULE] [ARG...]\n. \nMODULE\n\n  may reference a Python module defined in the model Guild file\n  directory or any Python available on the system. \nARG\n values are\n  passed through as arguments to the Python module.\n  \n\n  \nMODULE\n must not end in \n.py\n.\n  \n\n  \nARG\n values may contain references to \n#flags\n in the\n  format \n${FLAG_NAME}\n. Such references are resolved to the current\n  flag value when the command is executed.\n\n\nflags\n\n\nOperation flags (list of \nflags\n)\n  \n\n  Flags define the arguments that are passed to \nmain\n when the\n  command is executed. For more information, see \nFlags\n.\n\n\nplugin-op\n\n\nThe name of a plugin operation to used instead of \nmain\n (string)\n  \n\n  \nmain\n and \nplugin-op\n cannot both be used.\n\n\npre-process\n\n\nPre-processing shell command\n  \n\n  The command is executed as a shell script after required resources\n  are resolved and before the operation itself is started.\n  \n\n  Commands are executed in the run directory and have access to the\n  same set of environment variables as the operation itself. See\n  \nOperations\n for the\n  list of supported environment variables.\n\n\nrequired\n\n\nOne or more required resources (string or list of strings)\n  \n\n  Values must be in the form \n[PACKAGE_OR_MODEL/]RESOURCE\n.\n\n\nremote\n\n\nFlag indicating whether or not the operation is remote (boolean)\n\n\nstoppable\n\n\nFlag indicating that a terminated run should be considered completed\n  (boolean)\n  \n\n  By default, a terminated run (i.e. a run stopped by typing \nCtrl-C\n\n  or stopped with a \nSIGTERM\n signal such as that issued by the\n  \nstop\n command) has a status of \nterminated\n. If \nstoppable\n\n  is true however, the run status will be \ncompleted\n. Set this\n  value to \nyes\n when the operation is designed to be terminated\n  explicitly by the user.\n\n\n\n\nFlags\n\n\nFlags are defined for \noperations\n under the \nflags\n\nattribute as named objects.\n\n\nAttributes\n\n\n\n\ndescription\n\n\nFlag description (string)\n  \n\n  This may be a multi-line description.\n\n\ndefault\n\n\nDefault value if not specified by the user (string or number)\n\n\nrequired\n\n\nFlag indicating whether or not the flag is required (boolean)\n\n\narg-name\n\n\nName of the command argument used for flag values (string)\n  \n\n  Defaults to the flag name.\n\n\narg-skip\n\n\nBoolean indicating whether not to include the flag as a command\n  argument (boolean)\n\n\nchoices\n\n\nAllowed choices for the flag (list of \nchoices\n)\n\n\n\n\nFlag choices\n\n\nFlag choices limit the available values for a flag. They can also be\nused to apply multiple argument to a command when specified.\n\n\nAttributes\n\n\n\n\nvalue\n\n\nFlag value when choice is specified (string or number)\n\n\ndescription\n\n\nFlag choice description (string)\n\n\nargs\n\n\nMap of argument names to values (object)\n  \nThis attribute is used to define additional arguments that are\n  applied when the choice is selected. Arguments are applied in the\n  form \n--NAME VALUE\n where \nNAME\n and \nVALUE\n correspond to the\n  respective object name value pairs. Use \narg-skip\n to omit the flag\n  argument itself.\n\n\n\n\nExamples\n\n\nOperation that can train one of two model versions (default is \n1\n):\n\n\nmodel: my-model\noperations:\n  train:\n    main: train\n    flags:\n      version:\n        default: 1\n        choices: [1, 2]\n\n\n\n\nSnippet from the \nshared\nconfiguration\n\nin the \nslim\n package. Note that when \nimagenet\n is specified, the\narguments \ninput-mean\n and \ninput-std\n are included in the command\nargument.\n\n\n- config: slim-image-classifier\n  operations:\n    predict:\n      main: label_image --graph graph.pb --labels data/labels.txt\n      flags:\n        ...\n        dataset:\n          description: Dataset name to use for labels and image transformation\n          required: yes\n          arg-skip: yes\n          choices:\n            - cifar10\n            - mnist\n            - flowers\n            - value: imagenet\n              args:\n                input-mean: 0.0\n                input-std: 255\n            - custom\n\n\n\n\nResources\n\n\nResources may be included in packages and models under the \nresources\n\nobject attribute. Resources are identified by their object key.\n\n\nResources may be required by operations. Required resources are known\nas \noperation dependencies\n.\n\n\nA resource must contain at least one source. Sources may be files,\nURLs, operation output, or Python modules.\n\n\nAll required resource sources are resolved before an operation is run\nto ensure the operation has what it needs to run. Guild creates\nsymbolic links to resource sources in the run directory. For more\ninformation, see \nResource sources\n below.\n\n\nHere\u2019s a model with two resources, each with a single source file.\n\n\nmodel: my-model\nresources:\n  resource-a:\n    sources: [file-a]\n  resource-b:\n    sources: [file-b]\n\n\n\n\nAttributes\n\n\n\n\ndescription\n\n\nResource description (string)\n\n\npath\n\n\nRelative path within the run directory where resolved sources are\n  saved (string)\n\n\nsources\n\n\nList of resource sources (list of \nresource sources\n)\n\n\nprivate\n\n\nFlag indicating whether or not the resource is private (boolean)\n  \n\n  Private resources don\u2019t appear in resource lists.\n\n\nreferences\n\n\nList of reference URLs associated with the resource (list of URLs)\n  \n\n  References are displayed in help text.\n\n\n\n\nResource sources\n\n\nA resource source defines what is resolved and therefore available to\nan operation that requires the resource.\n\n\nSource files are provided to runs within the run directory as symbolic\nlinks.\n\n\nSource type\n\n\nSources have a \ntype\n, which is identified by the use of one and only\none of the following type attribute:\n\n\n\n\nfile\n\n\nSource is a file relative to the defining Guild file\n\n\nurl\n\n\nSource is a URL (\nhttp\n and \nhttps\n protocols are supported)\n\n\noperation\n\n\nSource is generated from an operation\n  \n\n  Value is an operation spec consisting of\n  \n[PACKAGE/[MODEL:]]OPERATION\n.  Multiple operation specs may be\n  specified separated with a comma. By default Guild will use the\n  latest completed or terminated run matching any of the operation\n  specs. Users may alternatively specify a run ID for the resource\n  when running the requiring operation.\n\n\nmodule\n\n\nSource is a Python module\n\n\n\n\nIf source is a string, the value is treated as a \nfile\n source type.\n\n\nAttributes\n\n\n\n\nsha256\n\n\nSHA 256 hash used to verify the source (string)\n\n\nunpack\n\n\nA flag indicating whether or not the source should be unpacked (boolean)\n  \n\n  By default Guild attempts to unpack files with common archive\n  extensions (\n.zip\n, \n.tar\n, \n.tar.gz\n, \n.tgz\n).\n\n\nselect\n\n\nOne or more regular expressions used to select sources from a\n  directory or unpacked archive (string or list of strings)\n\n\npost-process\n\n\nShell command executed after source has been resolved (string)\n  \n\n  This applies to \nurl\n sources only.\n  \n\n  Commands are executed in the context of the resource cache directory\n  containing the downloaded and unpacked URL source. Commands may use\n  the \n$RESDEF_DIR\n environment variable to reference files relative\n  to the directory containing the resource declaration (i.e. the\n  directory containing the project or package Guild file).\n\n\nhelp\n\n\nHelp text displayed when a source cannot be resolved (string)\n  \n\n  This can be used to help a user install a missing Python module, run\n  a required operation, etc.\n\n\n\n\nExamples\n\n\nFile \ndata-train.csv\n and \ndata-test.csv\n provided as a part of \ndata\n\nresource:\n\n\npackage: my-package\nresources:\n  data:\n    description: Data files\n    sources:\n    - data-train.csv\n    - data-test.csv\n\n\n\n\nMNIST IDX sources as a \ndataset\n resource, stored under\n\nmnist-idx-data\n in the run directory:\n\n\npackage: mnist\nresources:\n  dataset:\n    description: \"Yann Lecun's MNIST dataset in compressed IDX format\"\n    path: mnist-idx-data\n    sources:\n      - url: http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n        sha256: 440fcabf73cc546fa21475e81ea370265605f56be210a4024d2ca8f203523609\n      - url: http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n        sha256: 3552534a0a558bbed6aed32b30c495cca23d567ec52cac8be1a0730e8010255c\n      - url: http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n        sha256: 8d422c7b0a1c1c79245a5bcf07fe86e33eeafee792b84584aec276f5a2dbc4e6\n      - url: http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n        sha256: f7ae60f92e00ec6debd23a6088c31dbd2371eca3ffa0defaefb259924204aec6\n\n\n\n\nOperation \nfinetune\n depends on the output of \ntrain\n, which is stored\nunder \nmodel\n in the run directory:\n\n\nmodel: my-model\noperations:\n  train:\n    main: train\n  finetune:\n    main: finetune\n    requires: trained-model\nresources:\n  trained-model:\n    path: model\n    sources:\n      - operation: train\n        select: checkpoint|model\\.ckpt.*",
            "title": "Guild file reference"
        },
        {
            "location": "/docs/reference/guild-file/#guild-file-reference",
            "text": "Top level objects  Examples    Inheritance  Examples    Packages  Attributes  Examples    Models  Attributes  Examples    Operations  Attributes    Flags  Attributes    Flag choices  Attributes  Examples    Resources  Attributes    Resource sources  Source type  Attributes  Examples     Guild files are files named  guild.yml  that contain information that\nGuild needs to perform an operation.  Guild files are authored in  YAML .",
            "title": "Guild file reference"
        },
        {
            "location": "/docs/reference/guild-file/#top-level-objects",
            "text": "Guild files may contain a single top-level object or a list of\ntop-level objects.  A top-level object may one of:   package  model  config   Top-level objects are identified by the presence of an identifying\nattribute:  package ,  model , or  config . A Guild file may contain\ntop-level objects that do not have one of these identifying\nattributes, but these objects are ignored by Guild.  The value of identifying attributes are used as object identifiers.  A top-level object must only contain one and only one identifying\nattribute.",
            "title": "Top level objects"
        },
        {
            "location": "/docs/reference/guild-file/#examples",
            "text": "Top-level package:  package: my-package\ndescription: My package  Top-level model:  model: my-model\ndescription: My model  Top-level config:  config: my-config\ndescription: My config  List of top-level objects:  - package: my-package\n  description: My package\n\n- model: model-a\n  description: Model A\n\n- model: model-b\n  description: Model B\n\n- config: shared\n  description: Share config  Illegal top-level object (multiple identifiers):  model: my-model\npackage: my-package",
            "title": "Examples"
        },
        {
            "location": "/docs/reference/guild-file/#inheritance",
            "text": "Top-level objects may extend other top-level objects by specifying an extends  attribute. The value of  extends  may be a string, which\nidentifies a single object to extend, or a list of strings, which\nidentifies multiple objects to extend.  When an object extends another object, it inherits its\nattributes. Extending objects may redefine attributes of the objects\nthey extend.  When more than one object is extended, attributes of objects later in\nthe list take precedence of those higher in the list.  Config objects are used exclusively for inheritance and are not\notherwise used by Guild.",
            "title": "Inheritance"
        },
        {
            "location": "/docs/reference/guild-file/#examples_1",
            "text": "In the following example, two models extend a base config. The first\nmodel redefines its description while the second does not.  - config: base\n  description: A base config\n\n- model: model-a\n  extends: base\n  description: My model\n\n- model: model-b\n  extends: base",
            "title": "Examples"
        },
        {
            "location": "/docs/reference/guild-file/#packages",
            "text": "A Guild file must contain at most one  package  object.  Packages contain information used by Guild to generate Guild packages.",
            "title": "Packages"
        },
        {
            "location": "/docs/reference/guild-file/#attributes",
            "text": "package  Package name (required string)  description  Project description (string)\n   \n  This may be a multi-line description.  version  Project version (required string)  url  URL to package website (URL)  maintainer  Name of individual or organization package maintainer (string)  maintainer-email  Email of package maintainer (email address)  license  Name of package license (string)  tags  List of packages tags (list of strings)  python-tag  Value used as the Python tag when generating the package (string)  data_files  List of additional files to include in the package (list of strings)  resources  List of package resources (list of  resources )  python-requires  Version of Python required by the package (string)  requires  List of other packages required by the package (list of strings)",
            "title": "Attributes"
        },
        {
            "location": "/docs/reference/guild-file/#examples_2",
            "text": "Package definition for  slim.resnet :  package: slim.resnet\nversion: 0.3.0\ndescription:\n  TF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2)\nurl: https://github.com/guildai/index/tree/master/slim/resnet\nmaintainer: Guild AI\nmaintainer-email: packages@guild.ai\nrequires:\n  - slim>=0.3.0.dev11\n  - slim.datasets>=0.3.0.dev3\nlicense: Apache 2.0\ntags: [resnet, images, model]",
            "title": "Examples"
        },
        {
            "location": "/docs/reference/guild-file/#models",
            "text": "Models may may be defined as top-level Guild file objects using the model  identifying attribute.  Here\u2019s a Guild file that defines two models:  - model: model-a\n- model: model-b  Models define operations, which can be run using the  run \ncommand.  Models may also define resources that operations require.",
            "title": "Models"
        },
        {
            "location": "/docs/reference/guild-file/#attributes_1",
            "text": "model  Model name (required string)  description  Model description (string)\n   \n  This may be a multi-line description.  operations  Model operations (list of  operations )  resources  Model resources (list of  resources )  references  Model references (list of URLs)\n   \n  References are displayed in help text.  extra  Additional information used by Guild and Guild plugins",
            "title": "Attributes"
        },
        {
            "location": "/docs/reference/guild-file/#examples_3",
            "text": "Complete example of  mnist-layers  (from tensorflow.mnist ):  - model: mnist-layers\n  description: CNN estimator for MNIST using tf.layers\n  operations:\n    train:\n      description: Train the CNN\n      main: mnist/mnist --data_dir mnist-idx-data --model_dir . --export_dir .\n      requires:\n        - mnist-lib\n        - mnist/dataset\n      flags:\n        batch-size:\n          description: Number of images to process in a batch\n          default: 100\n        epochs:\n          description: Number of epochs to train\n          default: 40\n          arg-name: train_epochs\n  resources:\n    mnist-lib:\n      description: Python library for tensorflow.mnist\n      private: yes\n      sources:\n        - url: https://github.com/tensorflow/models/archive/v.1.6.0.zip\n          sha256: ed8fd7066bb014feccaed2cd2a46e516468ef24c40be8ef21a96a09849db7ff5\n          select: models-v.1.6.0/official/mnist\n  references:\n    - https://github.com/tensorflow/models/tree/v.1.6.0/official/mnist",
            "title": "Examples"
        },
        {
            "location": "/docs/reference/guild-file/#operations",
            "text": "Operations define commands that are run for a model. Operations are\ndefined as named objects under the  operations  model attribute.  Here\u2019s model with two operations,  train  and  test :  model: my-model\noperations:\n  train:\n    main: train --epochs 1\n  test:\n    main: test --data .",
            "title": "Operations"
        },
        {
            "location": "/docs/reference/guild-file/#attributes_2",
            "text": "description  Operation description (string)\n   \n  This may be a multi-line description.  handle-keyboard-interrupt  Handle keyboard interrupts from the user (boolean)\n   \n  By default, an operation must explicitly handle keyboard interrupts,\n  which are generated when the user types  Ctrl-C , by catching\n  Python\u2019s  KeyboardInterrupt  or the process will terminate with an\n  error and a Python traceback. Set  handle-keyboard-interrupt  to\n   yes  to indicate that Guild should handle  KeyboardInterrupt \n  and exit without printing an error message.\n   \n  If the operation is run with  --debug , Guild will print the full\n  traceback as a log message.\n   \n  Note that an operation terminated with  Ctrl-C  will still have a\n  status of  terminated  even if the interrupt is handled by\n  Guild. To indicate that the operation should be considered\n   completed , set the operation\u2019s  stoppable  attribute to  yes .  main  Main command module (required string unless  plugin-op  is used)\n   \n  Operation commands must be in the form  [MODULE] [ARG...] .  MODULE \n  may reference a Python module defined in the model Guild file\n  directory or any Python available on the system.  ARG  values are\n  passed through as arguments to the Python module.\n   \n   MODULE  must not end in  .py .\n   \n   ARG  values may contain references to  #flags  in the\n  format  ${FLAG_NAME} . Such references are resolved to the current\n  flag value when the command is executed.  flags  Operation flags (list of  flags )\n   \n  Flags define the arguments that are passed to  main  when the\n  command is executed. For more information, see  Flags .  plugin-op  The name of a plugin operation to used instead of  main  (string)\n   \n   main  and  plugin-op  cannot both be used.  pre-process  Pre-processing shell command\n   \n  The command is executed as a shell script after required resources\n  are resolved and before the operation itself is started.\n   \n  Commands are executed in the run directory and have access to the\n  same set of environment variables as the operation itself. See\n   Operations  for the\n  list of supported environment variables.  required  One or more required resources (string or list of strings)\n   \n  Values must be in the form  [PACKAGE_OR_MODEL/]RESOURCE .  remote  Flag indicating whether or not the operation is remote (boolean)  stoppable  Flag indicating that a terminated run should be considered completed\n  (boolean)\n   \n  By default, a terminated run (i.e. a run stopped by typing  Ctrl-C \n  or stopped with a  SIGTERM  signal such as that issued by the\n   stop  command) has a status of  terminated . If  stoppable \n  is true however, the run status will be  completed . Set this\n  value to  yes  when the operation is designed to be terminated\n  explicitly by the user.",
            "title": "Attributes"
        },
        {
            "location": "/docs/reference/guild-file/#flags",
            "text": "Flags are defined for  operations  under the  flags \nattribute as named objects.",
            "title": "Flags"
        },
        {
            "location": "/docs/reference/guild-file/#attributes_3",
            "text": "description  Flag description (string)\n   \n  This may be a multi-line description.  default  Default value if not specified by the user (string or number)  required  Flag indicating whether or not the flag is required (boolean)  arg-name  Name of the command argument used for flag values (string)\n   \n  Defaults to the flag name.  arg-skip  Boolean indicating whether not to include the flag as a command\n  argument (boolean)  choices  Allowed choices for the flag (list of  choices )",
            "title": "Attributes"
        },
        {
            "location": "/docs/reference/guild-file/#flag-choices",
            "text": "Flag choices limit the available values for a flag. They can also be\nused to apply multiple argument to a command when specified.",
            "title": "Flag choices"
        },
        {
            "location": "/docs/reference/guild-file/#attributes_4",
            "text": "value  Flag value when choice is specified (string or number)  description  Flag choice description (string)  args  Map of argument names to values (object)\n   This attribute is used to define additional arguments that are\n  applied when the choice is selected. Arguments are applied in the\n  form  --NAME VALUE  where  NAME  and  VALUE  correspond to the\n  respective object name value pairs. Use  arg-skip  to omit the flag\n  argument itself.",
            "title": "Attributes"
        },
        {
            "location": "/docs/reference/guild-file/#examples_4",
            "text": "Operation that can train one of two model versions (default is  1 ):  model: my-model\noperations:\n  train:\n    main: train\n    flags:\n      version:\n        default: 1\n        choices: [1, 2]  Snippet from the  shared\nconfiguration \nin the  slim  package. Note that when  imagenet  is specified, the\narguments  input-mean  and  input-std  are included in the command\nargument.  - config: slim-image-classifier\n  operations:\n    predict:\n      main: label_image --graph graph.pb --labels data/labels.txt\n      flags:\n        ...\n        dataset:\n          description: Dataset name to use for labels and image transformation\n          required: yes\n          arg-skip: yes\n          choices:\n            - cifar10\n            - mnist\n            - flowers\n            - value: imagenet\n              args:\n                input-mean: 0.0\n                input-std: 255\n            - custom",
            "title": "Examples"
        },
        {
            "location": "/docs/reference/guild-file/#resources",
            "text": "Resources may be included in packages and models under the  resources \nobject attribute. Resources are identified by their object key.  Resources may be required by operations. Required resources are known\nas  operation dependencies .  A resource must contain at least one source. Sources may be files,\nURLs, operation output, or Python modules.  All required resource sources are resolved before an operation is run\nto ensure the operation has what it needs to run. Guild creates\nsymbolic links to resource sources in the run directory. For more\ninformation, see  Resource sources  below.  Here\u2019s a model with two resources, each with a single source file.  model: my-model\nresources:\n  resource-a:\n    sources: [file-a]\n  resource-b:\n    sources: [file-b]",
            "title": "Resources"
        },
        {
            "location": "/docs/reference/guild-file/#attributes_5",
            "text": "description  Resource description (string)  path  Relative path within the run directory where resolved sources are\n  saved (string)  sources  List of resource sources (list of  resource sources )  private  Flag indicating whether or not the resource is private (boolean)\n   \n  Private resources don\u2019t appear in resource lists.  references  List of reference URLs associated with the resource (list of URLs)\n   \n  References are displayed in help text.",
            "title": "Attributes"
        },
        {
            "location": "/docs/reference/guild-file/#resource-sources",
            "text": "A resource source defines what is resolved and therefore available to\nan operation that requires the resource.  Source files are provided to runs within the run directory as symbolic\nlinks.",
            "title": "Resource sources"
        },
        {
            "location": "/docs/reference/guild-file/#source-type",
            "text": "Sources have a  type , which is identified by the use of one and only\none of the following type attribute:   file  Source is a file relative to the defining Guild file  url  Source is a URL ( http  and  https  protocols are supported)  operation  Source is generated from an operation\n   \n  Value is an operation spec consisting of\n   [PACKAGE/[MODEL:]]OPERATION .  Multiple operation specs may be\n  specified separated with a comma. By default Guild will use the\n  latest completed or terminated run matching any of the operation\n  specs. Users may alternatively specify a run ID for the resource\n  when running the requiring operation.  module  Source is a Python module   If source is a string, the value is treated as a  file  source type.",
            "title": "Source type"
        },
        {
            "location": "/docs/reference/guild-file/#attributes_6",
            "text": "sha256  SHA 256 hash used to verify the source (string)  unpack  A flag indicating whether or not the source should be unpacked (boolean)\n   \n  By default Guild attempts to unpack files with common archive\n  extensions ( .zip ,  .tar ,  .tar.gz ,  .tgz ).  select  One or more regular expressions used to select sources from a\n  directory or unpacked archive (string or list of strings)  post-process  Shell command executed after source has been resolved (string)\n   \n  This applies to  url  sources only.\n   \n  Commands are executed in the context of the resource cache directory\n  containing the downloaded and unpacked URL source. Commands may use\n  the  $RESDEF_DIR  environment variable to reference files relative\n  to the directory containing the resource declaration (i.e. the\n  directory containing the project or package Guild file).  help  Help text displayed when a source cannot be resolved (string)\n   \n  This can be used to help a user install a missing Python module, run\n  a required operation, etc.",
            "title": "Attributes"
        },
        {
            "location": "/docs/reference/guild-file/#examples_5",
            "text": "File  data-train.csv  and  data-test.csv  provided as a part of  data \nresource:  package: my-package\nresources:\n  data:\n    description: Data files\n    sources:\n    - data-train.csv\n    - data-test.csv  MNIST IDX sources as a  dataset  resource, stored under mnist-idx-data  in the run directory:  package: mnist\nresources:\n  dataset:\n    description: \"Yann Lecun's MNIST dataset in compressed IDX format\"\n    path: mnist-idx-data\n    sources:\n      - url: http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n        sha256: 440fcabf73cc546fa21475e81ea370265605f56be210a4024d2ca8f203523609\n      - url: http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n        sha256: 3552534a0a558bbed6aed32b30c495cca23d567ec52cac8be1a0730e8010255c\n      - url: http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n        sha256: 8d422c7b0a1c1c79245a5bcf07fe86e33eeafee792b84584aec276f5a2dbc4e6\n      - url: http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n        sha256: f7ae60f92e00ec6debd23a6088c31dbd2371eca3ffa0defaefb259924204aec6  Operation  finetune  depends on the output of  train , which is stored\nunder  model  in the run directory:  model: my-model\noperations:\n  train:\n    main: train\n  finetune:\n    main: finetune\n    requires: trained-model\nresources:\n  trained-model:\n    path: model\n    sources:\n      - operation: train\n        select: checkpoint|model\\.ckpt.*",
            "title": "Examples"
        },
        {
            "location": "/docs/reference/guild-home/",
            "text": "Guild home reference\n\n\n\n\nLayout\n\n\nRuns\n\n\nGuild metadata\n\n\n\n\n\n\nResource cache\n\n\nResource cache and virtual environments\n\n\n\n\n\n\nRuns index\n\n\n\n\nGuild home is a directory that Guild AI uses to store various files.\n\n\nEvery Guild command is associated with a Guild home. If Guild home\ndoes not exist, Guild will automatically create it.\n\n\nBy default, Guild home is one of two locations depending on whether or\nnot a command is run in a virtual environment.\n\n\nIf the command is not run in a virtual environment, Guild home is\n\n~/.guild\n by default. If the command is run in a virtual environment,\nGuild home is \n$VIRTUAL_ENV/.guild\n where \n$VIRTUAL_ENV\n is the\nvirtual environment directory.\n\n\nGuild home may be set explicitly by either defining the environment\nvariable \nGUILD_HOME\n or by using the \n-H\n option when running a\nGuild command.\n\n\nLayout\n\n\nGuild home is structured as follows:\n\n\n\n\n\n\nGuild home\nE.g. \n~/.guild\n or \n$VIRTUAL_ENV/.guild\n\n\ncache \nCaches used to optimize various Guild functions\n\n \n\n \nresources \nCached resources\n\n \nruns \nIndexed run data\n\n \n\n\n\n\nruns \nActive runs\n\n\ntrash \nDeleted runs\n\n\n\n\n\n\n\nRuns\n\n\nActive runs (i.e. non-deleted runs) are stored in\n\n$GUILD_HOME/runs\n. Each run is stored in a subdirectory with the same\nname as the run ID.\n\n\nRun directories contain all files associated with a run, including:\n\n\n\n\nResource source links\n\n\nOutput generated during the run\n\n\nGuild metadata\n\n\n\n\nGuild metadata is stored in a subdirectory named \n.guild\n in each run\ndirectory.\n\n\nGuild metadata\n\n\n\n\n\n\nRun directory\n\n.guild \nGuild metadata\n\n \n\n \noutput \nOutput generated by the run OS process\n\n \noutput.index \nTimestamp and stream information associated with run output\n\n \nattrs \nRun attributes\n\n   \n\n     \ncmd \nRun OS process command\n\n     \ndeps \nRun dependencies (required resources)\n\n     \nenv \nRun OS process environment\n\n     \nexit_status \nRun OS process exit status\n\n     \n_extra_NAME \nExtra config for NAME, defined for the model operation\n\n     \nflags \nFlag values used for the run\n\n     \nopref \nReference to the operation associated with the run\n\n     \nstarted \nStarted timestamp\n\n     \nstopped \nStopped timestamp\n\n   \n\n \n\n \n\n\n\n\n\n\n\n\n\nResource cache\n\n\nWhen Guild resolves resources, it uses \nGUILD_HOME/cache/resources\n\nfor two purposes:\n\n\n\n\nLocation for downloading URL resource sources\n\n\nLocation for unpacking archives\n\n\n\n\nResources are stored in directories that are named using hashes of the\ncorresponding resource source URI, which is an internal representation\nof a resource source location. If a resource exists in the cache, it\nis used rather than downloading or unpacking the resource again.\n\n\nGuild does not remove items from the resource cache.\n\n\nYou may clear the resource cache by deleting\n\nGUILD_HOME/cache/resources\n. Guild commands will continue to function\nnormally and the cache will be repopulated as needed.\n\n\nResource cache and virtual environments\n\n\nBy default, Guild reuses the resource cache in\n\n~/.guild/cache/resources\n (the user-level resource cache) for all\nvirtual environments. This means that cached resources available\nwithin virtual environments without having to re-download\nfiles. However, it also means that virtual environments can modify the\nuser-level resource cache.\n\n\nIf you want to isolate a virtual environment from the user-level\ncache, run the following command after you first activate the\nenvironment:\n\n\nguild init --local-resource-cache\n\n\n\n\nRuns index\n\n\nGuild maintains an index of run data in \nGUILD_HOME/cache/runs\n. This\nindex lets Guild lookup values such as training accuracy and loss\nquickly when performing comparisons.\n\n\nGuild updates the runs index lazily as it discovers new runs or new\ndata associated with a run.\n\n\nYou may clear the runs cache by deleting\n\nGUILD_HOME/cache/runs\n. Guild commands will continue to function\nnormally and runs will be indexed again as needed.",
            "title": "Guild home reference"
        },
        {
            "location": "/docs/reference/guild-home/#guild-home-reference",
            "text": "Layout  Runs  Guild metadata    Resource cache  Resource cache and virtual environments    Runs index   Guild home is a directory that Guild AI uses to store various files.  Every Guild command is associated with a Guild home. If Guild home\ndoes not exist, Guild will automatically create it.  By default, Guild home is one of two locations depending on whether or\nnot a command is run in a virtual environment.  If the command is not run in a virtual environment, Guild home is ~/.guild  by default. If the command is run in a virtual environment,\nGuild home is  $VIRTUAL_ENV/.guild  where  $VIRTUAL_ENV  is the\nvirtual environment directory.  Guild home may be set explicitly by either defining the environment\nvariable  GUILD_HOME  or by using the  -H  option when running a\nGuild command.",
            "title": "Guild home reference"
        },
        {
            "location": "/docs/reference/guild-home/#layout",
            "text": "Guild home is structured as follows:    Guild home E.g.  ~/.guild  or  $VIRTUAL_ENV/.guild  cache  Caches used to optimize various Guild functions \n  \n  resources  Cached resources \n  runs  Indexed run data \n    runs  Active runs  trash  Deleted runs",
            "title": "Layout"
        },
        {
            "location": "/docs/reference/guild-home/#runs",
            "text": "Active runs (i.e. non-deleted runs) are stored in $GUILD_HOME/runs . Each run is stored in a subdirectory with the same\nname as the run ID.  Run directories contain all files associated with a run, including:   Resource source links  Output generated during the run  Guild metadata   Guild metadata is stored in a subdirectory named  .guild  in each run\ndirectory.",
            "title": "Runs"
        },
        {
            "location": "/docs/reference/guild-home/#guild-metadata",
            "text": "Run directory .guild  Guild metadata \n  \n  output  Output generated by the run OS process \n  output.index  Timestamp and stream information associated with run output \n  attrs  Run attributes \n    \n      cmd  Run OS process command \n      deps  Run dependencies (required resources) \n      env  Run OS process environment \n      exit_status  Run OS process exit status \n      _extra_NAME  Extra config for NAME, defined for the model operation \n      flags  Flag values used for the run \n      opref  Reference to the operation associated with the run \n      started  Started timestamp \n      stopped  Stopped timestamp",
            "title": "Guild metadata"
        },
        {
            "location": "/docs/reference/guild-home/#resource-cache",
            "text": "When Guild resolves resources, it uses  GUILD_HOME/cache/resources \nfor two purposes:   Location for downloading URL resource sources  Location for unpacking archives   Resources are stored in directories that are named using hashes of the\ncorresponding resource source URI, which is an internal representation\nof a resource source location. If a resource exists in the cache, it\nis used rather than downloading or unpacking the resource again.  Guild does not remove items from the resource cache.  You may clear the resource cache by deleting GUILD_HOME/cache/resources . Guild commands will continue to function\nnormally and the cache will be repopulated as needed.",
            "title": "Resource cache"
        },
        {
            "location": "/docs/reference/guild-home/#resource-cache-and-virtual-environments",
            "text": "By default, Guild reuses the resource cache in ~/.guild/cache/resources  (the user-level resource cache) for all\nvirtual environments. This means that cached resources available\nwithin virtual environments without having to re-download\nfiles. However, it also means that virtual environments can modify the\nuser-level resource cache.  If you want to isolate a virtual environment from the user-level\ncache, run the following command after you first activate the\nenvironment:  guild init --local-resource-cache",
            "title": "Resource cache and virtual environments"
        },
        {
            "location": "/docs/reference/guild-home/#runs-index",
            "text": "Guild maintains an index of run data in  GUILD_HOME/cache/runs . This\nindex lets Guild lookup values such as training accuracy and loss\nquickly when performing comparisons.  Guild updates the runs index lazily as it discovers new runs or new\ndata associated with a run.  You may clear the runs cache by deleting GUILD_HOME/cache/runs . Guild commands will continue to function\nnormally and runs will be indexed again as needed.",
            "title": "Runs index"
        },
        {
            "location": "/docs/tutorials/",
            "text": "Tutorials\n\n\n\n\nIntroductory\n\n\nModel development\n\n\nTools\n\n\nTutorial virtual environments\n\n\nSetup a virtual environment on Linux or Mac OS\n\n\n\n\n\n\n\n\n\n\n\n\nIntroductory\n\n\nGo deeper with Guild AI\nTrain and predict with Cloud ML\n\n\n\n\nModel development\n\n\nDevelop a model from scratch\n\n\n\n\nTools\n\n\nWorkflow with Virtualenv\n\n\n\n\n\n\n\nTutorial virtual environments\n\n\nEach tutorial is structured to work with or without a virtual\nenvironment. However, there are some advantages to using a virtual\nenvironment for each tutorial:\n\n\n\n\n\n\nYou can focus on the packages, models, and operations specific to\n  that tutorial.\n\n\n\n\n\n\nYour work on a tutorial work won\u2019t interfere with other projects.\n\n\n\n\n\n\nWhen you\u2019ve completed a tutorial, you can archive or delete the\n  virtual environment directory if you no longer need it.\n\n\n\n\n\n\nFollow these steps to setup a virtual environment for a tutorial:\n\n\n\n\nEnsure that you have Virtualenv installed \u2014 see \nVirtualenv\n  Installation\n\n\nCreate a virtual environment: \nvirtualenv DIR\n\n\nActive the environment: \nsource DIR/bin/activate\n\n\nInstall Guild AI: \npip install guildai\n\n\nInstall TensorFlow: \npip install tensorflow\n or \npip install\n  tensorflow-gpu\n\n\nVerify your setup: \nguild check\n\n\n\n\nOnce you\u2019ve activated your environment, Guild operations will apply to\nthat environment, including package installation and model runs.\n\n\nBelow are scripts that you can copy-and-paste to setup your\nenvironments.\n\n\nSetup a virtual environment on Linux or Mac OS\n\n\necho -n \"Virtualenv directory name: \" && read DIR \\\n&& echo -n \"Use GPU enabled TensorFlow: (y/n) \" && read TF_GPU \\\n&& virtualenv $DIR \\\n&& cd $DIR && source bin/activate \\\n&& pip install guildai tensorflow${TF_GPU/y/-gpu} \\\n&& guild check",
            "title": "Tutorials"
        },
        {
            "location": "/docs/tutorials/#tutorials",
            "text": "Introductory  Model development  Tools  Tutorial virtual environments  Setup a virtual environment on Linux or Mac OS",
            "title": "Tutorials"
        },
        {
            "location": "/docs/tutorials/#introductory",
            "text": "Go deeper with Guild AI Train and predict with Cloud ML",
            "title": "Introductory"
        },
        {
            "location": "/docs/tutorials/#model-development",
            "text": "Develop a model from scratch",
            "title": "Model development"
        },
        {
            "location": "/docs/tutorials/#tools",
            "text": "Workflow with Virtualenv",
            "title": "Tools"
        },
        {
            "location": "/docs/tutorials/#tutorial-virtual-environments",
            "text": "Each tutorial is structured to work with or without a virtual\nenvironment. However, there are some advantages to using a virtual\nenvironment for each tutorial:    You can focus on the packages, models, and operations specific to\n  that tutorial.    Your work on a tutorial work won\u2019t interfere with other projects.    When you\u2019ve completed a tutorial, you can archive or delete the\n  virtual environment directory if you no longer need it.    Follow these steps to setup a virtual environment for a tutorial:   Ensure that you have Virtualenv installed \u2014 see  Virtualenv\n  Installation  Create a virtual environment:  virtualenv DIR  Active the environment:  source DIR/bin/activate  Install Guild AI:  pip install guildai  Install TensorFlow:  pip install tensorflow  or  pip install\n  tensorflow-gpu  Verify your setup:  guild check   Once you\u2019ve activated your environment, Guild operations will apply to\nthat environment, including package installation and model runs.  Below are scripts that you can copy-and-paste to setup your\nenvironments.",
            "title": "Tutorial virtual environments"
        },
        {
            "location": "/docs/tutorials/#setup-a-virtual-environment-on-linux-or-mac-os",
            "text": "echo -n \"Virtualenv directory name: \" && read DIR \\\n&& echo -n \"Use GPU enabled TensorFlow: (y/n) \" && read TF_GPU \\\n&& virtualenv $DIR \\\n&& cd $DIR && source bin/activate \\\n&& pip install guildai tensorflow${TF_GPU/y/-gpu} \\\n&& guild check",
            "title": "Setup a virtual environment on Linux or Mac OS"
        },
        {
            "location": "/docs/tutorials/train-mnist/",
            "text": "Go deeper with Guild AI\n\n\n\n\nRequirements\n\n\nFind the MNIST package\n\n\nInstall mnist\n\n\nTrain the softmax model\n\n\nExamine your training run\n\n\nVisualize your run\n\n\nGuild View\n\n\nTensorBoard\n\n\n\n\n\n\nTrain the CNN model\n\n\nView training metrics using TensorBoard\n\n\nCompare models\n\n\nSummary\n\n\nNext steps\n\n\n\n\nIn this tutorial we\u2019ll train two models with \nMNIST data\n. Training models with Guild AI\nis a simple process of finding a model, installing it, and running\nthe \ntrain\n command.\n\n\nRequirements\n\n\nThis tutorial assumes the following:\n\n\n\n\nGuild AI is \ninstalled and verified\n\n\nYour \nvirtual environment is activated\n\n  (if applicable)\n\n\nYou have a working Internet connection\n\n\n\n\nWhile not required, we recommend using a dedicated virtual environment\nfor this tutorial. To setup your environment, see\n\nTutorial virtual environments\n.\n\n\nFind the MNIST package\n\n\nStart by searching for MNIST models. From a \ncommand\nline\n, run:\n\n\nguild search mnist\n\n\n\n\nYou should see a list of \npackages\n that includes\n\nmnist\n. The \nmnist\n package is a sample that contains models that\ntrain quickly. We\u2019ll use it for this tutorial.\n\n\nInstall \nmnist\n\n\nInstall the \nmnist\n package by running:\n\n\nguild install mnist\n\n\n\n\nThis command downloads and installs \nmnist\n on your system.\n\n\nWhen the install has completed, list the installed packages:\n\n\nguild packages\n\n\n\n\nA package provides \nmodels\n. You can list the installed\nmodels by running:\n\n\nguild models\n\n\n\n\nThe list contains two models associated with the \nmnist\n package:\n\n\nmnist/mnist-cnn      CNN classifier for MNIST\nmnist/mnist-softmax  Softmax regression classifier for MNIST\n\n\n\n\nModels are displayed with their associated packages as\n\nPACKAGE/MODEL\n.\n\n\nEach model provides \noperations\n, which, when run,\nperform actions associated with the model.\n\n\nList available operations by running:\n\n\nguild operations\n\n\n\n\n\n\nNote\n\n\nYou can run \nguild ops\n as a shortcut \nguild operations\n.\n\n\n\n\nThe two models each provide a \ntrain\n operation:\n\n\nmnist/mnist-cnn:train      Train the CNN\nmnist/mnist-softmax:train  Train the softmax regression\n\n\n\n\nOperations are displayed with their associated models as\n\nPACKAGE/MODEL:OPERATION\n.\n\n\nFor background on these topics, see:\n\n\n\n\nPackages\n\n\nModels\n\n\nOperations\n\n\n\n\nTrain the softmax model\n\n\nIn a console, run:\n\n\nguild train mnist-softmax\n\n\n\n\nGuild displays information about the run and asks you to confirm the\noperation. The model defines two \nflags\n:\n\n\n\n\nbatch-size\n\n\nnumber of images to use per batch\n\n\nepochs\n\n\nnumber of epochs to train\n\n\n\n\nPress \nENTER\n to accept the default values and continue.\n\n\nThe operation first resolves any resources required by the model by\ndownloading them. It then runs a TensorFlow script to train the\n\nmnist-softmax\n model.\n\n\nAs the model trains, let\u2019s note a few things:\n\n\n\n\n\n\nThe model requires the MNIST dataset, which is automatically\n  downloaded and saved by Guild as a \nresource\n. This resource\n  remains available for future training operations.\n\n\n\n\n\n\nGuild starts and supervises a TensorFlow script, which is provided\n  by the \nmnist\n package. Scripts are standard Python applications\n  that require little to no modification for use in Guild.\n\n\n\n\n\n\nGuild creates a unique \nrun directory\n that is\n  associated with the training operation. This directory contains the\n  output generated by the script.\n\n\n\n\n\n\nYou\u2019ll learn more in the steps that follow!\n\n\nOnce the dataset files are downloaded, the model trains in a few\nseconds on most systems.\n\n\nCongratulations, you\u2019ve trained your first Guild model!\n\n\nExamine your training run\n\n\nGuild operations generate \nruns\n, which are artifacts that\nare available for inspection during and after the operation. You can\nlist runs as follows:\n\n\nguild runs\n\n\n\n\nThis displays the training run for the \nmnist-softmax\n model. You can\nsee when the operation was started and its status.\n\n\nTo get more information for a run:\n\n\nguild runs info\n\n\n\n\nThis displays detailed information for the latest run.\n\n\nGuild operations typically generate files, which are located in the\nrun directory. You can list the files by including the \n--files\n\noption:\n\n\nguild runs info --files\n\n\n\n\nFor more information on listing runs and getting run info, see \nruns\ninfo\n.\n\n\nVisualize your run\n\n\nGuild provides two visualization tools:\n\n\n\n\nGuild View\n\n\nTensorBoard\n\n\n\n\nGuild View is useful for browsing run details and viewing generating\nartifacts. TensorBoard is a tool created by the TensorFlow team for\nvisualizing training event logs.\n\n\nGuild View\n\n\nLet\u2019s start with Guild View. In a \nseparate console\n, run:\n\n\nguild view\n\n\n\n\nGuild View starts and opens a browser window:\n\n\n\n\nGuild View after initial training of MNIST \nmnist-softmax\n model\n\n\nLet\u2019s take a moment to explore the run.\n\n\n\n\n\n\nThe list on the left shows the available runs. Type something into\n  \nFilter\n to narrow the list.\n\n\n\n\n\n\nThe \nOVERVIEW\n tab contains general information about the run.\n\n\n\n\n\n\nThe \nFILES\n tab contains the list of files in the run directory.\n\n\n\n\n\n\nThe \n\n  button in the upper left is used to view the runs in\n  TensorBoard. We\u2019ll use this in the next section.\n\n\n\n\n\n\nTensorBoard\n\n\nTensorBoard is integrated into Guild in two ways:\n\n\n\n\nLink from Guild View\n\n\nStandalone \ntensorboard\n command\n\n\n\n\nSince we have have Guild View running, let\u2019s use the integrated link\nto open TensorBoard.\n\n\nIn the upper left of Guild View, click \n\n\nThis opens TensorBoard in a separate browser window that displays the\nTensorFlow event logs for the \nmnist-softmax\n run.\n\n\nKeep Guild View and TensorBoard open running in your browser\nthroughout this tutorials \u2014 they both refresh automatically as you\nwork!\n\n\nTrain the CNN model\n\n\nAs we saw earlier, the \nmnist\n package contains two models:\n\nmnist-softmax\n and \nmnist-cnn\n.\n\n\nWe\u2019ll now train the \nmnist-cnn\n model:\n\n\nguild train mnist-cnn\n\n\n\n\nYou\u2019ll again be prompted with the default flag values. Press \nENTER\n\nto accept the defaults and begin training.\n\n\nAs the model trains, let\u2019s note a few things:\n\n\n\n\n\n\nThe \nmnist-cnn\n model uses the same MNIST dataset for training used\n  by \nmnist-softmax\n. Because the dataset was downloaded earlier and\n  saved as a resource, it\u2019s available immediately for training by this\n  operation.\n\n\n\n\n\n\nThis model is considerably more time consuming to train than the\n  softmax model. If your system has a GPU and TensorFlow is configured\n  to use it, you should notice your fans spinning up!\n\n\n\n\n\n\nView training metrics using TensorBoard\n\n\nWhile the model is training, switch to TensorBoard in your browser. On\nthe \nSCALARS\n tab, note the \naccuracy\n graph. This shows the\naccuracy of the both the \nmnist-softmax\n and \nmnist-cnn\n training.\n\n\n\n\nMNIST accuracy in TensorBoard\n\n\nExpand the graph by clicking the fullscreen button\n\n\n\nAdditionally, in the top left of the window, uncheck \nIgnore outliers\nin chart scaling\n. This improves the chart view.\n\n\nNote the differences in accuracy between the two models. The CNN model\nhas a significantly higher accuracy but takes longer to train!\n\n\n\n\nTip\n\n\nYou can compare relative training times between runs by clicking\n\nRELATIVE\n on the left in TensorBoard. Notice how much longer the\n\nmnist-cnn\n model takes to train, even with a GPU!\n\n\n\n\nIn TensorBoard on the \nSCALARS\n tab, you\u2019ll fine a section named\n\nsystem\n. Click \nsystem\n to expand the section. This information\ncontains system metrics for CPU, GPU, memory, and I/O. Guild collects\nand logs this information as the model is trained.\n\n\n\n\nsystem\n section in TensorBoard contains system metrics for CPU,\nGPU, memory, and I/O\n\n\nThe \nSCALARS\n tab contains a lot of information! You can filter the\nresults using the \nFilter tags\n field at the top. For example, to\nview GPU metrics, type \ngpu\n:\n\n\n\n\nFilter to view GPU metrics\n\n\nTake some time and explore the information available in TensorBoard:\n\n\n\n\n\n\nIMAGES\n contains samples of images used for training and\n  validation.\n\n\n\n\n\n\nGRAPHS\n displays a visual representation of the TensorFlow\n  computation graphs for each model.\n\n\n\n\n\n\nDISTRIBUTIONS\n and \nHISTOGRAMS\n show model variable statistics\n  that can be used to debug training issues.\n\n\n\n\n\n\nTensorBoard supports other visualizations (e.g. text and audio) which\nare activate for other models.\n\n\nFor more information, see \nTensorBoard\n.\n\n\nCompare models\n\n\nWhen the \nmnist-cnn\n has finished training, run the following:\n\n\nguild compare\n\n\n\n\nThis opens a live table that you can use to compare runs. Note the\naccuracy of the two models.\n\n\nThe \ncompare\n command is useful for quickly comparing runs.\n\n\nFinally, let\u2019s export the comparison to a CSV file.\n\n\nType \nq\n to exit the compare program and then run the following\ncommand:\n\n\nguild compare --csv > mnist-compare.csv\n\n\n\n\nGuild automatically keeps track of all your training experiments. As\nyou\u2019ve seen, you have quick access to run details from a variety of\ntools:\n\n\n\n\nUse Guild View to browse run results\n\n\nUse TensorBoard to visualize event logs\n\n\nUse \ncompare\n to compare and export results\n\n\n\n\nSummary\n\n\nIn this tutorial we installed and trained two MNIST models. Below is a\nsummary of our workflow and associated commands.\n\n\n\n\nFind an MNIST model\n\n\nguild search mnist\n\n\nInstall models\n\n\nguild install mnist\n\n\nTrain models\n\n\nguild train mnist-cnn\n\n\nVisualize training runs, including TensorBoard integration\n\n\nguild view\n\n\nCompare runs\n\n\nguild compare\n\n\n\n\nNext steps\n\n\nThere\u2019s a lot more in Guild to explore! Consider these next steps:\n\n\n\n\n\n\n\nBrowse Guild AI documentation",
            "title": "Go deeper with Guild AI"
        },
        {
            "location": "/docs/tutorials/train-mnist/#go-deeper-with-guild-ai",
            "text": "Requirements  Find the MNIST package  Install mnist  Train the softmax model  Examine your training run  Visualize your run  Guild View  TensorBoard    Train the CNN model  View training metrics using TensorBoard  Compare models  Summary  Next steps   In this tutorial we\u2019ll train two models with  MNIST data . Training models with Guild AI\nis a simple process of finding a model, installing it, and running\nthe  train  command.",
            "title": "Go deeper with Guild AI"
        },
        {
            "location": "/docs/tutorials/train-mnist/#requirements",
            "text": "This tutorial assumes the following:   Guild AI is  installed and verified  Your  virtual environment is activated \n  (if applicable)  You have a working Internet connection   While not required, we recommend using a dedicated virtual environment\nfor this tutorial. To setup your environment, see Tutorial virtual environments .",
            "title": "Requirements"
        },
        {
            "location": "/docs/tutorials/train-mnist/#find-the-mnist-package",
            "text": "Start by searching for MNIST models. From a  command\nline , run:  guild search mnist  You should see a list of  packages  that includes mnist . The  mnist  package is a sample that contains models that\ntrain quickly. We\u2019ll use it for this tutorial.",
            "title": "Find the MNIST package"
        },
        {
            "location": "/docs/tutorials/train-mnist/#install-mnist",
            "text": "Install the  mnist  package by running:  guild install mnist  This command downloads and installs  mnist  on your system.  When the install has completed, list the installed packages:  guild packages  A package provides  models . You can list the installed\nmodels by running:  guild models  The list contains two models associated with the  mnist  package:  mnist/mnist-cnn      CNN classifier for MNIST\nmnist/mnist-softmax  Softmax regression classifier for MNIST  Models are displayed with their associated packages as PACKAGE/MODEL .  Each model provides  operations , which, when run,\nperform actions associated with the model.  List available operations by running:  guild operations   Note  You can run  guild ops  as a shortcut  guild operations .   The two models each provide a  train  operation:  mnist/mnist-cnn:train      Train the CNN\nmnist/mnist-softmax:train  Train the softmax regression  Operations are displayed with their associated models as PACKAGE/MODEL:OPERATION .  For background on these topics, see:   Packages  Models  Operations",
            "title": "Install mnist"
        },
        {
            "location": "/docs/tutorials/train-mnist/#train-the-softmax-model",
            "text": "In a console, run:  guild train mnist-softmax  Guild displays information about the run and asks you to confirm the\noperation. The model defines two  flags :   batch-size  number of images to use per batch  epochs  number of epochs to train   Press  ENTER  to accept the default values and continue.  The operation first resolves any resources required by the model by\ndownloading them. It then runs a TensorFlow script to train the mnist-softmax  model.  As the model trains, let\u2019s note a few things:    The model requires the MNIST dataset, which is automatically\n  downloaded and saved by Guild as a  resource . This resource\n  remains available for future training operations.    Guild starts and supervises a TensorFlow script, which is provided\n  by the  mnist  package. Scripts are standard Python applications\n  that require little to no modification for use in Guild.    Guild creates a unique  run directory  that is\n  associated with the training operation. This directory contains the\n  output generated by the script.    You\u2019ll learn more in the steps that follow!  Once the dataset files are downloaded, the model trains in a few\nseconds on most systems.  Congratulations, you\u2019ve trained your first Guild model!",
            "title": "Train the softmax model"
        },
        {
            "location": "/docs/tutorials/train-mnist/#examine-your-training-run",
            "text": "Guild operations generate  runs , which are artifacts that\nare available for inspection during and after the operation. You can\nlist runs as follows:  guild runs  This displays the training run for the  mnist-softmax  model. You can\nsee when the operation was started and its status.  To get more information for a run:  guild runs info  This displays detailed information for the latest run.  Guild operations typically generate files, which are located in the\nrun directory. You can list the files by including the  --files \noption:  guild runs info --files  For more information on listing runs and getting run info, see  runs\ninfo .",
            "title": "Examine your training run"
        },
        {
            "location": "/docs/tutorials/train-mnist/#visualize-your-run",
            "text": "Guild provides two visualization tools:   Guild View  TensorBoard   Guild View is useful for browsing run details and viewing generating\nartifacts. TensorBoard is a tool created by the TensorFlow team for\nvisualizing training event logs.",
            "title": "Visualize your run"
        },
        {
            "location": "/docs/tutorials/train-mnist/#guild-view",
            "text": "Let\u2019s start with Guild View. In a  separate console , run:  guild view  Guild View starts and opens a browser window:   Guild View after initial training of MNIST  mnist-softmax  model  Let\u2019s take a moment to explore the run.    The list on the left shows the available runs. Type something into\n   Filter  to narrow the list.    The  OVERVIEW  tab contains general information about the run.    The  FILES  tab contains the list of files in the run directory.    The  \n  button in the upper left is used to view the runs in\n  TensorBoard. We\u2019ll use this in the next section.",
            "title": "Guild View"
        },
        {
            "location": "/docs/tutorials/train-mnist/#tensorboard",
            "text": "TensorBoard is integrated into Guild in two ways:   Link from Guild View  Standalone  tensorboard  command   Since we have have Guild View running, let\u2019s use the integrated link\nto open TensorBoard.  In the upper left of Guild View, click   This opens TensorBoard in a separate browser window that displays the\nTensorFlow event logs for the  mnist-softmax  run.  Keep Guild View and TensorBoard open running in your browser\nthroughout this tutorials \u2014 they both refresh automatically as you\nwork!",
            "title": "TensorBoard"
        },
        {
            "location": "/docs/tutorials/train-mnist/#train-the-cnn-model",
            "text": "As we saw earlier, the  mnist  package contains two models: mnist-softmax  and  mnist-cnn .  We\u2019ll now train the  mnist-cnn  model:  guild train mnist-cnn  You\u2019ll again be prompted with the default flag values. Press  ENTER \nto accept the defaults and begin training.  As the model trains, let\u2019s note a few things:    The  mnist-cnn  model uses the same MNIST dataset for training used\n  by  mnist-softmax . Because the dataset was downloaded earlier and\n  saved as a resource, it\u2019s available immediately for training by this\n  operation.    This model is considerably more time consuming to train than the\n  softmax model. If your system has a GPU and TensorFlow is configured\n  to use it, you should notice your fans spinning up!",
            "title": "Train the CNN model"
        },
        {
            "location": "/docs/tutorials/train-mnist/#view-training-metrics-using-tensorboard",
            "text": "While the model is training, switch to TensorBoard in your browser. On\nthe  SCALARS  tab, note the  accuracy  graph. This shows the\naccuracy of the both the  mnist-softmax  and  mnist-cnn  training.   MNIST accuracy in TensorBoard  Expand the graph by clicking the fullscreen button  Additionally, in the top left of the window, uncheck  Ignore outliers\nin chart scaling . This improves the chart view.  Note the differences in accuracy between the two models. The CNN model\nhas a significantly higher accuracy but takes longer to train!   Tip  You can compare relative training times between runs by clicking RELATIVE  on the left in TensorBoard. Notice how much longer the mnist-cnn  model takes to train, even with a GPU!   In TensorBoard on the  SCALARS  tab, you\u2019ll fine a section named system . Click  system  to expand the section. This information\ncontains system metrics for CPU, GPU, memory, and I/O. Guild collects\nand logs this information as the model is trained.   system  section in TensorBoard contains system metrics for CPU,\nGPU, memory, and I/O  The  SCALARS  tab contains a lot of information! You can filter the\nresults using the  Filter tags  field at the top. For example, to\nview GPU metrics, type  gpu :   Filter to view GPU metrics  Take some time and explore the information available in TensorBoard:    IMAGES  contains samples of images used for training and\n  validation.    GRAPHS  displays a visual representation of the TensorFlow\n  computation graphs for each model.    DISTRIBUTIONS  and  HISTOGRAMS  show model variable statistics\n  that can be used to debug training issues.    TensorBoard supports other visualizations (e.g. text and audio) which\nare activate for other models.  For more information, see  TensorBoard .",
            "title": "View training metrics using TensorBoard"
        },
        {
            "location": "/docs/tutorials/train-mnist/#compare-models",
            "text": "When the  mnist-cnn  has finished training, run the following:  guild compare  This opens a live table that you can use to compare runs. Note the\naccuracy of the two models.  The  compare  command is useful for quickly comparing runs.  Finally, let\u2019s export the comparison to a CSV file.  Type  q  to exit the compare program and then run the following\ncommand:  guild compare --csv > mnist-compare.csv  Guild automatically keeps track of all your training experiments. As\nyou\u2019ve seen, you have quick access to run details from a variety of\ntools:   Use Guild View to browse run results  Use TensorBoard to visualize event logs  Use  compare  to compare and export results",
            "title": "Compare models"
        },
        {
            "location": "/docs/tutorials/train-mnist/#summary",
            "text": "In this tutorial we installed and trained two MNIST models. Below is a\nsummary of our workflow and associated commands.   Find an MNIST model  guild search mnist  Install models  guild install mnist  Train models  guild train mnist-cnn  Visualize training runs, including TensorBoard integration  guild view  Compare runs  guild compare",
            "title": "Summary"
        },
        {
            "location": "/docs/tutorials/train-mnist/#next-steps",
            "text": "There\u2019s a lot more in Guild to explore! Consider these next steps:    Browse Guild AI documentation",
            "title": "Next steps"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/",
            "text": "Train and predict with Cloud ML\n\n\n\n\nRequirements\n\n\nInstall the census package\n\n\nTrain the model locally\n\n\nView training results\n\n\nLabel your run\n\n\nTrain the model in Cloud ML\n\n\nCompare runs\n\n\nUse TensorBoard to compare runs\n\n\n\n\n\n\nImprove model accuracy\n\n\nScale up\n\n\nHyperparameter tuning\n\n\nDeploy a model\n\n\nUse a deployed model to make predictions\n\n\nOnline prediction\n\n\nBatch prediction\n\n\n\n\n\n\nSummary\n\n\nCleanup\n\n\nDelete unneeded Cloud ML jobs\n\n\nDelete unneeded Cloud ML model versions\n\n\nDelete Guild runs\n\n\n\n\n\n\nNext steps\n\n\n\n\nIn this tutorial, we\u2019ll train a model with Google\u2019s Cloud Machine\nLearning Engine (Cloud ML) and use the trained model to make\nprediction.\n\n\nThis tutorial is based on \nCloud ML Engine - Getting Started\n\nand follows similar steps.\n\n\n\n\nImportant\n\n\nThis tutorial uses Cloud ML to train models, which will incur\nchanges to your Google Cloud account. While these charges will be\nnominal (under $5) if you\u2019re concerned about the cost of training\nin Cloud ML, feel free to skip any of the \ncloudml\n operations\nbelow.\n\n\n\n\nRequirements\n\n\nThis tutorial assumes the following:\n\n\n\n\nGuild AI is \ninstalled and verified\n\n\nGoogle Cloud SDK\n is\n  installed (see verification step below)\n\n\nYour \nvirtual environment is activated\n\n  (if applicable)\n\n\nYou have a working Internet connection\n\n\n\n\nWhile not required, we recommend using a dedicated virtual environment\nfor this tutorial. To setup your environment, see\n\nTutorial virtual environments\n.\n\n\nTo verify you have the Google Cloud SDK \ngcloud\n program installed and\nconfigured, run the following:\n\n\ngcloud ml-engine jobs list\n\n\n\n\nIf the command reports an error, resolve the issue before proceeding.\n\n\nInstall the census package\n\n\nWe\u2019ll be training a model in the \ncloudml.census\n package. From a\ncommand line, run:\n\n\nguild install cloudml.census\n\n\n\n\nYou can list the model operations available from the package by\nrunning:\n\n\nguild ops cloudml.census\n\n\n\n\nWe\u2019ll run each of these operations over the course of this tutorial.\n\n\nTrain the model locally\n\n\nBefore we jump into training in Cloud ML, let\u2019s train our model\nlocally. This will confirm that we can get a reasonable result with\nthe data we\u2019re using.\n\n\nTrain the model locally using 1,000 steps:\n\n\nguild train census train-steps=1000\n\n\n\n\nGuild lets you review the flag values for the operation before\nstarted. Press \nENTER\n to accept the values.\n\n\nWith only 1,000 training steps, the model should be trained on most\nsystems in under a minute.\n\n\nAfter the run has finished, list it by running:\n\n\nguild runs\n\n\n\n\nThe \nruns\n command is shorthand for \nruns list\n,\nwhich shows your runs. If you trained other models you\u2019ll see those\nruns in the list as well. If you\u2019d like to limit your listing to runs\nassociated with the \ncloudml-census\n model, use:\n\n\nguild runs -o census\n\n\n\n\nThis command shows run that have an operation containing the string\n\u201ccensus\u201d. Feel free to experiment with other filters to show only the\nruns you\u2019re interested in. Try \nguild runs --help\n for a\nlist of filter options.\n\n\nView training results\n\n\nThe training run generates a number of files. Let\u2019s view them by\nrunning:\n\n\nguild runs info --files\n\n\n\n\nruns info\n is used to view information for a run. By\ndefault the command shows information for the latest run. The\n\n--files\n option tells Guild to include file information in the\noutput.\n\n\nHere\u2019s a breakdown of the files associated with our training run:\n\n\n\n\ncensus-data/*\n\n\nCensus data used for training and test\n\n\ncheckpoint\n\n\nLatest TensorFlow checkpoint marker\n\n\neval_census-eval/events.out.tfevents.*\n\n\nTensorFlow event log generated during evaluation\n\n\nevents.out.tfevents.*\n\n\nTensorFlow event log generated during training\n\n\nexport/census/*/saved_model.pb\n\n\nTensorFlow SavedModel\n\n\nexport/census/*/variables/\n\n\nTensorFlow variables\n\n\ngraph.pbtxt\n\n\nTensorFlow GraphDef\n\n\nmodel.ckpt-1.*\n\n\nTensorFlow checkpoint at training step 1\n\n\nmodel.ckpt-1000.*\n\n\nTensorFlow checkpoint at training step 1000\n\n\ntrainer/\n\n\nLink to training scripts\n\n\n\n\nRun files are stored locally on your system. To view the full path to\nrun files, use the \n--full-path\n option:\n\n\nguild runs info --files --full-path\n\n\n\n\nThis can be useful if you want to refer to a file from the command\nline (e.g. to copy it, open it, etc.)\n\n\nNext we\u2019ll use Guild View to view our training results. In a\n\nseparate console\n, run:\n\n\nguild view\n\n\n\n\nThis opens a browser window showing you the list of runs for the\ncensus classifier. You can see the list of files for a run by clicking\nthe \nFILES\n tab.\n\n\nIf you have runs from other models, you can filter the runs that Guild\nView displays using the \n-o\n option (short for \n--operation\n). To\nlimit runs for operations containing \ncensus\n, use:\n\n\nguild view -o census\n\n\n\n\n\n\nNote\n\n\nThe \nview\n command will not terminate until you stop it by\ntyping \nCtrl-C\n. To continue running Guild commands, run Guild\nView from a separate console.\n\n\n\n\nFrom Guild View, you can open TensorBoard by clicking \n in the upper left of\nthe window.\n\n\nIn TensorBoard, note the model \naccuracy\n, which can be viewed under\nthe \nSCALARS\n tab. The value for our first run should be\napproximately 80%.\n\n\nYou may keep the Guild View and TensorBoard windows open for the\nremainder of this tutorial \u2014 they\u2019ll automatically refresh as you\ngenerate runs. When you no longer need them, close the browser windows\nand stop Guild View by typing \nCtrl-C\n.\n\n\nLabel your run\n\n\nOver time you\u2019ll generate a lot of runs and it\u2019s helpful to label them\nfor easy identification. Guild provides \nruns label\n,\nwhich associates a run with a simple string.\n\n\nLet\u2019s label our run to indicate that it was local and used 1,000\ntraining steps:\n\n\nguild runs label local-1000\n\n\n\n\nGuild will confirm that you want to label the latest run \u2014 press\n\nENTER\n to apply the label.\n\n\nYou can view the new label when you list runs:\n\n\nguild runs\n\n\n\n\n\n\nTip\n\n\nYou can filter runs using their label by running \nguild runs -l\nLABEL\n where \nLABEL\n is a label or part of a label to filter\non. For example, assuming we consistently label our local runs\nusing the convention \nlocal-NNNN\n, we could filter them using\n\nguild runs -l local\n.\n\n\n\n\nTrain the model in Cloud ML\n\n\nNow that we\u2019ve trained the model locally, let\u2019s train it in Cloud ML!\n\n\nWhen training in Cloud ML, we need a Cloud Storage bucket that you\nhave write access to. The bucket is used to store the Cloud ML jobs.\n\n\nFor instructions on creating and configuring a bucket, see \nSet up\nyour Cloud Storage bucket\n\nin the Cloud ML Getting Started guide.\n\n\nWe\u2019ll use the name of your Cloud Storage bucket throughout this\ntutorial so let\u2019s create a variable for it. Run this command to set\nthe name of your bucket:\n\n\necho -n \"Google Cloud Storage bucket name: \" && read BUCKET\n\n\n\n\nVerify that you can use \ngsutil\n to write to the bucket by running:\n\n\necho 'It works!' | gsutil cp - gs://$BUCKET/guild_test \\\n && gsutil cat gs://$BUCKET/guild_test\n\n\n\n\nIf you have write access to \n$BUCKET\n you should see:\n\n\nIt works!\n\n\n\n\nWhen you\u2019ve confirmed that you can write to the Cloud Storage bucket,\ntrain the census classifier in Cloud ML by running:\n\n\nguild run census:cloudml-train \\\n  bucket=$BUCKET \\\n  train-steps=1000 \\\n  --label cloud-1000\n\n\n\n\nReview the operation flag values and press \nENTER\n to begin training.\n\n\nThe \ncloudml-train\n operation is otherwise identical to the \ntrain\n\noperation, but it\u2019s run remotely on Cloud ML rather than locally. The\ncommand needs \nbucket\n to know where to create the Cloud ML job.\n\n\nNote that we included a label for our run using the \n--label\n\noption. We\u2019ll use this convention throughout the tutorial so we can\neasily identity our runs. In this case, we\u2019re indicating that the run\nis cloud based and uses 1,000 steps.\n\n\n\n\nNote\n\n\nEarlier we used \nguild train census\n to train our model. The\n\ntrain\n command is an \nalias\n for the\n\ntrain\n operation, so our previous command was equivalent to running\n\nguild run census:train\n. The \ncloudml-train\n operation doesn\u2019t\nhave an alias, so we have to use the \nrun\n command in this\ncase.\n\n\n\n\nThe remote operation will take longer to run because Cloud ML first\nprovisions a job and waits for TensorFlow to start. This can take up\nto several minutes. You can view the run status as it progresses in\nthe console.\n\n\nYou can also view the run status from Guild View and TensorBoard,\nwhich will both reflect the latest information.\n\n\nAs the Cloud ML job progresses, Guild automatically synchronizes with\nit. This includes downloading generated files such as TensorFlow event\nlogs and saved models as well as updating run status.\n\n\n\n\nNote\n\n\nIf Guild becomes disconnected from a remote job \u2014 for example,\nyou terminate the command by typing \nCtrl-C\n or lose network\nconnectivity \u2014 the job will continue to run in Cloud ML. You can\nsynchronize with the job by running \nguild sync\n. If you\u2019d like\nto reconnect to a running job, run \nguild sync --watch\n. For\nmore information, see the \nsync\n command.\n\n\n\n\nWhen the run has finished, view the list of runs:\n\n\nguild runs\n\n\n\n\nYou should see that the run \ncloud-1000\n has completed.\n\n\nCompare runs\n\n\nEach time you train a new model, you\u2019ll want to compare the results to\nthose of previous runs. Guild\u2019s \ncompare\n command can be used to\nquickly sort and compare training accuracies and loss.\n\n\nCompare your runs by running:\n\n\nguild compare\n\n\n\n\nThis command starts Guild Compare, which is a spreadsheet-like\napplication that displays run details. You can view run status, time,\naccuracy, loss, and hyperparameters. We\u2019ll learn more about\nhyperparameters later in this tutorial.\n\n\nYou can use the arrow keys to navigate to cells that you can\u2019t see.\n\n\nIn Guild Compare, note the accuracy for the two runs \u2014 they should\nbe similar. We\u2019d expect this because we trained the same model with\nthe same flags and data. The only difference between the runs is where\nthe training occurred!\n\n\nTo exit Guild Compare, press \nq\n (quit).\n\n\n\n\nNote\n\n\nAs with the \nview\n command, \ncompare\n will not exit\nuntil you explicitly stop it.  If you want to keep it running,\nstart the command in a \nseparate console\n. Press \nr\n\n(refresh) whenever you want to update the display with the latest\nrun status.\n\n\nFor a complete list of commands supported by Guild Compare, press\n\n?\n while running Guild Compare.\n\n\n\n\nIf you\u2019d like to export the comparison data in \nCSV format\n run:\n\n\nguild compare --csv\n\n\n\n\nTo save the comparison to a file, use your shell\u2019s output redirection:\n\n\nguild compare --csv > census-runs.csv\n\n\n\n\nUse TensorBoard to compare runs\n\n\nLet\u2019s now compare run results in TensorBoard. If you have TensorBoard\nopen from the previous run, it will automatically refresh to display\nthe current training results. If you need to restart Guild View, run:\n\n\nguild view\n\n\n\n\nIn Guild View, click \nVIEW IN TENSORBOARD\n to open TensorBoard.\n\n\nYou may alternatively open TensorBoard directly using the\n\ntensorboard\n command:\n\n\nguild tensorboard\n\n\n\n\n\n\nTip\n\n\nIn TensorBoard, it helps to make two changes when viewing results\nfor census training in this tutorial:\n\n\n\n\nIn the left sidebar, uncheck \nIgnore outliers in chart scaling\n\n\nIn the left sidebar, use the slider to set \nSmoothing\n to \n0\n\n\n\n\nYou can also maximize the \naccuracy\n scalar by clicking\n\n under the chart.\n\n\n\n\nThe values for \naccuracy\n in TensorBoard correspond to the values\ndisplayed in Guild Compare. TensorBoard shows all available scalars for\nselected runs, including their values at various stages of training.\n\n\nImprove model accuracy\n\n\nLet\u2019s try to improve the accuracy of the model with more training.\n\n\nTo increase the training steps, set the \ntrain-steps\n flag to a higher\nlevel. We\u2019ll use 10,000 steps this time:\n\n\nguild run census:cloudml-train \\\n  bucket=$BUCKET \\\n  train-steps=10000 \\\n  --label cloud-10000\n\n\n\n\nWhile this is a 10x increase, the training time Cloud ML should not be\nmuch longer than our previous cloud run.\n\n\n\n\nNote\n\n\nThe census model trains quickly even with 10,000 steps. The time\nspent in Cloud ML for fast training models like \ncensus\n is\nprimarily the setup and teardown of the Cloud ML job.\n\n\n\n\nYou can monitor the training progress using Guild View, Guild Compare\nor TensorBoard.\n\n\nAs the training proceeds, the model accuracy should surpass that of\nthe previous two runs.\n\n\nLet\u2019s use Guild Compare to confirm. If Guild Compare is already\nrunning, press \nr\n to refresh the display. If it\u2019s not running,\nstart it:\n\n\nguild compare\n\n\n\n\nNote the accuracy of the newly trained model \u2014 it should be a few\npercentage points higher than the other two! When you\u2019re done\ncomparing, press \nq\n to exit Guild Compare.\n\n\nScale up\n\n\nLet\u2019s take advantage of Cloud ML\u2019s ability to scale! We can train our\nmodel using distributed TensorFlow and multiple workers by setting the\n\nscale-tier\n flag to a multi-worker tier.\n\n\nSee \nScale Tier\n\nfor more information on Cloud ML\u2019s supported environments.\n\n\nBy default, Cloud ML operations in Guild AI use the \nBASIC\n scale\ntier, which is suitable for small models and experimentation. We can\nalternatively use the \nSTANDARD_1\n scale tier, which uses multiple\nworkers to train the model in parallel.\n\n\nTrain the model again using 10,000 steps, but this time on the\n\nSTANDARD_1\n scale tier:\n\n\nguild run census:cloudml-train \\\n  bucket=$BUCKET \\\n  scale-tier=STANDARD_1 \\\n  train-steps=10000 \\\n  --label scaled-10000\n\n\n\n\nThis operation will take a similar amount of time as the previous\noperation, despite being run on a higher scale tier. This is because\nthe job setup and teardown overhead dominates the overall operation\ntime.\n\n\nWe can use TensorBoard to view the time spent in training. If\nTensorBoard isn\u2019t already running, you can start it directly by\nrunning the following in a \nseparate console\n:\n\n\nguild tensorboard\n\n\n\n\nIn TensorBoard, in the left sidebar under \nHorizontal Axis\n, click\n\nRELATIVE\n. This will plot scalars using their relative times along\nthe horizontal axis. This is useful for viewing time spent actually\ntraining as it does not include job setup and teardown time.\n\n\nNote the relative times between the last two runs. Our latest run,\nwhich used a higher scale tier, should be noticeably faster for the\nsame number of steps.\n\n\nHyperparameter tuning\n\n\nCloud ML provides a feature called \nhyperparameter tuning\n, which can\noptimize training results by automatically modifying model\nhyperparameters. A hyperparameter is a model setting that can be\nconfigured with flags. The census model we\u2019re training has three\ntunable hyperparameters:\n\n\n\n\nfirst-layer-size\n\n\nNumber of nodes in the first layer of the DNN (default is 100)\n\n\nlayers\n\n\nNumber of layers in the DNN (default is 4)\n\n\nscale-factor\n\n\nHow quickly the size of the layers in the DNN decay (default is 0.7)\n\n\n\n\nUp to this point we\u2019ve training using the default values for each\nflag. But what if we could improve our model\u2019s predictive accuracy by\nusing different values?\n\n\nCloud ML hyperparameter tuning tries different values for us using an\nalgorithm that attempts to optimize the training result. In this case\nwe want to maximize predictive accuracy.\n\n\nFor more information see \nOverview of Hyperparameter Tuning\n\nin the Cloud ML Engine documentation.\n\n\nLet\u2019s see if we can improve model accuracy accuracy using Cloud ML\nhyperparameter tuning. We can use our model\u2019s \ncloudml-hptune\n\noperation for this.\n\n\n\n\nImportant\n\n\nThe command below trains the census model 6 times, each\ntime using 10,000 steps. While this is still a relatively low cost\ntraining run, if you\u2019re concerned about the cost of this\noperation, feel free to skip this command.\n\n\n\n\nStart a hyperparameter tuning run with 6 trials of 10,000 training\nsteps each by running:\n\n\nguild run census:cloudml-hptune \\\n  bucket=$BUCKET \\\n  max-trials=6 \\\n  train-steps=10000 \\\n  --label tune-6x-10000\n\n\n\n\nThis uses the \ndefault tuning configuration\n\nprovided by the \ncloudml.census\n package and changes the default\n\nmax-trials\n count to 6 from 4. The \nmax-trials\n flag determines how many\ntrials are run during the hyperparameter tuning operation.\n\n\nEach trial in Cloud ML is synchronized as a new Guild run, which you\ncan treat like any other run.\n\n\nAs hyperparameter tuning progresses, use Guild Compare to view the\ntrial results. As each trial is completed, it will appear as a new run\nwith its own accuracy and loss. Guild automatically labels trial runs\nso you can identify the tuning run that generated them.\n\n\nTo view trial run results while the tuning operation is running, start\nGuild Compare in a \nseparate console\n by running:\n\n\nguild compare\n\n\n\n\nAs the trials are completed, press \nr\n in Guild Compare to refresh\nthe display and view the trial results. You can compare the\ndifferences in accuracy and loss to see what hyperparameter values\nprovide the best result.\n\n\n\n\nNote\n\n\nThe \ncloudml-hptune\n itself will not have accuracy or loss. It\u2019s\njob is limited to starting training run trials, which do have\naccuracy and loss.\n\n\n\n\n\n\nTip\n\n\nIn Guild Compare, you can sort any column in numeric descending\norder by moving the cursor to the column and pressing \n1\n. This\nis useful for sorting runs by accuracy \u2014 the most accuracy model\nwill appear at the top of the list. You can sort in ascending\norder by pressing \n!\n. This is useful for sorting runs by loss.\n\n\nNote that you must re-order a column after you refresh the display.\n\n\n\n\nWait for the hyperparameter tuning operation to finish. In the next\nsection we\u2019ll deploy the trial that has the highest accuracy.\n\n\nDeploy a model\n\n\nNow that we\u2019ve tuned our hyperparameters to find an optimal model (at\nleast within the six trials that we ran) it\u2019s time to deploy a model!\n\n\nUsing Guild Compare, select the run with the highest accuracy. You may\nsort runs by accuracy by moving the cursor to the \naccuracy\n column\nand pressing \n1\n. This will reorder the runs, displaying the most\naccurate run first. Note its run ID (from the \nrun\n column on the\nfar left) \u2013 we\u2019ll use this ID for deployment.\n\n\nExit Guild Compare by pressing \nq\n.\n\n\nDefine a variable \nDEPLOY_RUN\n with the run ID you selected:\n\n\necho -n \"Run ID to deploy: \" && read DEPLOY_RUN\n\n\n\n\nWhen prompted, enter the run ID you\u2019d like to deploy (e.g. the run\nwith the highest accuracy).\n\n\n\n\nNote\n\n\nWhen specifying run IDs in Guild, you don\u2019t have to provide\nthe entire run ID \u2014 you may use a run ID prefix as long as it\u2019s\nunique. Usually the first few characters is enough to identify a\nrun.\n\n\n\n\nVerify that the specified run is the one you\u2019d like to deploy. You can\nconfirm the accuracy for \nDEPLOY_RUN\n by running:\n\n\nguild compare $DEPLOY_RUN --table\n\n\n\n\nThe table should contain a single run. If it contains a run other than\nthe one you want to deploy, re-enter the run ID using the step above.\n\n\nWhen you\u2019re ready, deploy the model for your selected run using the\n\ncloudml-deploy\n operation:\n\n\nguild run census:cloudml-deploy run=$DEPLOY_RUN bucket=$BUCKET\n\n\n\n\nThis will create a model in Cloud ML named \ncensus_dnn\n if one\ndoesn\u2019t already exist. This name is generated using the deployed model\nname \ncensus-dnn\n. Cloud ML doesn\u2019t allow certain characters in\nmodel names (e.g. \n-\n) and Guild replaces these with an underscore\n(\n_\n). If you want to specify a different model name, use the\n\nmodel\n flag when running the \ncloudml-deploy\n operation.\n\n\nAfter ensuring that a Cloud ML model exists, Guild creates a model\n\nversion\n. This deploys the trained model to Cloud ML. By default,\nGuild uses a version containing the deployment run ID. If you want to\nspecify a different version, use the \nversion\n flag when running the\n\ncloudml-deploy\n operation.\n\n\nFor more information on model deployment in Cloud ML, see:\n\n\n\n\nPrediction Basics - Model deployment\n\n  Cloud ML concepts\n\n\nDeploying Models\n Cloud\n  ML how-to guide\n\n\n\n\nVerify the deployed model and its version by running:\n\n\ngcloud ml-engine versions list --model census_dnn\n\n\n\n\nYou should see the newly deployed \ncensus_dnn\n model version.\n\n\n\n\nNote\n\n\nGuild does not provide a complete interface for managing deployed\nCloud ML models and versions. For a details on Cloud ML models and\nversions, see \ngcloud ml-engine\n\ncommand line reference.\n\n\n\n\nList your current runs:\n\n\nguild runs\n\n\n\n\nNote the latest run \u2014 it will be for the \ncloudml-deploy\n operation\nand have a label that contains the deployed model run ID. Deployments\nare like any other operation in Guild.\n\n\nLet\u2019s examine the \ncloudml-deploy\n run:\n\n\nguild runs info\n\n\n\n\nNote the following attributes in the output:\n\n\n\n\ncloudml_model_name\n\n\nName of the deployed Cloud ML model\n\n\ncloudml_model_version\n\n\nVersion of the deployed Cloud ML model\n\n\ncloudml_model_binaries\n\n\nGoogle Cloud Storage location of the deployed model binaries\n\n\ntrained_model_run\n\n\nRun ID of the deployed trained model\n\n\n\n\nThis information can be used to verify a deployment and is retained as\nan artifact of the deploy operation. In the next section, we\u2019ll see\nhow this information is used to run predictions.\n\n\nUse a deployed model to make predictions\n\n\nIn our final tutorial segment, we\u2019ll use our recently deployed model\nto make predictions!\n\n\nCloud ML makes predictions by running the inference operation of\ndeployed model with new data. Data are provided as one or more\n\ninstances\n, each corresponding to a prediction that will be made\nusing the trained model. You may provide instances as JSON or plain\ntext format.\n\n\nFor details on how Cloud ML makes predictions, see \nPrediction Basics\n in\nCloud ML concepts.\n\n\nCloud ML can make predictions in two ways: \nonline\n and\n\nbatch\n. Online predictions are made immediately and returned in the\noperation result. Batch predictions are submitted to Cloud ML as a job\nthat runs in the background, similar to training. When a batched\nprediction job is completed, predictions are written to a file in the\njob directory.\n\n\nIn general, online prediction is faster if you have a small number of\npredictions to make while batch processing is faster when you need to\nmake a large number of predictions. The specific performance\ncharacteristics will vary across models. Refer to \nOnline prediction\nversus batch prediction\n\nin Cloud ML Prediction Basics for more information.\n\n\nWe\u2019ll use both techniques to see how each works.\n\n\nOnline prediction\n\n\nFirst, let\u2019s run an online prediction:\n\n\nguild run census:cloudml-predict\n\n\n\n\nGuild will prompt you before running the operation. Press \nENTER\n to\nrun the operation.\n\n\nThe prediction is made using the latest deployed version of the\n\ncensus-dnn\n model (\ncensus_dnn\n in Cloud ML). The model package\nprovides sample inputs to make the prediction. You can specify your\nown using the \ninstances\n flags.\n\n\nTo experiment with your own inputs, download the \ncensus prediction\nsamples\n\nand make changes accordingly. Assuming your samples is named\n\ncensus-inputs.json\n and is located in the current directory, you can\nrun a \ncloudml-predict\n operation using:\n\n\nguild run census:cloudml-predict instances=census-inputs.json\n\n\n\n\nFor online prediction, the results are printed to the\nconsole. However, they are also saved as a file in the run\ndirectory. Let\u2019s take a look!\n\n\nIf Guild View isn\u2019t already running, start it in a\n\nseparate console\n by running:\n\n\nguild view\n\n\n\n\nIn Guild View, select the latest run \u2014 it should be a\n\ncloudml-predict\n operation \u2014 and click the \nFILES\n tab. The run\nwill have two files:\n\n\n\n\nprediction.inputs\n\n\nThe inputs used in the prediction\n\n\nprediction.results\n\n\nThe prediction results\n\n\n\n\nClick on \nprediction.results\n to see how the instances were\nclassified by the deployed model. The values correspond to the inputs\nprovided in \nprediction.inputs\n. Click the \nNEXT\n button to view the\ncontents of \nprediction.inputs\n.\n\n\n\n\nTip\n\n\nGuild View can be used to view the contents of some files:\ntext files, images, and audio. Look for a grey button background\non the file name, which can be clicked to open the file.\n\n\n\n\nAs you can see, the \ncloudml-predict\n operation generates a run like\nother operations. In this case the run contains information about the\nmodel version used to make the prediction along with the prediction\ninputs and outputs.\n\n\nBatch prediction\n\n\nIn the previous section we performed an \nonline\n prediction that\nreturned the result immediately. In this section we\u2019ll use \nbatch\n\nprediction. Batch prediction runs on Cloud ML as a job.\n\n\nWe use the \ncloudml-batch-predict\n operation to start a prediction\njob. Start a batch prediction job by running:\n\n\nguild run census:cloudml-batch-predict bucket=$BUCKET\n\n\n\n\nThis will use the sample prediction inputs as before. We need to\nspecify \nbucket\n to tell Guild where to create the job.\n\n\nThe job will take some time to complete. When finished, it will save\ngenerated a file named \nprediction.results-00000-of-00001\n, which you\ncan view in Guild View, or inspect from the command line.\n\n\nWhen the operation has finished, view the generated files by running:\n\n\nguild runs info --files --full-path\n\n\n\n\nThe \n--full-path\n option is used to show the full path to\n\nprediction.results-00000-of-00001\n, which you can use to copy or view\nthe file.\n\n\nYou may use online or batch predictions as needed to use your deployed\nmodels in Cloud ML!\n\n\nSummary\n\n\nIn this tutorial we worked with Google\u2019s Cloud Machine Leaning Engine\n(Cloud ML) to train and deploy a classifier.\n\n\nWe covered a number of topics:\n\n\n\n\n\n\nTrain a model locally to sanity-check our results and then train in\n  Cloud ML to scale up\n\n\n\n\n\n\nUse hyperparameter tuning in Cloud ML to optimize our predictive\n  accuracy with a set of hyperparameter values\n\n\n\n\n\n\nEvaluate runs using Guild View, Guild Compare, and TensorBoard\n\n\n\n\n\n\nSelect an optimized model and deploy it to Cloud ML\n\n\n\n\n\n\nUse a model deployed in Cloud ML to make predictions using new data\n\n\n\n\n\n\nGuild AI models and their operations encapsulate the complexities of\nthis process, letting you focus on the high level workflow and get\nyour work done faster!\n\n\nCleanup\n\n\nOver the course of this tutorial we generated a number of runs and\nCloud ML jobs. If you no longer need these, you may delete them to\nfree up resources.\n\n\nDelete unneeded Cloud ML jobs\n\n\nUse the \ngsutil\n program to delete any jobs from your Cloud Storage\nbucket that you don\u2019t need. To delete all Guild related files, run:\n\n\ngsutil rm -r gs://$BUCKET/guild_*\n\n\n\n\nDelete unneeded Cloud ML model versions\n\n\nUse the \ngcloud\n program to delete any model versions you don\u2019t need.\n\n\nFirst, list the model versions:\n\n\ngcloud ml-engine versions list --model census_dnn\n\n\n\n\nFor each version that you want to delete, run:\n\n\ngcloud ml-engine versions delete --model census_dnn VERSION\n\n\n\n\nwhere \nVERSION\n is the model version you want to delete.\n\n\nIf you no longer need the model, you may delete it provided there are\nno versions associated with it:\n\n\ngcloud ml-engine models delete census_dnn\n\n\n\n\nDelete Guild runs\n\n\nIf you ran the tutorials from a virtual environment, you can simply\ndelete the virtual environment directory, which will free up all disk\nspace used by steps from this tutorial.\n\n\n\n\nImportant\n\n\nOnly delete virtual environments when you\u2019re certain you\nno longer need them.\n\n\n\n\nIf you want to only delete runs associated with the \ncloudml-census\n\nmodel, which was used in this tutorial, run:\n\n\nguild runs delete -o cloudml-census -p\n\n\n\n\nThe \n-p\n option indicates that the delete should be\n\npermanent\n. This ensures that the runs no longer consume disk\nspace. Omit this option if you want to retain the ability to restore\nthem at a later time. Note that disk space will not be freed up for\nthese jobs until you permanently delete them (see the \npurge\n\ncommand).\n\n\nFinally, if you deleted any runs without using the \n-p\n option and\nyou want to free up disk space consumed by them, you can can\npermanently delete them by running:\n\n\nguild runs purge\n\n\n\n\n\n\nImportant\n\n\nReview the list of runs carefully before permanently deleting\nthem!\n\n\n\n\nNext steps\n\n\n\n\nRead \nCloud ML Engine - Getting Started\n\n  to view the same workflow using lower-level Cloud ML commands",
            "title": "Train and predict with Cloud ML"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#train-and-predict-with-cloud-ml",
            "text": "Requirements  Install the census package  Train the model locally  View training results  Label your run  Train the model in Cloud ML  Compare runs  Use TensorBoard to compare runs    Improve model accuracy  Scale up  Hyperparameter tuning  Deploy a model  Use a deployed model to make predictions  Online prediction  Batch prediction    Summary  Cleanup  Delete unneeded Cloud ML jobs  Delete unneeded Cloud ML model versions  Delete Guild runs    Next steps   In this tutorial, we\u2019ll train a model with Google\u2019s Cloud Machine\nLearning Engine (Cloud ML) and use the trained model to make\nprediction.  This tutorial is based on  Cloud ML Engine - Getting Started \nand follows similar steps.   Important  This tutorial uses Cloud ML to train models, which will incur\nchanges to your Google Cloud account. While these charges will be\nnominal (under $5) if you\u2019re concerned about the cost of training\nin Cloud ML, feel free to skip any of the  cloudml  operations\nbelow.",
            "title": "Train and predict with Cloud ML"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#requirements",
            "text": "This tutorial assumes the following:   Guild AI is  installed and verified  Google Cloud SDK  is\n  installed (see verification step below)  Your  virtual environment is activated \n  (if applicable)  You have a working Internet connection   While not required, we recommend using a dedicated virtual environment\nfor this tutorial. To setup your environment, see Tutorial virtual environments .  To verify you have the Google Cloud SDK  gcloud  program installed and\nconfigured, run the following:  gcloud ml-engine jobs list  If the command reports an error, resolve the issue before proceeding.",
            "title": "Requirements"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#install-the-census-package",
            "text": "We\u2019ll be training a model in the  cloudml.census  package. From a\ncommand line, run:  guild install cloudml.census  You can list the model operations available from the package by\nrunning:  guild ops cloudml.census  We\u2019ll run each of these operations over the course of this tutorial.",
            "title": "Install the census package"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#train-the-model-locally",
            "text": "Before we jump into training in Cloud ML, let\u2019s train our model\nlocally. This will confirm that we can get a reasonable result with\nthe data we\u2019re using.  Train the model locally using 1,000 steps:  guild train census train-steps=1000  Guild lets you review the flag values for the operation before\nstarted. Press  ENTER  to accept the values.  With only 1,000 training steps, the model should be trained on most\nsystems in under a minute.  After the run has finished, list it by running:  guild runs  The  runs  command is shorthand for  runs list ,\nwhich shows your runs. If you trained other models you\u2019ll see those\nruns in the list as well. If you\u2019d like to limit your listing to runs\nassociated with the  cloudml-census  model, use:  guild runs -o census  This command shows run that have an operation containing the string\n\u201ccensus\u201d. Feel free to experiment with other filters to show only the\nruns you\u2019re interested in. Try  guild runs --help  for a\nlist of filter options.",
            "title": "Train the model locally"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#view-training-results",
            "text": "The training run generates a number of files. Let\u2019s view them by\nrunning:  guild runs info --files  runs info  is used to view information for a run. By\ndefault the command shows information for the latest run. The --files  option tells Guild to include file information in the\noutput.  Here\u2019s a breakdown of the files associated with our training run:   census-data/*  Census data used for training and test  checkpoint  Latest TensorFlow checkpoint marker  eval_census-eval/events.out.tfevents.*  TensorFlow event log generated during evaluation  events.out.tfevents.*  TensorFlow event log generated during training  export/census/*/saved_model.pb  TensorFlow SavedModel  export/census/*/variables/  TensorFlow variables  graph.pbtxt  TensorFlow GraphDef  model.ckpt-1.*  TensorFlow checkpoint at training step 1  model.ckpt-1000.*  TensorFlow checkpoint at training step 1000  trainer/  Link to training scripts   Run files are stored locally on your system. To view the full path to\nrun files, use the  --full-path  option:  guild runs info --files --full-path  This can be useful if you want to refer to a file from the command\nline (e.g. to copy it, open it, etc.)  Next we\u2019ll use Guild View to view our training results. In a separate console , run:  guild view  This opens a browser window showing you the list of runs for the\ncensus classifier. You can see the list of files for a run by clicking\nthe  FILES  tab.  If you have runs from other models, you can filter the runs that Guild\nView displays using the  -o  option (short for  --operation ). To\nlimit runs for operations containing  census , use:  guild view -o census   Note  The  view  command will not terminate until you stop it by\ntyping  Ctrl-C . To continue running Guild commands, run Guild\nView from a separate console.   From Guild View, you can open TensorBoard by clicking   in the upper left of\nthe window.  In TensorBoard, note the model  accuracy , which can be viewed under\nthe  SCALARS  tab. The value for our first run should be\napproximately 80%.  You may keep the Guild View and TensorBoard windows open for the\nremainder of this tutorial \u2014 they\u2019ll automatically refresh as you\ngenerate runs. When you no longer need them, close the browser windows\nand stop Guild View by typing  Ctrl-C .",
            "title": "View training results"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#label-your-run",
            "text": "Over time you\u2019ll generate a lot of runs and it\u2019s helpful to label them\nfor easy identification. Guild provides  runs label ,\nwhich associates a run with a simple string.  Let\u2019s label our run to indicate that it was local and used 1,000\ntraining steps:  guild runs label local-1000  Guild will confirm that you want to label the latest run \u2014 press ENTER  to apply the label.  You can view the new label when you list runs:  guild runs   Tip  You can filter runs using their label by running  guild runs -l\nLABEL  where  LABEL  is a label or part of a label to filter\non. For example, assuming we consistently label our local runs\nusing the convention  local-NNNN , we could filter them using guild runs -l local .",
            "title": "Label your run"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#train-the-model-in-cloud-ml",
            "text": "Now that we\u2019ve trained the model locally, let\u2019s train it in Cloud ML!  When training in Cloud ML, we need a Cloud Storage bucket that you\nhave write access to. The bucket is used to store the Cloud ML jobs.  For instructions on creating and configuring a bucket, see  Set up\nyour Cloud Storage bucket \nin the Cloud ML Getting Started guide.  We\u2019ll use the name of your Cloud Storage bucket throughout this\ntutorial so let\u2019s create a variable for it. Run this command to set\nthe name of your bucket:  echo -n \"Google Cloud Storage bucket name: \" && read BUCKET  Verify that you can use  gsutil  to write to the bucket by running:  echo 'It works!' | gsutil cp - gs://$BUCKET/guild_test \\\n && gsutil cat gs://$BUCKET/guild_test  If you have write access to  $BUCKET  you should see:  It works!  When you\u2019ve confirmed that you can write to the Cloud Storage bucket,\ntrain the census classifier in Cloud ML by running:  guild run census:cloudml-train \\\n  bucket=$BUCKET \\\n  train-steps=1000 \\\n  --label cloud-1000  Review the operation flag values and press  ENTER  to begin training.  The  cloudml-train  operation is otherwise identical to the  train \noperation, but it\u2019s run remotely on Cloud ML rather than locally. The\ncommand needs  bucket  to know where to create the Cloud ML job.  Note that we included a label for our run using the  --label \noption. We\u2019ll use this convention throughout the tutorial so we can\neasily identity our runs. In this case, we\u2019re indicating that the run\nis cloud based and uses 1,000 steps.   Note  Earlier we used  guild train census  to train our model. The train  command is an  alias  for the train  operation, so our previous command was equivalent to running guild run census:train . The  cloudml-train  operation doesn\u2019t\nhave an alias, so we have to use the  run  command in this\ncase.   The remote operation will take longer to run because Cloud ML first\nprovisions a job and waits for TensorFlow to start. This can take up\nto several minutes. You can view the run status as it progresses in\nthe console.  You can also view the run status from Guild View and TensorBoard,\nwhich will both reflect the latest information.  As the Cloud ML job progresses, Guild automatically synchronizes with\nit. This includes downloading generated files such as TensorFlow event\nlogs and saved models as well as updating run status.   Note  If Guild becomes disconnected from a remote job \u2014 for example,\nyou terminate the command by typing  Ctrl-C  or lose network\nconnectivity \u2014 the job will continue to run in Cloud ML. You can\nsynchronize with the job by running  guild sync . If you\u2019d like\nto reconnect to a running job, run  guild sync --watch . For\nmore information, see the  sync  command.   When the run has finished, view the list of runs:  guild runs  You should see that the run  cloud-1000  has completed.",
            "title": "Train the model in Cloud ML"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#compare-runs",
            "text": "Each time you train a new model, you\u2019ll want to compare the results to\nthose of previous runs. Guild\u2019s  compare  command can be used to\nquickly sort and compare training accuracies and loss.  Compare your runs by running:  guild compare  This command starts Guild Compare, which is a spreadsheet-like\napplication that displays run details. You can view run status, time,\naccuracy, loss, and hyperparameters. We\u2019ll learn more about\nhyperparameters later in this tutorial.  You can use the arrow keys to navigate to cells that you can\u2019t see.  In Guild Compare, note the accuracy for the two runs \u2014 they should\nbe similar. We\u2019d expect this because we trained the same model with\nthe same flags and data. The only difference between the runs is where\nthe training occurred!  To exit Guild Compare, press  q  (quit).   Note  As with the  view  command,  compare  will not exit\nuntil you explicitly stop it.  If you want to keep it running,\nstart the command in a  separate console . Press  r \n(refresh) whenever you want to update the display with the latest\nrun status.  For a complete list of commands supported by Guild Compare, press ?  while running Guild Compare.   If you\u2019d like to export the comparison data in  CSV format  run:  guild compare --csv  To save the comparison to a file, use your shell\u2019s output redirection:  guild compare --csv > census-runs.csv",
            "title": "Compare runs"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#use-tensorboard-to-compare-runs",
            "text": "Let\u2019s now compare run results in TensorBoard. If you have TensorBoard\nopen from the previous run, it will automatically refresh to display\nthe current training results. If you need to restart Guild View, run:  guild view  In Guild View, click  VIEW IN TENSORBOARD  to open TensorBoard.  You may alternatively open TensorBoard directly using the tensorboard  command:  guild tensorboard   Tip  In TensorBoard, it helps to make two changes when viewing results\nfor census training in this tutorial:   In the left sidebar, uncheck  Ignore outliers in chart scaling  In the left sidebar, use the slider to set  Smoothing  to  0   You can also maximize the  accuracy  scalar by clicking  under the chart.   The values for  accuracy  in TensorBoard correspond to the values\ndisplayed in Guild Compare. TensorBoard shows all available scalars for\nselected runs, including their values at various stages of training.",
            "title": "Use TensorBoard to compare runs"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#improve-model-accuracy",
            "text": "Let\u2019s try to improve the accuracy of the model with more training.  To increase the training steps, set the  train-steps  flag to a higher\nlevel. We\u2019ll use 10,000 steps this time:  guild run census:cloudml-train \\\n  bucket=$BUCKET \\\n  train-steps=10000 \\\n  --label cloud-10000  While this is a 10x increase, the training time Cloud ML should not be\nmuch longer than our previous cloud run.   Note  The census model trains quickly even with 10,000 steps. The time\nspent in Cloud ML for fast training models like  census  is\nprimarily the setup and teardown of the Cloud ML job.   You can monitor the training progress using Guild View, Guild Compare\nor TensorBoard.  As the training proceeds, the model accuracy should surpass that of\nthe previous two runs.  Let\u2019s use Guild Compare to confirm. If Guild Compare is already\nrunning, press  r  to refresh the display. If it\u2019s not running,\nstart it:  guild compare  Note the accuracy of the newly trained model \u2014 it should be a few\npercentage points higher than the other two! When you\u2019re done\ncomparing, press  q  to exit Guild Compare.",
            "title": "Improve model accuracy"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#scale-up",
            "text": "Let\u2019s take advantage of Cloud ML\u2019s ability to scale! We can train our\nmodel using distributed TensorFlow and multiple workers by setting the scale-tier  flag to a multi-worker tier.  See  Scale Tier \nfor more information on Cloud ML\u2019s supported environments.  By default, Cloud ML operations in Guild AI use the  BASIC  scale\ntier, which is suitable for small models and experimentation. We can\nalternatively use the  STANDARD_1  scale tier, which uses multiple\nworkers to train the model in parallel.  Train the model again using 10,000 steps, but this time on the STANDARD_1  scale tier:  guild run census:cloudml-train \\\n  bucket=$BUCKET \\\n  scale-tier=STANDARD_1 \\\n  train-steps=10000 \\\n  --label scaled-10000  This operation will take a similar amount of time as the previous\noperation, despite being run on a higher scale tier. This is because\nthe job setup and teardown overhead dominates the overall operation\ntime.  We can use TensorBoard to view the time spent in training. If\nTensorBoard isn\u2019t already running, you can start it directly by\nrunning the following in a  separate console :  guild tensorboard  In TensorBoard, in the left sidebar under  Horizontal Axis , click RELATIVE . This will plot scalars using their relative times along\nthe horizontal axis. This is useful for viewing time spent actually\ntraining as it does not include job setup and teardown time.  Note the relative times between the last two runs. Our latest run,\nwhich used a higher scale tier, should be noticeably faster for the\nsame number of steps.",
            "title": "Scale up"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#hyperparameter-tuning",
            "text": "Cloud ML provides a feature called  hyperparameter tuning , which can\noptimize training results by automatically modifying model\nhyperparameters. A hyperparameter is a model setting that can be\nconfigured with flags. The census model we\u2019re training has three\ntunable hyperparameters:   first-layer-size  Number of nodes in the first layer of the DNN (default is 100)  layers  Number of layers in the DNN (default is 4)  scale-factor  How quickly the size of the layers in the DNN decay (default is 0.7)   Up to this point we\u2019ve training using the default values for each\nflag. But what if we could improve our model\u2019s predictive accuracy by\nusing different values?  Cloud ML hyperparameter tuning tries different values for us using an\nalgorithm that attempts to optimize the training result. In this case\nwe want to maximize predictive accuracy.  For more information see  Overview of Hyperparameter Tuning \nin the Cloud ML Engine documentation.  Let\u2019s see if we can improve model accuracy accuracy using Cloud ML\nhyperparameter tuning. We can use our model\u2019s  cloudml-hptune \noperation for this.   Important  The command below trains the census model 6 times, each\ntime using 10,000 steps. While this is still a relatively low cost\ntraining run, if you\u2019re concerned about the cost of this\noperation, feel free to skip this command.   Start a hyperparameter tuning run with 6 trials of 10,000 training\nsteps each by running:  guild run census:cloudml-hptune \\\n  bucket=$BUCKET \\\n  max-trials=6 \\\n  train-steps=10000 \\\n  --label tune-6x-10000  This uses the  default tuning configuration \nprovided by the  cloudml.census  package and changes the default max-trials  count to 6 from 4. The  max-trials  flag determines how many\ntrials are run during the hyperparameter tuning operation.  Each trial in Cloud ML is synchronized as a new Guild run, which you\ncan treat like any other run.  As hyperparameter tuning progresses, use Guild Compare to view the\ntrial results. As each trial is completed, it will appear as a new run\nwith its own accuracy and loss. Guild automatically labels trial runs\nso you can identify the tuning run that generated them.  To view trial run results while the tuning operation is running, start\nGuild Compare in a  separate console  by running:  guild compare  As the trials are completed, press  r  in Guild Compare to refresh\nthe display and view the trial results. You can compare the\ndifferences in accuracy and loss to see what hyperparameter values\nprovide the best result.   Note  The  cloudml-hptune  itself will not have accuracy or loss. It\u2019s\njob is limited to starting training run trials, which do have\naccuracy and loss.    Tip  In Guild Compare, you can sort any column in numeric descending\norder by moving the cursor to the column and pressing  1 . This\nis useful for sorting runs by accuracy \u2014 the most accuracy model\nwill appear at the top of the list. You can sort in ascending\norder by pressing  ! . This is useful for sorting runs by loss.  Note that you must re-order a column after you refresh the display.   Wait for the hyperparameter tuning operation to finish. In the next\nsection we\u2019ll deploy the trial that has the highest accuracy.",
            "title": "Hyperparameter tuning"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#deploy-a-model",
            "text": "Now that we\u2019ve tuned our hyperparameters to find an optimal model (at\nleast within the six trials that we ran) it\u2019s time to deploy a model!  Using Guild Compare, select the run with the highest accuracy. You may\nsort runs by accuracy by moving the cursor to the  accuracy  column\nand pressing  1 . This will reorder the runs, displaying the most\naccurate run first. Note its run ID (from the  run  column on the\nfar left) \u2013 we\u2019ll use this ID for deployment.  Exit Guild Compare by pressing  q .  Define a variable  DEPLOY_RUN  with the run ID you selected:  echo -n \"Run ID to deploy: \" && read DEPLOY_RUN  When prompted, enter the run ID you\u2019d like to deploy (e.g. the run\nwith the highest accuracy).   Note  When specifying run IDs in Guild, you don\u2019t have to provide\nthe entire run ID \u2014 you may use a run ID prefix as long as it\u2019s\nunique. Usually the first few characters is enough to identify a\nrun.   Verify that the specified run is the one you\u2019d like to deploy. You can\nconfirm the accuracy for  DEPLOY_RUN  by running:  guild compare $DEPLOY_RUN --table  The table should contain a single run. If it contains a run other than\nthe one you want to deploy, re-enter the run ID using the step above.  When you\u2019re ready, deploy the model for your selected run using the cloudml-deploy  operation:  guild run census:cloudml-deploy run=$DEPLOY_RUN bucket=$BUCKET  This will create a model in Cloud ML named  census_dnn  if one\ndoesn\u2019t already exist. This name is generated using the deployed model\nname  census-dnn . Cloud ML doesn\u2019t allow certain characters in\nmodel names (e.g.  - ) and Guild replaces these with an underscore\n( _ ). If you want to specify a different model name, use the model  flag when running the  cloudml-deploy  operation.  After ensuring that a Cloud ML model exists, Guild creates a model version . This deploys the trained model to Cloud ML. By default,\nGuild uses a version containing the deployment run ID. If you want to\nspecify a different version, use the  version  flag when running the cloudml-deploy  operation.  For more information on model deployment in Cloud ML, see:   Prediction Basics - Model deployment \n  Cloud ML concepts  Deploying Models  Cloud\n  ML how-to guide   Verify the deployed model and its version by running:  gcloud ml-engine versions list --model census_dnn  You should see the newly deployed  census_dnn  model version.   Note  Guild does not provide a complete interface for managing deployed\nCloud ML models and versions. For a details on Cloud ML models and\nversions, see  gcloud ml-engine \ncommand line reference.   List your current runs:  guild runs  Note the latest run \u2014 it will be for the  cloudml-deploy  operation\nand have a label that contains the deployed model run ID. Deployments\nare like any other operation in Guild.  Let\u2019s examine the  cloudml-deploy  run:  guild runs info  Note the following attributes in the output:   cloudml_model_name  Name of the deployed Cloud ML model  cloudml_model_version  Version of the deployed Cloud ML model  cloudml_model_binaries  Google Cloud Storage location of the deployed model binaries  trained_model_run  Run ID of the deployed trained model   This information can be used to verify a deployment and is retained as\nan artifact of the deploy operation. In the next section, we\u2019ll see\nhow this information is used to run predictions.",
            "title": "Deploy a model"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#use-a-deployed-model-to-make-predictions",
            "text": "In our final tutorial segment, we\u2019ll use our recently deployed model\nto make predictions!  Cloud ML makes predictions by running the inference operation of\ndeployed model with new data. Data are provided as one or more instances , each corresponding to a prediction that will be made\nusing the trained model. You may provide instances as JSON or plain\ntext format.  For details on how Cloud ML makes predictions, see  Prediction Basics  in\nCloud ML concepts.  Cloud ML can make predictions in two ways:  online  and batch . Online predictions are made immediately and returned in the\noperation result. Batch predictions are submitted to Cloud ML as a job\nthat runs in the background, similar to training. When a batched\nprediction job is completed, predictions are written to a file in the\njob directory.  In general, online prediction is faster if you have a small number of\npredictions to make while batch processing is faster when you need to\nmake a large number of predictions. The specific performance\ncharacteristics will vary across models. Refer to  Online prediction\nversus batch prediction \nin Cloud ML Prediction Basics for more information.  We\u2019ll use both techniques to see how each works.",
            "title": "Use a deployed model to make predictions"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#online-prediction",
            "text": "First, let\u2019s run an online prediction:  guild run census:cloudml-predict  Guild will prompt you before running the operation. Press  ENTER  to\nrun the operation.  The prediction is made using the latest deployed version of the census-dnn  model ( census_dnn  in Cloud ML). The model package\nprovides sample inputs to make the prediction. You can specify your\nown using the  instances  flags.  To experiment with your own inputs, download the  census prediction\nsamples \nand make changes accordingly. Assuming your samples is named census-inputs.json  and is located in the current directory, you can\nrun a  cloudml-predict  operation using:  guild run census:cloudml-predict instances=census-inputs.json  For online prediction, the results are printed to the\nconsole. However, they are also saved as a file in the run\ndirectory. Let\u2019s take a look!  If Guild View isn\u2019t already running, start it in a separate console  by running:  guild view  In Guild View, select the latest run \u2014 it should be a cloudml-predict  operation \u2014 and click the  FILES  tab. The run\nwill have two files:   prediction.inputs  The inputs used in the prediction  prediction.results  The prediction results   Click on  prediction.results  to see how the instances were\nclassified by the deployed model. The values correspond to the inputs\nprovided in  prediction.inputs . Click the  NEXT  button to view the\ncontents of  prediction.inputs .   Tip  Guild View can be used to view the contents of some files:\ntext files, images, and audio. Look for a grey button background\non the file name, which can be clicked to open the file.   As you can see, the  cloudml-predict  operation generates a run like\nother operations. In this case the run contains information about the\nmodel version used to make the prediction along with the prediction\ninputs and outputs.",
            "title": "Online prediction"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#batch-prediction",
            "text": "In the previous section we performed an  online  prediction that\nreturned the result immediately. In this section we\u2019ll use  batch \nprediction. Batch prediction runs on Cloud ML as a job.  We use the  cloudml-batch-predict  operation to start a prediction\njob. Start a batch prediction job by running:  guild run census:cloudml-batch-predict bucket=$BUCKET  This will use the sample prediction inputs as before. We need to\nspecify  bucket  to tell Guild where to create the job.  The job will take some time to complete. When finished, it will save\ngenerated a file named  prediction.results-00000-of-00001 , which you\ncan view in Guild View, or inspect from the command line.  When the operation has finished, view the generated files by running:  guild runs info --files --full-path  The  --full-path  option is used to show the full path to prediction.results-00000-of-00001 , which you can use to copy or view\nthe file.  You may use online or batch predictions as needed to use your deployed\nmodels in Cloud ML!",
            "title": "Batch prediction"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#summary",
            "text": "In this tutorial we worked with Google\u2019s Cloud Machine Leaning Engine\n(Cloud ML) to train and deploy a classifier.  We covered a number of topics:    Train a model locally to sanity-check our results and then train in\n  Cloud ML to scale up    Use hyperparameter tuning in Cloud ML to optimize our predictive\n  accuracy with a set of hyperparameter values    Evaluate runs using Guild View, Guild Compare, and TensorBoard    Select an optimized model and deploy it to Cloud ML    Use a model deployed in Cloud ML to make predictions using new data    Guild AI models and their operations encapsulate the complexities of\nthis process, letting you focus on the high level workflow and get\nyour work done faster!",
            "title": "Summary"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#cleanup",
            "text": "Over the course of this tutorial we generated a number of runs and\nCloud ML jobs. If you no longer need these, you may delete them to\nfree up resources.",
            "title": "Cleanup"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#delete-unneeded-cloud-ml-jobs",
            "text": "Use the  gsutil  program to delete any jobs from your Cloud Storage\nbucket that you don\u2019t need. To delete all Guild related files, run:  gsutil rm -r gs://$BUCKET/guild_*",
            "title": "Delete unneeded Cloud ML jobs"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#delete-unneeded-cloud-ml-model-versions",
            "text": "Use the  gcloud  program to delete any model versions you don\u2019t need.  First, list the model versions:  gcloud ml-engine versions list --model census_dnn  For each version that you want to delete, run:  gcloud ml-engine versions delete --model census_dnn VERSION  where  VERSION  is the model version you want to delete.  If you no longer need the model, you may delete it provided there are\nno versions associated with it:  gcloud ml-engine models delete census_dnn",
            "title": "Delete unneeded Cloud ML model versions"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#delete-guild-runs",
            "text": "If you ran the tutorials from a virtual environment, you can simply\ndelete the virtual environment directory, which will free up all disk\nspace used by steps from this tutorial.   Important  Only delete virtual environments when you\u2019re certain you\nno longer need them.   If you want to only delete runs associated with the  cloudml-census \nmodel, which was used in this tutorial, run:  guild runs delete -o cloudml-census -p  The  -p  option indicates that the delete should be permanent . This ensures that the runs no longer consume disk\nspace. Omit this option if you want to retain the ability to restore\nthem at a later time. Note that disk space will not be freed up for\nthese jobs until you permanently delete them (see the  purge \ncommand).  Finally, if you deleted any runs without using the  -p  option and\nyou want to free up disk space consumed by them, you can can\npermanently delete them by running:  guild runs purge   Important  Review the list of runs carefully before permanently deleting\nthem!",
            "title": "Delete Guild runs"
        },
        {
            "location": "/docs/tutorials/train-and-predict-with-cloudml/#next-steps",
            "text": "Read  Cloud ML Engine - Getting Started \n  to view the same workflow using lower-level Cloud ML commands",
            "title": "Next steps"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/",
            "text": "Develop a model from scratch\n\n\n\n\nRequirements\n\n\nInitialize a Guild AI project\n\n\nProject Guild file\n\n\nTrain the sample model\n\n\nReview the training script\n\n\nMain function\n\n\nParse command line arguments\n\n\nInitialize the training and test data\n\n\nInitialize the model\n\n\nTrain the model\n\n\nEvaluate the model\n\n\nMain handler\n\n\n\n\n\n\nLoad the Iris data\n\n\nDownload iris_data.py\n\n\nImport iris_data\n\n\nModify init_data\n\n\nTest your changes\n\n\n\n\n\n\nDefine the model\n\n\nImplement train and evaluate\n\n\nDelete the trial runs\n\n\nTrain the DNN\n\n\nView runs\n\n\nSummary\n\n\nNext steps\n\n\n\n\nIn this tutorial, we\u2019ll develop a model with Guild AI from\nscratch. We\u2019ll follow this general process:\n\n\n\n\nCreate a training script skeleton\n\n\nImplement a model using one of the available TensorFlow model APIs\n\n\nObtain a training dataset\n\n\nIteratively train, evaluate and improve the model\n\n\n\n\nOur final goal is a trained model that predicts classes from the Iris\ndataset. We\u2019ll use the model architecture outlined in \nGetting Started\nwith TensorFlow\n.\n\n\nRequirements\n\n\nThis tutorial assumes the following:\n\n\n\n\nGuild AI is \ninstalled and verified\n\n\nYour \nvirtual environment is activated\n\n  (if applicable)\n\n\nYou have a working Internet connection\n\n\n\n\nWhile not required, we recommend using a dedicated virtual environment\nfor this tutorial. To setup your environment, see\n\nTutorial virtual environments\n.\n\n\nInitialize a Guild AI project\n\n\nGuild projects are directories that contain a \nGuild\nfile\n \u2014 i.e. a file named \nguild.yml\n. A Guild file\ncontains information used to train models.\n\n\nLet\u2019s create a new Guild project for our iris classifier by running:\n\n\nguild init --project iris\n\n\n\n\nGuild asks you to confirm the operation. Press \nENTER\n to initialize\nthe new project.\n\n\nThe commands creates a new directory:\n\n\n\n\n\n\n./iris \nProject directory\n\n \n\n \nguild.yml\nGuild file\n\n \ntrain.py\nTraining script\n\n\n\n\n\n\n\n\n\n\n\nChange to the \niris\n directory and run \nguild help\n:\n\n\ncd iris\nguild help\n\n\n\n\nThe help screen displays information in the Guild file\n\nguild.yml\n. We\u2019ll look more closely at this file next.\n\n\nPress \nq\n to stop viewing the project help.\n\n\nProject Guild file\n\n\nGuild files are named \nguild.yml\n and are located in the project\ndirectory. Guild files contain the information needed to train your\nmodel.\n\n\nBefore we turn our attention to the model training script, let\u2019s make\na quick change to \nguild.yml\n.\n\n\nIn a text editor, open \n./iris/guild.yml\n.\n\n\n- model: iris\n  description: A basic model.\n  operations:\n    train:\n      description: Train the model.\n      cmd: train\n      flags:\n        train-steps:\n          description: Number of steps to train.\n          default: 1000\n        batch-size:\n          description: Training batch size.\n          default: 64\n\n\n\n\nModify the model \ndescription\n and replace:\n\n\nA basic model.\n\n\n\nwith:\n\n\nDNN classifier for Iris data.\n\n\n\nSave \nguild.yml\n.\n\n\n\n\nNote\n\n\nLeave \nguild.yml\n open in your text editor \u2014 we\u2019ll make changes\nto it as we go along. If your text editor is a command line\napplication, open a \nseparate console\n to run commands in.\n\n\n\n\nFrom a command line in the \n./iris\n project directory, run:\n\n\nguild models\n\n\n\n\nYou should see this output:\n\n\nLimiting models to the current directory (use --all to include all)\n./iris  DNN classifier for Iris data\n\n\n\n\nThe message \nLimiting models to the current directory\n is displayed\nwhen showing information about models defined in the current\ndirectory. In this case, Guild notices \nguild.yml\n and infers you\u2019re\nworking on a project. Rather than show you all of the models available\non the system, it limits the results to models defined in the project.\n\n\nFor the remainder of this tutorial, we\u2019ll omit this message from the\nexpected output for brevity.\n\n\nNext, list the operations available:\n\n\nguild operations\n\n\n\n\nGuild displays the operations for our model:\n\n\n./iris:train  Train the model\n\n\n\n\nAll of this information is defined in \nguild.yml\n. Over the course of\nthis tutorial, we\u2019ll evolve this file to support all the features of\nour Iris classifier.\n\n\nTrain the sample model\n\n\nAt this point our model is just a sample \u2014 it doesn\u2019t train anything\nor know about Iris data. However, we can still run the \ntrain\n\noperation, which simulates the training process.\n\n\nFrom a command line in the \n./iris\n directory, run:\n\n\nguild train\n\n\n\n\nGuild prompts you with this message:\n\n\nYou are about to run ./iris:train\n  batch-size: 64\n  train-steps: 1000\nContinue? (Y/n)\n\n\n\n\nThis lets you review the \nflag values\n that will be used\nfor the \ntrain\n operation.\n\n\nAccept the values and start the training by pressing \nENTER\n.\n\n\nGuild runs the sample training operation and exits:\n\n\nSample train: step 0\nSample train: step 100\nSample train: step 200\nSample train: step 300\nSample train: step 400\nSample train: step 500\nSample train: step 600\nSample train: step 700\nSample train: step 800\nSample train: step 900\nSample evaluate: 0.888 accuracy\n\n\n\n\nWhile this is only a stub, we did execute a training run. You can list\nruns for a project by running:\n\n\nguild runs\n\n\n\n\nYou should see the completed training run.\n\n\nGuild runs automatically generated when you run \ntrain\n. Each run\ncontains a record of the operation, which includes information about\nthe operation along with files generated during the run.\n\n\nYou can show information about the latest run by running:\n\n\nguild runs info\n\n\n\n\nThis shows run details including:\n\n\n\n\nRun ID\n\n\nA unique identifier for the run\n\n\nRun operation\n\n\nThe model operation that generated the run\n\n\nStart and stop time\n\n\nWhen the run was started and stopped\n\n\nRun directory\n\n\nThe file system directory containing run files\n\n\nCommand details\n\n\nThe command used to run the operation, its exit status (if it\n  exited) and its operation system PID (if it\u2019s still running)\n\n\n\n\nWe\u2019ll revisit runs later in this tutorial.\n\n\nNext, we\u2019ll review our project\u2019s training script.\n\n\nReview the training script\n\n\nIn your text editor, open \n./iris/train.py\n, which is the sample\ntraining script that Guild created when you initialized the project.\n\n\nHere\u2019s the script with imports and comments removed for brevity. We\u2019ll\nlook at each part in turn.\n\n\ndef main():\n    args = parse_args()\n    data = init_data(args)\n    model = init_model(data, args)\n    train(model, data, args)\n    evaluate(model, data, args)\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--train-steps', default=1000, type=int)\n    parser.add_argument('--batch-size', default=100, type=int)\n    parser.add_argument('--data-dir', default='data')\n    parser.add_argument('--model-dir', default='model')\n    return parser.parse_args()\n\ndef init_data(args):\n    train_x, train_y = [], []\n    test_x, test_y = [], []\n    return (train_x, train_y), (test_x, test_y)\n\ndef init_model(data, args):\n    return 'sample model'\n\ndef train(model, data, args):\n    for step in range(args.train_steps):\n        if step % 100 == 0:\n            print('Sample train: step %s' % step)\n\ndef evaluate(model, data, args):\n    print('Sample evaluate: 0.888 accuracy')\n\nif __name__ == '__main__':\n    main()\n\n\n\n\nMain function\n\n\ndef main():\n    args = parse_args()\n    data = init_data(args)\n    model = init_model(data, args)\n    train(model, data, args)\n    evaluate(model, data, args)\n\n\n\n\nThis function orchestrates the training operation:\n\n\n\n\nParse command line arguments\n\n\nInitialize data used for training and evaluation\n\n\nInitialize the model\n\n\nTrain the model\n\n\nEvaluate the model\n\n\n\n\nWe won\u2019t modify this function because it already does what we need.\n\n\nParse command line arguments\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--train-steps', default=1000, type=int)\n    parser.add_argument('--batch-size', default=100, type=int)\n    parser.add_argument('--data-dir', default='data')\n    parser.add_argument('--model-dir', default='model')\n    return parser.parse_args()\n\n\n\n\nThis function parses command line arguments passed to the script.\n\n\nRather than hard-code hyperparameter values, consider using command\nline arguments. This pattern provides a couple of benefits:\n\n\n\n\n\n\nUsers can experiment with different hyperparameter values without\n  modifying source code.\n\n\n\n\n\n\nBy defining hyperparameters as command line arguments, you document\n  values that can be changed without compromising the model\n  architecture.\n\n\n\n\n\n\nInitialize the training and test data\n\n\ndef init_data(args):\n    train_x, train_y = [], []\n    test_x, test_y = [], []\n    return (train_x, train_y), (test_x, test_y)\n\n\n\n\nThis function is a stub for loading and initializing data. We\u2019ll\nmodify this function later in this tutorial to load and prepare our\nIris data for training and validation.\n\n\nInitialize the model\n\n\ndef init_model(data, args):\n    return 'sample model'\n\n\n\n\nThis function is a stub for initializing the model. We\u2019ll modify it\nlater in this tutorial to return a \ntf.estimator.DNNClassifier\n that\ncan be used to train with the Iris data.\n\n\nTrain the model\n\n\ndef train(model, data, args):\n    for step in range(args.train_steps):\n        if step % 100 == 0:\n            print('Sample train: step %s' % step)\n\n\n\n\nThis function is a stub for the training loop. We\u2019ll modify it later\nin this tutorial to call the \ntrain\n method of our model.\n\n\nEvaluate the model\n\n\ndef evaluate(model, data, args):\n    print('Sample evaluate: 0.888 accuracy')\n\n\n\n\nOur final function is a stub for evaluating the trained model. We\u2019ll\nmodify it later in this tutorial to call the \nevaluate\n method of our\nmodel.\n\n\nMain handler\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\nThis statement calls \nmain\n only when then module is loaded as a\nscript.\n\n\nThis pattern serves two purposes:\n\n\n\n\nThe script can be executed from a command line\n\n\nThe script can also be imported by other Python modules without\n  automatically running\n\n\n\n\nIt\u2019s a good idea to use this pattern because it allows your training\nscript to be used as a module by other Python programs.\n\n\nLoad the Iris data\n\n\nLet\u2019s start modifying our training script to work with Iris data.\n\n\nFor our project, we\u2019ll use the Iris data prepared by Google for\n\nGetting Started with TensorFlow\n.\n\n\nDownload \niris_data.py\n\n\nDownload \niris_data.py\n and save it in your \n./iris\n project\ndirectory.\n\n\nOnce you\u2019ve saved the file, your project directory should look like this:\n\n\n\n\n\n\n./iris\n \n\n \nguild.yml\n\n \niris_data.py \nNewly downloaded file\n\n \ntrain.py\n\n\n\n\n\n\n\n\n\n\n\nImport \niris_data\n\n\nIn your text editor, modify \n./iris/train.py\n by adding the following\nline to the imports section of the file, after the line \nimport\ntensorflow as tf\n:\n\n\nimport iris_data\n\n\n\n\nYour imports should look like this:\n\n\nimport argparse\n\nimport tensorflow as tf\n\nimport iris_data   # newly added line\n\n\n\n\nModify \ninit_data\n\n\nNext, we\u2019ll modify the \ninit_data\n function to load the Iris data.\n\n\nIn you text editor, modify \n./iris/train.py\n to change the \ninit_data\n\nfunction to be this:\n\n\ndef init_data(_args):\n    return iris_data.load_data()\n\n\n\n\nNote our changes:\n\n\n\n\n\n\nWe\u2019re passing through the call to \ninit_data\n to our data module\n  without making additional changes to the data. Refer to\n  \niris_data.py\n for details on how the data is loaded.\n\n\n\n\n\n\nWe\u2019re not using the \nargs\n argument, so we rename it to\n  \n_args\n. Feel free to remove \nargs\n altogether, but if you do, you\n  must also update the \nmain\n function to use \ninit_data()\n.\n\n\n\n\n\n\nTest your changes\n\n\nIn your text editor, save your changes.\n\n\nOur script should now load the Iris data but continue the simulated\ntraining and evaluation without using the data. If we have problems\nhere we can fix them before continuing to the next steps.\n\n\nFrom a command line in the \n./iris\n directory, run:\n\n\nguild train -y\n\n\n\n\n\n\nNote\n\n\nWe use the \n-y\n option here to bypass the prompt, saving us\na step.\n\n\n\n\nBelow are some issues you might experience at this point.\n\n\n\n\nImportError: No module named iris_data\n\n\nVerify that you downloaded \niris_data.py\n into the project\n  directory. See \nDownload iris_data.py\n above\n  for help.\n\n\nNameError: global name 'iris_data' is not defined\n\n\nEnsure that you are importing the \niris_data\n module. See \nImport\n  iris_data \n above for help.\n\n\nAn error originating from \niris_data.py\n\n\nVerify that you \niris_data.py\n was saved correctly.\n\n\n\n\nIf the command succeeds, you should see the same result as from our\nprevious run.\n\n\nYou can list the runs by running:\n\n\nguild runs\n\n\n\n\nNext, we\u2019ll define our model.\n\n\nDefine the model\n\n\nIn this step, we\u2019ll implement a simple deep neural network as\ndescribed in \nGetting Started with TensorFlow - Instantiate an\nestimator\n.\n\n\nIn your text editor, modify \n./iris/train.py\n and change the\n\ninit_model\n function to be this:\n\n\ndef init_model(data, args):\n    train_x = data[0][0]\n    feature_columns = [\n        tf.feature_column.numeric_column(key=key)\n        for key in train_x.keys()\n    ]\n    return tf.estimator.DNNClassifier(\n        feature_columns=feature_columns,\n        hidden_units=[10, 10],\n        n_classes=3,\n        model_dir=args.model_dir)\n\n\n\n\nThis is the most complex change to our script, so let\u2019s take a moment\nto understand it.\n\n\nThe purpose of \ninit_model\n is to return something that can be trained\nand evaluated. In this case, we\u2019re using the \nTensorFlow Estimators\n\nhigh-level framework to create a DNN classifier.\n\n\nThe classifier requires a list of feature columns, which we provide\nfrom the training data.\n\n\nWe also specify \nmodel_dir\n, which tells TensorFlow to generate logs\nand save the trained model during training. We\u2019ll see this in action\nlater.\n\n\nIn your text editor, save your changes.\n\n\nFeel free to stop now and test your changes by running:\n\n\nguild train -y\n\n\n\n\nWhile we\u2019re still not yet training our model, you will see additional\noutput from the command, which is generated by TensorFlow when we\ninstantiate the model.\n\n\nImplement train and evaluate\n\n\nIn this step, we\u2019ll modify our \ntrain\n and \nevaluate\n functions to\nperform real work!\n\n\nIn your text editor, modify \n./iris/train.py\n and change the \ntrain\n\nfunction to be:\n\n\ndef train(model, data, args):\n    (train_x, train_y), _ = data\n    input_fn = lambda: iris_data.train_input_fn(\n        train_x, train_y, args.batch_size)\n    model.train(input_fn=input_fn, steps=args.train_steps)\n\n\n\n\n\nNext, change the \nevaluate\n function to be:\n\n\ndef evaluate(model, data, args):\n    _, (test_x, test_y) = data\n    input_fn = lambda: iris_data.eval_input_fn(\n        test_x, test_y, args.batch_size)\n    eval_result = model.evaluate(input_fn=input_fn)\n    print('Test set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n\n\n\n\nBoth functions are similar:\n\n\n\n\n\n\nThey both call a single method on \nmodel\n to perform the work.\n\n\n\n\n\n\nThe both use a callback \ninput_fn\n to read batched data.\n\n\n\n\n\n\nNote that we\u2019re using \nargs.batch_size\n rather than hard-coding a\nvalue. This makes it easy to experiment with different values without\nchanging the source code.\n\n\nIn your text editor, save your changes.\n\n\nAt this point our training script is ready to train the Iris data!\n\n\n\n\nNote\n\n\nIf you\u2019d like, feel free to delete any remaining \nTODO\n\ncomments.\n\n\n\n\nDelete the trial runs\n\n\nBefore we proceed, let\u2019s delete all of our runs up to this point,\nwhich we won\u2019t use because they were just trials. From a command line,\nrun:\n\n\nguild runs delete\n\n\n\n\nGuild will ask you to confirm the operation before proceeding. Press\n\nENTER\n to delete the runs.\n\n\n\n\nNote\n\n\nGuild lets you restore deleted runs using the \nruns\nrestore\n command. This can come in handy if you\naccidentally delete something you need!\n\n\n\n\nTrain the DNN\n\n\n\n\nNote\n\n\nThe full source code to the project at this point can be found here:\n\n\n\n\nhttps://github.com/guildai/examples/tree/master/iris\n\n\n\n\n\n\nTrain our new DNN by running:\n\n\nguild train -y\n\n\n\n\nYou should see real training progress this time, with updates from\nTensorFlow about training loss and other training statistics.\n\n\nWhen the training is completed, the script prints a final accuracy,\nwhich is calculated using test data. It should be 0.967.\n\n\nCongratulations - you\u2019ve just trained a classifier for Iris data!\n\n\nView runs\n\n\nFrom a command line, run:\n\n\nguild view\n\n\n\n\nThis command opens Guild View in a browser. Use Guild View to explore\nyour run(s). Click \n in the upper-right\nof the page to view your runs in TensorBoard.\n\n\nClick the \nFILES\n tab to view the files generated by the\noperation. These include TensorFlow event logs, which are used by\nTensorBoard, and the saved trained model.\n\n\nYou can list the files associated with the latest run by running:\n\n\nguild runs info --files\n\n\n\n\nTo see the full path to each file \u2014 for example, to reference the\nfile in another command \u2014 use the \n-P\n option:\n\n\nguild runs info --files -P\n\n\n\n\nSummary\n\n\nIn this tutorial, we developed a new Guild AI project from scratch,\ncontaining the DNN Iris data classifier defined in \nGetting Started\nwith TensorFlow\n.\n\n\nIn this tutorial, we saw the following:\n\n\n\n\n\n\nGuild files (\nguild.yml\n) define models that can be trained.\n\n\n\n\n\n\nModel operations are wrappers for commands to Python modules.\n\n\n\n\n\n\nOperation flags are passed through to training scripts as command\n  line arguments.\n\n\n\n\n\n\nGuild automatically tracks each run.\n\n\n\n\n\n\nGuild provides tools to help you evaluate runs, including\n  integration with TensorBoard.\n\n\n\n\n\n\nNext steps\n\n\n\n\nBrowse Guild AI documentation",
            "title": "Develop a model from scratch"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#develop-a-model-from-scratch",
            "text": "Requirements  Initialize a Guild AI project  Project Guild file  Train the sample model  Review the training script  Main function  Parse command line arguments  Initialize the training and test data  Initialize the model  Train the model  Evaluate the model  Main handler    Load the Iris data  Download iris_data.py  Import iris_data  Modify init_data  Test your changes    Define the model  Implement train and evaluate  Delete the trial runs  Train the DNN  View runs  Summary  Next steps   In this tutorial, we\u2019ll develop a model with Guild AI from\nscratch. We\u2019ll follow this general process:   Create a training script skeleton  Implement a model using one of the available TensorFlow model APIs  Obtain a training dataset  Iteratively train, evaluate and improve the model   Our final goal is a trained model that predicts classes from the Iris\ndataset. We\u2019ll use the model architecture outlined in  Getting Started\nwith TensorFlow .",
            "title": "Develop a model from scratch"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#requirements",
            "text": "This tutorial assumes the following:   Guild AI is  installed and verified  Your  virtual environment is activated \n  (if applicable)  You have a working Internet connection   While not required, we recommend using a dedicated virtual environment\nfor this tutorial. To setup your environment, see Tutorial virtual environments .",
            "title": "Requirements"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#initialize-a-guild-ai-project",
            "text": "Guild projects are directories that contain a  Guild\nfile  \u2014 i.e. a file named  guild.yml . A Guild file\ncontains information used to train models.  Let\u2019s create a new Guild project for our iris classifier by running:  guild init --project iris  Guild asks you to confirm the operation. Press  ENTER  to initialize\nthe new project.  The commands creates a new directory:    ./iris  Project directory \n  \n  guild.yml Guild file \n  train.py Training script      Change to the  iris  directory and run  guild help :  cd iris\nguild help  The help screen displays information in the Guild file guild.yml . We\u2019ll look more closely at this file next.  Press  q  to stop viewing the project help.",
            "title": "Initialize a Guild AI project"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#project-guild-file",
            "text": "Guild files are named  guild.yml  and are located in the project\ndirectory. Guild files contain the information needed to train your\nmodel.  Before we turn our attention to the model training script, let\u2019s make\na quick change to  guild.yml .  In a text editor, open  ./iris/guild.yml .  - model: iris\n  description: A basic model.\n  operations:\n    train:\n      description: Train the model.\n      cmd: train\n      flags:\n        train-steps:\n          description: Number of steps to train.\n          default: 1000\n        batch-size:\n          description: Training batch size.\n          default: 64  Modify the model  description  and replace:  A basic model.  with:  DNN classifier for Iris data.  Save  guild.yml .   Note  Leave  guild.yml  open in your text editor \u2014 we\u2019ll make changes\nto it as we go along. If your text editor is a command line\napplication, open a  separate console  to run commands in.   From a command line in the  ./iris  project directory, run:  guild models  You should see this output:  Limiting models to the current directory (use --all to include all)\n./iris  DNN classifier for Iris data  The message  Limiting models to the current directory  is displayed\nwhen showing information about models defined in the current\ndirectory. In this case, Guild notices  guild.yml  and infers you\u2019re\nworking on a project. Rather than show you all of the models available\non the system, it limits the results to models defined in the project.  For the remainder of this tutorial, we\u2019ll omit this message from the\nexpected output for brevity.  Next, list the operations available:  guild operations  Guild displays the operations for our model:  ./iris:train  Train the model  All of this information is defined in  guild.yml . Over the course of\nthis tutorial, we\u2019ll evolve this file to support all the features of\nour Iris classifier.",
            "title": "Project Guild file"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#train-the-sample-model",
            "text": "At this point our model is just a sample \u2014 it doesn\u2019t train anything\nor know about Iris data. However, we can still run the  train \noperation, which simulates the training process.  From a command line in the  ./iris  directory, run:  guild train  Guild prompts you with this message:  You are about to run ./iris:train\n  batch-size: 64\n  train-steps: 1000\nContinue? (Y/n)  This lets you review the  flag values  that will be used\nfor the  train  operation.  Accept the values and start the training by pressing  ENTER .  Guild runs the sample training operation and exits:  Sample train: step 0\nSample train: step 100\nSample train: step 200\nSample train: step 300\nSample train: step 400\nSample train: step 500\nSample train: step 600\nSample train: step 700\nSample train: step 800\nSample train: step 900\nSample evaluate: 0.888 accuracy  While this is only a stub, we did execute a training run. You can list\nruns for a project by running:  guild runs  You should see the completed training run.  Guild runs automatically generated when you run  train . Each run\ncontains a record of the operation, which includes information about\nthe operation along with files generated during the run.  You can show information about the latest run by running:  guild runs info  This shows run details including:   Run ID  A unique identifier for the run  Run operation  The model operation that generated the run  Start and stop time  When the run was started and stopped  Run directory  The file system directory containing run files  Command details  The command used to run the operation, its exit status (if it\n  exited) and its operation system PID (if it\u2019s still running)   We\u2019ll revisit runs later in this tutorial.  Next, we\u2019ll review our project\u2019s training script.",
            "title": "Train the sample model"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#review-the-training-script",
            "text": "In your text editor, open  ./iris/train.py , which is the sample\ntraining script that Guild created when you initialized the project.  Here\u2019s the script with imports and comments removed for brevity. We\u2019ll\nlook at each part in turn.  def main():\n    args = parse_args()\n    data = init_data(args)\n    model = init_model(data, args)\n    train(model, data, args)\n    evaluate(model, data, args)\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--train-steps', default=1000, type=int)\n    parser.add_argument('--batch-size', default=100, type=int)\n    parser.add_argument('--data-dir', default='data')\n    parser.add_argument('--model-dir', default='model')\n    return parser.parse_args()\n\ndef init_data(args):\n    train_x, train_y = [], []\n    test_x, test_y = [], []\n    return (train_x, train_y), (test_x, test_y)\n\ndef init_model(data, args):\n    return 'sample model'\n\ndef train(model, data, args):\n    for step in range(args.train_steps):\n        if step % 100 == 0:\n            print('Sample train: step %s' % step)\n\ndef evaluate(model, data, args):\n    print('Sample evaluate: 0.888 accuracy')\n\nif __name__ == '__main__':\n    main()",
            "title": "Review the training script"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#main-function",
            "text": "def main():\n    args = parse_args()\n    data = init_data(args)\n    model = init_model(data, args)\n    train(model, data, args)\n    evaluate(model, data, args)  This function orchestrates the training operation:   Parse command line arguments  Initialize data used for training and evaluation  Initialize the model  Train the model  Evaluate the model   We won\u2019t modify this function because it already does what we need.",
            "title": "Main function"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#parse-command-line-arguments",
            "text": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--train-steps', default=1000, type=int)\n    parser.add_argument('--batch-size', default=100, type=int)\n    parser.add_argument('--data-dir', default='data')\n    parser.add_argument('--model-dir', default='model')\n    return parser.parse_args()  This function parses command line arguments passed to the script.  Rather than hard-code hyperparameter values, consider using command\nline arguments. This pattern provides a couple of benefits:    Users can experiment with different hyperparameter values without\n  modifying source code.    By defining hyperparameters as command line arguments, you document\n  values that can be changed without compromising the model\n  architecture.",
            "title": "Parse command line arguments"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#initialize-the-training-and-test-data",
            "text": "def init_data(args):\n    train_x, train_y = [], []\n    test_x, test_y = [], []\n    return (train_x, train_y), (test_x, test_y)  This function is a stub for loading and initializing data. We\u2019ll\nmodify this function later in this tutorial to load and prepare our\nIris data for training and validation.",
            "title": "Initialize the training and test data"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#initialize-the-model",
            "text": "def init_model(data, args):\n    return 'sample model'  This function is a stub for initializing the model. We\u2019ll modify it\nlater in this tutorial to return a  tf.estimator.DNNClassifier  that\ncan be used to train with the Iris data.",
            "title": "Initialize the model"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#train-the-model",
            "text": "def train(model, data, args):\n    for step in range(args.train_steps):\n        if step % 100 == 0:\n            print('Sample train: step %s' % step)  This function is a stub for the training loop. We\u2019ll modify it later\nin this tutorial to call the  train  method of our model.",
            "title": "Train the model"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#evaluate-the-model",
            "text": "def evaluate(model, data, args):\n    print('Sample evaluate: 0.888 accuracy')  Our final function is a stub for evaluating the trained model. We\u2019ll\nmodify it later in this tutorial to call the  evaluate  method of our\nmodel.",
            "title": "Evaluate the model"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#main-handler",
            "text": "if __name__ == \"__main__\":\n    main()  This statement calls  main  only when then module is loaded as a\nscript.  This pattern serves two purposes:   The script can be executed from a command line  The script can also be imported by other Python modules without\n  automatically running   It\u2019s a good idea to use this pattern because it allows your training\nscript to be used as a module by other Python programs.",
            "title": "Main handler"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#load-the-iris-data",
            "text": "Let\u2019s start modifying our training script to work with Iris data.  For our project, we\u2019ll use the Iris data prepared by Google for Getting Started with TensorFlow .",
            "title": "Load the Iris data"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#download-iris_datapy",
            "text": "Download  iris_data.py  and save it in your  ./iris  project\ndirectory.  Once you\u2019ve saved the file, your project directory should look like this:    ./iris\n  \n  guild.yml \n  iris_data.py  Newly downloaded file \n  train.py",
            "title": "Download iris_data.py"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#import-iris_data",
            "text": "In your text editor, modify  ./iris/train.py  by adding the following\nline to the imports section of the file, after the line  import\ntensorflow as tf :  import iris_data  Your imports should look like this:  import argparse\n\nimport tensorflow as tf\n\nimport iris_data   # newly added line",
            "title": "Import iris_data"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#modify-init_data",
            "text": "Next, we\u2019ll modify the  init_data  function to load the Iris data.  In you text editor, modify  ./iris/train.py  to change the  init_data \nfunction to be this:  def init_data(_args):\n    return iris_data.load_data()  Note our changes:    We\u2019re passing through the call to  init_data  to our data module\n  without making additional changes to the data. Refer to\n   iris_data.py  for details on how the data is loaded.    We\u2019re not using the  args  argument, so we rename it to\n   _args . Feel free to remove  args  altogether, but if you do, you\n  must also update the  main  function to use  init_data() .",
            "title": "Modify init_data"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#test-your-changes",
            "text": "In your text editor, save your changes.  Our script should now load the Iris data but continue the simulated\ntraining and evaluation without using the data. If we have problems\nhere we can fix them before continuing to the next steps.  From a command line in the  ./iris  directory, run:  guild train -y   Note  We use the  -y  option here to bypass the prompt, saving us\na step.   Below are some issues you might experience at this point.   ImportError: No module named iris_data  Verify that you downloaded  iris_data.py  into the project\n  directory. See  Download iris_data.py  above\n  for help.  NameError: global name 'iris_data' is not defined  Ensure that you are importing the  iris_data  module. See  Import\n  iris_data   above for help.  An error originating from  iris_data.py  Verify that you  iris_data.py  was saved correctly.   If the command succeeds, you should see the same result as from our\nprevious run.  You can list the runs by running:  guild runs  Next, we\u2019ll define our model.",
            "title": "Test your changes"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#define-the-model",
            "text": "In this step, we\u2019ll implement a simple deep neural network as\ndescribed in  Getting Started with TensorFlow - Instantiate an\nestimator .  In your text editor, modify  ./iris/train.py  and change the init_model  function to be this:  def init_model(data, args):\n    train_x = data[0][0]\n    feature_columns = [\n        tf.feature_column.numeric_column(key=key)\n        for key in train_x.keys()\n    ]\n    return tf.estimator.DNNClassifier(\n        feature_columns=feature_columns,\n        hidden_units=[10, 10],\n        n_classes=3,\n        model_dir=args.model_dir)  This is the most complex change to our script, so let\u2019s take a moment\nto understand it.  The purpose of  init_model  is to return something that can be trained\nand evaluated. In this case, we\u2019re using the  TensorFlow Estimators \nhigh-level framework to create a DNN classifier.  The classifier requires a list of feature columns, which we provide\nfrom the training data.  We also specify  model_dir , which tells TensorFlow to generate logs\nand save the trained model during training. We\u2019ll see this in action\nlater.  In your text editor, save your changes.  Feel free to stop now and test your changes by running:  guild train -y  While we\u2019re still not yet training our model, you will see additional\noutput from the command, which is generated by TensorFlow when we\ninstantiate the model.",
            "title": "Define the model"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#implement-train-and-evaluate",
            "text": "In this step, we\u2019ll modify our  train  and  evaluate  functions to\nperform real work!  In your text editor, modify  ./iris/train.py  and change the  train \nfunction to be:  def train(model, data, args):\n    (train_x, train_y), _ = data\n    input_fn = lambda: iris_data.train_input_fn(\n        train_x, train_y, args.batch_size)\n    model.train(input_fn=input_fn, steps=args.train_steps)  Next, change the  evaluate  function to be:  def evaluate(model, data, args):\n    _, (test_x, test_y) = data\n    input_fn = lambda: iris_data.eval_input_fn(\n        test_x, test_y, args.batch_size)\n    eval_result = model.evaluate(input_fn=input_fn)\n    print('Test set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))  Both functions are similar:    They both call a single method on  model  to perform the work.    The both use a callback  input_fn  to read batched data.    Note that we\u2019re using  args.batch_size  rather than hard-coding a\nvalue. This makes it easy to experiment with different values without\nchanging the source code.  In your text editor, save your changes.  At this point our training script is ready to train the Iris data!   Note  If you\u2019d like, feel free to delete any remaining  TODO \ncomments.",
            "title": "Implement train and evaluate"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#delete-the-trial-runs",
            "text": "Before we proceed, let\u2019s delete all of our runs up to this point,\nwhich we won\u2019t use because they were just trials. From a command line,\nrun:  guild runs delete  Guild will ask you to confirm the operation before proceeding. Press ENTER  to delete the runs.   Note  Guild lets you restore deleted runs using the  runs\nrestore  command. This can come in handy if you\naccidentally delete something you need!",
            "title": "Delete the trial runs"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#train-the-dnn",
            "text": "Note  The full source code to the project at this point can be found here:   https://github.com/guildai/examples/tree/master/iris    Train our new DNN by running:  guild train -y  You should see real training progress this time, with updates from\nTensorFlow about training loss and other training statistics.  When the training is completed, the script prints a final accuracy,\nwhich is calculated using test data. It should be 0.967.  Congratulations - you\u2019ve just trained a classifier for Iris data!",
            "title": "Train the DNN"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#view-runs",
            "text": "From a command line, run:  guild view  This command opens Guild View in a browser. Use Guild View to explore\nyour run(s). Click   in the upper-right\nof the page to view your runs in TensorBoard.  Click the  FILES  tab to view the files generated by the\noperation. These include TensorFlow event logs, which are used by\nTensorBoard, and the saved trained model.  You can list the files associated with the latest run by running:  guild runs info --files  To see the full path to each file \u2014 for example, to reference the\nfile in another command \u2014 use the  -P  option:  guild runs info --files -P",
            "title": "View runs"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#summary",
            "text": "In this tutorial, we developed a new Guild AI project from scratch,\ncontaining the DNN Iris data classifier defined in  Getting Started\nwith TensorFlow .  In this tutorial, we saw the following:    Guild files ( guild.yml ) define models that can be trained.    Model operations are wrappers for commands to Python modules.    Operation flags are passed through to training scripts as command\n  line arguments.    Guild automatically tracks each run.    Guild provides tools to help you evaluate runs, including\n  integration with TensorBoard.",
            "title": "Summary"
        },
        {
            "location": "/docs/tutorials/develop-a-model-from-scratch/#next-steps",
            "text": "Browse Guild AI documentation",
            "title": "Next steps"
        },
        {
            "location": "/docs/tutorials/workflow-with-virtualenv/",
            "text": "Workflow with Virtualenv\n\n\n\n\nWhen to use a virtual environment\n\n\nInstalling Virtualenv\n\n\nCreating a virtual environment\n\n\nVirtual environments and GUILD_HOME\n\n\nUsing a virtual environment\n\n\nSummary\n\n\n\n\nVirtualenv\n is a tool that creates\nisolated environments for running Python applications. A virtual\nenvironment is contained in a single file system directory and does\nnot rely on Python packages installed at the system level.\n\n\nYou are free to create as many virtual environments as you need. Each\nenvironment is independent of the others and may use different\nsoftware versions, including different versions of Python.\n\n\nGuild AI is designed to work with or without a virtual environment \u2014\nthe choice is yours.\n\n\nWhen to use a virtual environment\n\n\nThere are a number of reasons to use virtual environments:\n\n\n\n\nIsolate Guild AI and is library dependencies\n\n\nManage different versions of Guild AI\n\n\nManage versions of Python and TensorFlow used by Guild\n\n\nMaintain separate model and run repositories\n\n\n\n\nThere are some downsides to using virtual environments:\n\n\n\n\nEach environment requires its own set of installed packages, which\n  may be excessively time consuming for short-lived environments\n\n\nYou must remember to activate the environment to use it\n\n\nYou may accidentally use the wrong environment\n\n\n\n\nIf you don\u2019t need to isolate Guild and its dependencies or to manage\ndifferent software versions, the added complexity of Virtualenv may be\nmore hindrance than help.\n\n\nInstalling Virtualenv\n\n\nRefer to \nhttps://virtualenv.pypa.io/installation/\n for details on\ninstalling Virtualenv.\n\n\nGenerally speaking though, you simply run:\n\n\npip install virtualenv\n\n\n\n\nIf you need to install as a privileged user, run:\n\n\nsudo pip install virtualenv\n\n\n\n\nCreating a virtual environment\n\n\nOnce Virtualenv is installed, it\u2019s easy to create new\nenvironments. Simply run:\n\n\nvirtualenv DIRECTORY\n\n\n\n\nwhere \nDIRECTORY\n is the file system directory that will contain the\nenvironment.\n\n\nYou can specify the Python version to use in environment using the\n\n-p\n option. For example, to indicate that Python 2 should be used,\nrun:\n\n\nvirtualenv -p python2 DIRECTORY\n\n\n\n\nUsers who make extensive use of virtual environments typically create\nthem in a common parent directory. Here\u2019s a theoretical directory\nstructure that could be used to manage different configurations of\nGuild:\n\n\n\n\n\n\nEnvironments \nDirectory containing the virtual environments\n\n \n\n \nguild-python2\nGuild installed with Python 2\n\n \nguild-python3\nGuild installed with Python 3\n\n \nguild-tensorflow-gpu\nGuild with the GPU version of TensorFlow\n\n \ngans\nEnvironment for work on GAN models\n\n\n\n\n\n\n\n\n\n\n\nVirtual environments and \nGUILD_HOME\n\n\nGuild manages \npackages\n, \nruns\n, and\n\nindexes\n under a single directory named\n\nGUILD_HOME\n. The default value of \nGUILD_HOME\n depends how Guild is\nrun:\n\n\n\n\n\n\nIf Guild is run within a virtual environment, the value is\n  \n$VIRTUAL_ENV/.guild\n\n\n\n\n\n\nIf Guild is not run within a virtual environment, the value is\n  \n$HOME/.guild\n\n\n\n\n\n\nThis behavior is designed to completely isolate Guild within a virtual\nenvironment. However, you may want to share packages and runs across\nenvironments. You can do this by explicitly setting the \nGUILD_HOME\n\nenvironment variable to the shared location. This should be done in\nthe \nactivate\n script of the environment.\n\n\nTo share a common \nGUILD_HOME\n across environments, modify\n\n$VIRTUAL_ENV/bin/active\n for each environment and add the following\nline at the end of the file:\n\n\nexport GUILD_HOME=~/.guild\n\n\n\n\nIf you want to use a different location, change \n~/.guild\n\naccordingly.\n\n\nUsing a virtual environment\n\n\nYou must activate a virtual environment to use it. This is done by\n\nsourcing\n the script as follows:\n\n\nsource $VIRTUAL_ENV/bin/activate\n\n\n\n\nThis will configure your shell environment to use the Python runtime\nand associated packages contained in the environment. It will also set\n\nGUILD_HOME\n if you modified \nactivate\n as per the instructions above.\n\n\nFor more information, refer to \nVirtualenv User Guide: activate\nscript\n.\n\n\nOnce activated, run Guild commands as you normally would.\n\n\nYou can verify Guild details by running \nguild check\n, which will\ndisplay \nguild_home\n and \nguild_install_location\n settings, letting\nyou confirm that Guild is isolated as expected.\n\n\nSummary\n\n\nIn this tutorial we reviewed Virtualenv and how it can be used to\nisolate Guild on a system. We described how \nGUILD_HOME\n can be shared\nacross virtual environments by explicitly setting an environment\nvariable in the \nactivate\n script.",
            "title": "Workflow with Virtualenv"
        },
        {
            "location": "/docs/tutorials/workflow-with-virtualenv/#workflow-with-virtualenv",
            "text": "When to use a virtual environment  Installing Virtualenv  Creating a virtual environment  Virtual environments and GUILD_HOME  Using a virtual environment  Summary   Virtualenv  is a tool that creates\nisolated environments for running Python applications. A virtual\nenvironment is contained in a single file system directory and does\nnot rely on Python packages installed at the system level.  You are free to create as many virtual environments as you need. Each\nenvironment is independent of the others and may use different\nsoftware versions, including different versions of Python.  Guild AI is designed to work with or without a virtual environment \u2014\nthe choice is yours.",
            "title": "Workflow with Virtualenv"
        },
        {
            "location": "/docs/tutorials/workflow-with-virtualenv/#when-to-use-a-virtual-environment",
            "text": "There are a number of reasons to use virtual environments:   Isolate Guild AI and is library dependencies  Manage different versions of Guild AI  Manage versions of Python and TensorFlow used by Guild  Maintain separate model and run repositories   There are some downsides to using virtual environments:   Each environment requires its own set of installed packages, which\n  may be excessively time consuming for short-lived environments  You must remember to activate the environment to use it  You may accidentally use the wrong environment   If you don\u2019t need to isolate Guild and its dependencies or to manage\ndifferent software versions, the added complexity of Virtualenv may be\nmore hindrance than help.",
            "title": "When to use a virtual environment"
        },
        {
            "location": "/docs/tutorials/workflow-with-virtualenv/#installing-virtualenv",
            "text": "Refer to  https://virtualenv.pypa.io/installation/  for details on\ninstalling Virtualenv.  Generally speaking though, you simply run:  pip install virtualenv  If you need to install as a privileged user, run:  sudo pip install virtualenv",
            "title": "Installing Virtualenv"
        },
        {
            "location": "/docs/tutorials/workflow-with-virtualenv/#creating-a-virtual-environment",
            "text": "Once Virtualenv is installed, it\u2019s easy to create new\nenvironments. Simply run:  virtualenv DIRECTORY  where  DIRECTORY  is the file system directory that will contain the\nenvironment.  You can specify the Python version to use in environment using the -p  option. For example, to indicate that Python 2 should be used,\nrun:  virtualenv -p python2 DIRECTORY  Users who make extensive use of virtual environments typically create\nthem in a common parent directory. Here\u2019s a theoretical directory\nstructure that could be used to manage different configurations of\nGuild:    Environments  Directory containing the virtual environments \n  \n  guild-python2 Guild installed with Python 2 \n  guild-python3 Guild installed with Python 3 \n  guild-tensorflow-gpu Guild with the GPU version of TensorFlow \n  gans Environment for work on GAN models",
            "title": "Creating a virtual environment"
        },
        {
            "location": "/docs/tutorials/workflow-with-virtualenv/#virtual-environments-and-guild_home",
            "text": "Guild manages  packages ,  runs , and indexes  under a single directory named GUILD_HOME . The default value of  GUILD_HOME  depends how Guild is\nrun:    If Guild is run within a virtual environment, the value is\n   $VIRTUAL_ENV/.guild    If Guild is not run within a virtual environment, the value is\n   $HOME/.guild    This behavior is designed to completely isolate Guild within a virtual\nenvironment. However, you may want to share packages and runs across\nenvironments. You can do this by explicitly setting the  GUILD_HOME \nenvironment variable to the shared location. This should be done in\nthe  activate  script of the environment.  To share a common  GUILD_HOME  across environments, modify $VIRTUAL_ENV/bin/active  for each environment and add the following\nline at the end of the file:  export GUILD_HOME=~/.guild  If you want to use a different location, change  ~/.guild \naccordingly.",
            "title": "Virtual environments and GUILD_HOME"
        },
        {
            "location": "/docs/tutorials/workflow-with-virtualenv/#using-a-virtual-environment",
            "text": "You must activate a virtual environment to use it. This is done by sourcing  the script as follows:  source $VIRTUAL_ENV/bin/activate  This will configure your shell environment to use the Python runtime\nand associated packages contained in the environment. It will also set GUILD_HOME  if you modified  activate  as per the instructions above.  For more information, refer to  Virtualenv User Guide: activate\nscript .  Once activated, run Guild commands as you normally would.  You can verify Guild details by running  guild check , which will\ndisplay  guild_home  and  guild_install_location  settings, letting\nyou confirm that Guild is isolated as expected.",
            "title": "Using a virtual environment"
        },
        {
            "location": "/docs/tutorials/workflow-with-virtualenv/#summary",
            "text": "In this tutorial we reviewed Virtualenv and how it can be used to\nisolate Guild on a system. We described how  GUILD_HOME  can be shared\nacross virtual environments by explicitly setting an environment\nvariable in the  activate  script.",
            "title": "Summary"
        },
        {
            "location": "/troubleshooting/",
            "text": "Troubleshooting\n\n\n\n\nPython\n\n\nAttributeError: module \u2018enum\u2019 has no attribute \u2018IntFlag\u2019\n\n\n\n\n\n\nTensorFlow\n\n\nTensorFlow is not installed\n\n\n\n\n\n\nCUDA and cuDNN errors\n\n\nMissing libcublas.so\n\n\nMissing libcudnn.so\n\n\n\n\n\n\nGPU stats\n\n\nNVML library version mismatch\n\n\n\n\n\n\n\n\nThis guide addresses various issues you may face when using Guild AI.\n\n\nMost setup related errors can be discovered by running \nguild\ncheck\n.\n\n\nRefer to the issues below for help. If you don\u2019t see your issue,\n\nopen an issue on GitHub\n and we\u2019ll work with you to resolve it.\n\n\nPython\n\n\nAttributeError: module \u2018enum\u2019 has no attribute \u2018IntFlag\u2019\n\n\nThis issue may arise on some systems, in particular Mac OS. Refer to\n\nPython 3.6.1 throws AttributeError: module \u2018enum\u2019 has no attribute\n\u2018IntFlag\u2019\n\nfor background.\n\n\nThe solution may be to uninstall the pip package \nenum34\n:\n\n\npip uninstall enum34\n\n\n\n\nTensorFlow\n\n\nTensorFlow is not installed\n\n\nWhen you run \nguild check\n, you may see this error:\n\n\ntensorflow_version: NOT INSTALLED (No module named 'tensorflow')\n\n\n\n\nThe \ntensorflow\n Python module must be available for Guild training\nscripts. You can test whether this module is available directly by\nrunning:\n\n\npython -m tensorflow\n\n\n\n\nIf the command exits without an error, TensorFlow is available but is\nnot visible to Guild. In this case \nopen an issue on GitHub\n and we\u2019ll\nhelp you resolve it.\n\n\nIf the command exits with the message \nNo module named tensorflow\n\nyou must install TensorFlow.\n\n\nRefer to \nInstalling TensorFlow\n for instructions for your\nsystem.\n\n\nIn most cases, you can install TensorFlow by running:\n\n\npip install tensorflow\n\n\n\n\nIf your system has a GPU, run:\n\n\npip install tensorflow-gpu\n\n\n\n\nCUDA and cuDNN errors\n\n\nMissing libcublas.so\n\n\nWhen loading the \ntensorflow\n Python module, you may see this error:\n\n\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\n\n\n\n\nNote\n\n\nThe version of \nlibcublas.so\n will differ based on the version of\nTensorFlow you\u2019re trying to load.\n\n\n\n\nThis indicates that TensorFlow cannot load NVIDIA\u2019s CUDA library\nbecause either the required version is not installed or it\u2019s not\nloadable from your shell environment.\n\n\nNote the version number in the \nlibcublas.so\n filename. This indicates\nthe required CUDA version.\n\n\nFor instructions on installing CUDA for your system, see \nCUDA Toolkit\nDocumentation\n.\n\n\nMissing libcudnn.so\n\n\nWhen loading the \ntensorflow\n Python module, you may see this error:\n\n\nImportError: libcudnn.so.7: cannot open shared object file: No such file or directory\n\n\n\n\n\n\nNote\n\n\nThe version of \nlibcudnn.so\n will differ based on the version of\nTensorFlow you\u2019re trying to load.\n\n\n\n\nThis indicates that TensorFlow cannot load NVIDIA\u2019s cuDNN library\nbecause either the required version is not installed or it\u2019s not\nloadable from your shell environment.\n\n\nNote the version number in the \nlibcudnn.so\n filename. This indicates\nthe required cuDNN version.\n\n\nFor instructions on installing cuDNN for your system, see \nDeep\nLearning SDK Documentation\n.\n\n\nGPU stats\n\n\nNVML library version mismatch\n\n\nWhen training on a GPU enabled system, you may see this error printed\nto the console:\n\n\nERROR: [guild.gpu] reading GPU stats (smi output: '[['Failed to initialize NVML: Driver/library version mismatch']]')\n\n\n\n\nThis indicates that \nnvidia-smi\n is installed but the CUDA library\nversion is incompatible.\n\n\nEnsure that your version of \nnvidia-smi\n is compatible with the\nversion of CUDA installed.\n\n\nYou can get the CUDA version by running:\n\n\nguild check | grep cuda",
            "title": "Troubleshooting"
        },
        {
            "location": "/troubleshooting/#troubleshooting",
            "text": "Python  AttributeError: module \u2018enum\u2019 has no attribute \u2018IntFlag\u2019    TensorFlow  TensorFlow is not installed    CUDA and cuDNN errors  Missing libcublas.so  Missing libcudnn.so    GPU stats  NVML library version mismatch     This guide addresses various issues you may face when using Guild AI.  Most setup related errors can be discovered by running  guild\ncheck .  Refer to the issues below for help. If you don\u2019t see your issue, open an issue on GitHub  and we\u2019ll work with you to resolve it.",
            "title": "Troubleshooting"
        },
        {
            "location": "/troubleshooting/#python",
            "text": "",
            "title": "Python"
        },
        {
            "location": "/troubleshooting/#attributeerror-module-enum-has-no-attribute-intflag",
            "text": "This issue may arise on some systems, in particular Mac OS. Refer to Python 3.6.1 throws AttributeError: module \u2018enum\u2019 has no attribute\n\u2018IntFlag\u2019 \nfor background.  The solution may be to uninstall the pip package  enum34 :  pip uninstall enum34",
            "title": "AttributeError: module &lsquo;enum&rsquo; has no attribute &lsquo;IntFlag&rsquo;"
        },
        {
            "location": "/troubleshooting/#tensorflow",
            "text": "",
            "title": "TensorFlow"
        },
        {
            "location": "/troubleshooting/#tensorflow-is-not-installed",
            "text": "When you run  guild check , you may see this error:  tensorflow_version: NOT INSTALLED (No module named 'tensorflow')  The  tensorflow  Python module must be available for Guild training\nscripts. You can test whether this module is available directly by\nrunning:  python -m tensorflow  If the command exits without an error, TensorFlow is available but is\nnot visible to Guild. In this case  open an issue on GitHub  and we\u2019ll\nhelp you resolve it.  If the command exits with the message  No module named tensorflow \nyou must install TensorFlow.  Refer to  Installing TensorFlow  for instructions for your\nsystem.  In most cases, you can install TensorFlow by running:  pip install tensorflow  If your system has a GPU, run:  pip install tensorflow-gpu",
            "title": "TensorFlow is not installed"
        },
        {
            "location": "/troubleshooting/#cuda-and-cudnn-errors",
            "text": "",
            "title": "CUDA and cuDNN errors"
        },
        {
            "location": "/troubleshooting/#missing-libcublasso",
            "text": "When loading the  tensorflow  Python module, you may see this error:  ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory   Note  The version of  libcublas.so  will differ based on the version of\nTensorFlow you\u2019re trying to load.   This indicates that TensorFlow cannot load NVIDIA\u2019s CUDA library\nbecause either the required version is not installed or it\u2019s not\nloadable from your shell environment.  Note the version number in the  libcublas.so  filename. This indicates\nthe required CUDA version.  For instructions on installing CUDA for your system, see  CUDA Toolkit\nDocumentation .",
            "title": "Missing libcublas.so"
        },
        {
            "location": "/troubleshooting/#missing-libcudnnso",
            "text": "When loading the  tensorflow  Python module, you may see this error:  ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory   Note  The version of  libcudnn.so  will differ based on the version of\nTensorFlow you\u2019re trying to load.   This indicates that TensorFlow cannot load NVIDIA\u2019s cuDNN library\nbecause either the required version is not installed or it\u2019s not\nloadable from your shell environment.  Note the version number in the  libcudnn.so  filename. This indicates\nthe required cuDNN version.  For instructions on installing cuDNN for your system, see  Deep\nLearning SDK Documentation .",
            "title": "Missing libcudnn.so"
        },
        {
            "location": "/troubleshooting/#gpu-stats",
            "text": "",
            "title": "GPU stats"
        },
        {
            "location": "/troubleshooting/#nvml-library-version-mismatch",
            "text": "When training on a GPU enabled system, you may see this error printed\nto the console:  ERROR: [guild.gpu] reading GPU stats (smi output: '[['Failed to initialize NVML: Driver/library version mismatch']]')  This indicates that  nvidia-smi  is installed but the CUDA library\nversion is incompatible.  Ensure that your version of  nvidia-smi  is compatible with the\nversion of CUDA installed.  You can get the CUDA version by running:  guild check | grep cuda",
            "title": "NVML library version mismatch"
        },
        {
            "location": "/models/",
            "text": "Guild AI models\n\n\n\n\n\n\nCloud ML\n\n\nCensus Income Predictor\nMNIST\nDatasets\n\n\n\n\nGuild AI\n\n\nMNIST\n\n\n\n\nKeras\n\n\nDeep Dream\nMNIST\n\n\n\n\nMagenta\n\n\nImage\nMusic\n\n\n\n\nOfficial TensorFlow\n\n\nMNIST\nResNet\nWide and Deep\n\n\n\n\nTF Slim\n\n\nInception\nResNet\nVGG\n\n\n\n\nWikipedia\n\n\nDatasets",
            "title": "Models"
        },
        {
            "location": "/models/#guild-ai-models",
            "text": "",
            "title": "Guild AI models"
        },
        {
            "location": "/models/#cloud-ml",
            "text": "Census Income Predictor MNIST Datasets",
            "title": "Cloud ML"
        },
        {
            "location": "/models/#guild-ai",
            "text": "MNIST",
            "title": "Guild AI"
        },
        {
            "location": "/models/#keras",
            "text": "Deep Dream MNIST",
            "title": "Keras"
        },
        {
            "location": "/models/#magenta",
            "text": "Image Music",
            "title": "Magenta"
        },
        {
            "location": "/models/#official-tensorflow",
            "text": "MNIST ResNet Wide and Deep",
            "title": "Official TensorFlow"
        },
        {
            "location": "/models/#tf-slim",
            "text": "Inception ResNet VGG",
            "title": "TF Slim"
        },
        {
            "location": "/models/#wikipedia",
            "text": "Datasets",
            "title": "Wikipedia"
        },
        {
            "location": "/models/cloudml/",
            "text": "Cloud ML\n\n\nGoogle\u2019s Cloud Machine Learning Engine (Cloud ML) can be used to train\nand deploy TensorFlow models. The models below are provided by the\nCloud ML team as working examples that you can experiment and learn\nfrom in developing your own.\n\n\nCloud ML enabled models can be trained locally or in the cloud. They\nstart and manage remote runs such as training or predicting.\n\n\n\n\n\n\nPackages\n\n\nCensus Income Predictor\nMNIST\nDatasets",
            "title": "Cloud ML"
        },
        {
            "location": "/models/cloudml/#cloud-ml",
            "text": "Google\u2019s Cloud Machine Learning Engine (Cloud ML) can be used to train\nand deploy TensorFlow models. The models below are provided by the\nCloud ML team as working examples that you can experiment and learn\nfrom in developing your own.  Cloud ML enabled models can be trained locally or in the cloud. They\nstart and manage remote runs such as training or predicting.",
            "title": "Cloud ML"
        },
        {
            "location": "/models/cloudml/#packages",
            "text": "Census Income Predictor MNIST Datasets",
            "title": "Packages"
        },
        {
            "location": "/models/cloudml/census/",
            "text": "Cloud ML Census Income Predictor\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ncloudml.census\n\n    \n\n    \n\n      \nDescription\n\n      \nCensus models for Cloud ML\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/cloudml/census\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \ncensus-dnn\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \ncensus-dnn model\n\n\n  \nWide and deep classifier for census income data using TensorFlow estimators.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ncloudml-batch-predict\n\n    \nSubmit a prediction job using a model deployed in Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ninstances\n\n          \nInstances to use for prediction.\n\n          \n\n            \n            \nprediction-samples.json\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ncloudml-deploy\n\n    \nDeploy a trained classifier in Cloud ML.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ncloudml-hptune\n\n    \nTune model hyperparameters in Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nconfig\n\n          \nHyperparameter tuning configuration.\n\n          \n\n            \n            \nhptuning_config.yaml\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ncloudml-predict\n\n    \nPerform an online prediction using a model deployed in Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ninstances\n\n          \nInstances to use for prediction.\n\n          \n\n            \n            \nprediction-samples.json\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ncloudml-train\n\n    \nTrain the classifier in Cloud ML.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain the classifier locally.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nembedding-size\n\n          \nNumber of embedding dimensions for categorical columns.\n\n          \n\n            \n            \n8\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of training data epochs\nIf both train-steps and epochs are specified, the training job will run for train-steps or epochs, whichever occurs first. If unspecified will run for train-steps.\n.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \neval-batch-size\n\n          \nBatch size for evaluation steps.\n\n          \n\n            \n            \n40\n\n            \n          \n\n        \n\n        \n        \n\n          \neval-steps\n\n          \nNumber of steps to run evalution for at each checkpoint.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nexport-format\n\n          \nThe input format of the exported SavedModel binary\nValues may be JSON, CSV, or EXAMPLE.\n.\n\n          \n\n            \n            \nJSON\n\n            \n          \n\n        \n\n        \n        \n\n          \nfirst-layer-size\n\n          \nNumber of nodes in the first layer of the DNN.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nlayers\n\n          \nNumber of layers in the DNN.\n\n          \n\n            \n            \n4\n\n            \n          \n\n        \n\n        \n        \n\n          \nscale-factor\n\n          \nHow quickly the size of the layers in the DNN decay.\n\n          \n\n            \n            \n0.7\n\n            \n          \n\n        \n\n        \n        \n\n          \ntrain-batch-size\n\n          \nBatch size for training steps.\n\n          \n\n            \n            \n40\n\n            \n          \n\n        \n\n        \n        \n\n          \ntrain-steps\n\n          \nSteps to run the training job for.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \nverbosity\n\n          \nLog level (use DEBUG for more information)\nValues may be DEBUG, INFO, WARN, ERROR, or FATAL.\n.\n\n          \n\n            \n            \nINFO",
            "title": "Cloud ML Census Income Predictor"
        },
        {
            "location": "/models/cloudml/census/#cloud-ml-census-income-predictor",
            "text": "Name \n       cloudml.census \n     \n     \n       Description \n       Census models for Cloud ML \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/cloudml/census \n     \n     \n       Maintainer",
            "title": "Cloud ML Census Income Predictor"
        },
        {
            "location": "/models/cloudml/census/#models",
            "text": "census-dnn",
            "title": "Models"
        },
        {
            "location": "/models/cloudml/census/#census-dnn",
            "text": "Wide and deep classifier for census income data using TensorFlow estimators. \n\n   \n     Operations",
            "title": "census-dnn model"
        },
        {
            "location": "/models/cloudml/census/#census-dnn-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-batch-predict",
            "text": "Submit a prediction job using a model deployed in Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           instances \n           Instances to use for prediction. \n           \n            \n             prediction-samples.json",
            "title": "cloudml-batch-predict"
        },
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-deploy",
            "text": "Deploy a trained classifier in Cloud ML. \n\n     \n      \n       This operation does not have any flags.",
            "title": "cloudml-deploy"
        },
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-hptune",
            "text": "Tune model hyperparameters in Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           config \n           Hyperparameter tuning configuration. \n           \n            \n             hptuning_config.yaml",
            "title": "cloudml-hptune"
        },
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-predict",
            "text": "Perform an online prediction using a model deployed in Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           instances \n           Instances to use for prediction. \n           \n            \n             prediction-samples.json",
            "title": "cloudml-predict"
        },
        {
            "location": "/models/cloudml/census/#census-dnn-cloudml-train",
            "text": "Train the classifier in Cloud ML. \n\n     \n      \n       This operation does not have any flags.",
            "title": "cloudml-train"
        },
        {
            "location": "/models/cloudml/census/#census-dnn-train",
            "text": "Train the classifier locally. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           embedding-size \n           Number of embedding dimensions for categorical columns. \n           \n            \n             8 \n            \n           \n         \n        \n         \n           epochs \n           Number of training data epochs\nIf both train-steps and epochs are specified, the training job will run for train-steps or epochs, whichever occurs first. If unspecified will run for train-steps.\n. \n           \n            \n           \n         \n        \n         \n           eval-batch-size \n           Batch size for evaluation steps. \n           \n            \n             40 \n            \n           \n         \n        \n         \n           eval-steps \n           Number of steps to run evalution for at each checkpoint. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           export-format \n           The input format of the exported SavedModel binary\nValues may be JSON, CSV, or EXAMPLE.\n. \n           \n            \n             JSON \n            \n           \n         \n        \n         \n           first-layer-size \n           Number of nodes in the first layer of the DNN. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           layers \n           Number of layers in the DNN. \n           \n            \n             4 \n            \n           \n         \n        \n         \n           scale-factor \n           How quickly the size of the layers in the DNN decay. \n           \n            \n             0.7 \n            \n           \n         \n        \n         \n           train-batch-size \n           Batch size for training steps. \n           \n            \n             40 \n            \n           \n         \n        \n         \n           train-steps \n           Steps to run the training job for. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           verbosity \n           Log level (use DEBUG for more information)\nValues may be DEBUG, INFO, WARN, ERROR, or FATAL.\n. \n           \n            \n             INFO",
            "title": "train"
        },
        {
            "location": "/models/cloudml/mnist/",
            "text": "Cloud ML MNIST\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ncloudml.mnist\n\n    \n\n    \n\n      \nDescription\n\n      \nMNIST model for Cloud ML\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/cloudml/mnist\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmnist-cnn\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-cnn model\n\n\n  \nCNN classifier for MNIST data.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ncloudml-deploy\n\n    \nDeploy a model to Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nruntime-version\n\n          \nTensorFlow runtime version.\n\n          \n\n            \n            \n1.2\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ncloudml-predict\n\n    \nSend a prediction request to Cloud ML.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ncloudml-train\n\n    \nTrain the classifier in Cloud ML.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nruntime-version\n\n          \nTensorFlow runtime version.\n\n          \n\n            \n            \n1.2\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nprepare-data\n\n    \nPrepare the MNIST data for training.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nvalidation-size\n\n          \nNumber of examples to separate from the training data for the validation\n.\n\n          \n\n            \n            \n5000\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nprepare-request\n\n    \nGenerate a sample request file.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain the classifier locally.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \neval-batch-size\n\n          \nBatch size for evaluation steps.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \neval-steps\n\n          \nNumber of steps to run evalution for at each checkpoint.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \ntrain-batch-size\n\n          \nBatch size for training steps.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \ntrain-steps\n\n          \nSteps to run the training job for.\n\n          \n\n            \n            \n10000\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ndata\n\n    \nPrepared MNIST data.\n\n    \n\n      \n\n        \n        \n['.*\\\\.tfrecords']\n from \nprepare-data\n operation",
            "title": "Cloud ML MNIST"
        },
        {
            "location": "/models/cloudml/mnist/#cloud-ml-mnist",
            "text": "Name \n       cloudml.mnist \n     \n     \n       Description \n       MNIST model for Cloud ML \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/cloudml/mnist \n     \n     \n       Maintainer",
            "title": "Cloud ML MNIST"
        },
        {
            "location": "/models/cloudml/mnist/#models",
            "text": "mnist-cnn",
            "title": "Models"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn",
            "text": "CNN classifier for MNIST data. \n\n   \n     Operations \n    \n     Resources",
            "title": "mnist-cnn model"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-cloudml-deploy",
            "text": "Deploy a model to Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           runtime-version \n           TensorFlow runtime version. \n           \n            \n             1.2",
            "title": "cloudml-deploy"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-cloudml-predict",
            "text": "Send a prediction request to Cloud ML. \n\n     \n      \n       This operation does not have any flags.",
            "title": "cloudml-predict"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-cloudml-train",
            "text": "Train the classifier in Cloud ML. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           runtime-version \n           TensorFlow runtime version. \n           \n            \n             1.2",
            "title": "cloudml-train"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-prepare-data",
            "text": "Prepare the MNIST data for training. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           validation-size \n           Number of examples to separate from the training data for the validation\n. \n           \n            \n             5000",
            "title": "prepare-data"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-prepare-request",
            "text": "Generate a sample request file. \n\n     \n      \n       This operation does not have any flags.",
            "title": "prepare-request"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-train",
            "text": "Train the classifier locally. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           eval-batch-size \n           Batch size for evaluation steps. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           eval-steps \n           Number of steps to run evalution for at each checkpoint. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           train-batch-size \n           Batch size for training steps. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           train-steps \n           Steps to run the training job for. \n           \n            \n             10000",
            "title": "train"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/cloudml/mnist/#mnist-cnn-data-res",
            "text": "Prepared MNIST data. \n     \n       \n        \n         ['.*\\\\.tfrecords']  from  prepare-data  operation",
            "title": "data"
        },
        {
            "location": "/models/cloudml/datasets/",
            "text": "Cloud ML Datasets\n\n\nCloud ML datasets are used with Cloud ML models. They are provided as\npackage resources and automatically downloaded when needed by a Cloud\nML model operation.\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ncloudml.datasets\n\n    \n\n    \n\n      \nDescription\n\n      \nCloud ML datasets\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/cloudml/datasets\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n\n  \n  \n\n    \n\n      \nResources\n\n    \n    \ncensus\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \ncensus resource\n\n\n  \nCensus income dataset hosted on Google Cloud Storage.\n\n\n  \nSources\n\n  \n\n    \n\n      \n      \nhttps://storage.googleapis.com/cloudml-public/census/data/adult.data.csv\n\n      \n      \nhttps://storage.googleapis.com/cloudml-public/census/data/adult.test.csv\n\n      \n    \n\n  \n\n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://cloud.google.com/ml-engine/docs/getting-started-training-prediction#about-data\n\n      \n      \nhttps://archive.ics.uci.edu/ml/datasets/Census+Income",
            "title": "Cloud ML Datasets"
        },
        {
            "location": "/models/cloudml/datasets/#cloud-ml-datasets",
            "text": "Cloud ML datasets are used with Cloud ML models. They are provided as\npackage resources and automatically downloaded when needed by a Cloud\nML model operation.  \n\n   \n     \n       Name \n       cloudml.datasets \n     \n     \n       Description \n       Cloud ML datasets \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/cloudml/datasets \n     \n     \n       Maintainer",
            "title": "Cloud ML Datasets"
        },
        {
            "location": "/models/cloudml/datasets/#resources",
            "text": "census",
            "title": "Resources"
        },
        {
            "location": "/models/cloudml/datasets/#census",
            "text": "Census income dataset hosted on Google Cloud Storage.",
            "title": "census resource"
        },
        {
            "location": "/models/cloudml/datasets/#census-sources",
            "text": "https://storage.googleapis.com/cloudml-public/census/data/adult.data.csv \n      \n       https://storage.googleapis.com/cloudml-public/census/data/adult.test.csv",
            "title": "Sources"
        },
        {
            "location": "/models/cloudml/datasets/#census-references",
            "text": "https://cloud.google.com/ml-engine/docs/getting-started-training-prediction#about-data \n      \n       https://archive.ics.uci.edu/ml/datasets/Census+Income",
            "title": "References"
        },
        {
            "location": "/models/mnist/",
            "text": "Guild AI MNIST\n\n\nThis package contains softmax and CNN models for training with the\nMNIST dataset. These are used primarily for introducing Guild AI and\nfor testing.\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nmnist\n\n    \n\n    \n\n      \nDescription\n\n      \nCNN and softmax regression classifiers for MNIST digits\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/mnist\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmnist-cnn\n\n    \n    \nmnist-samples\n\n    \n    \nmnist-softmax\n\n    \n    \n\n  \n\n  \n\n  \n  \n\n    \n\n      \nResources\n\n    \n    \ndataset\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-cnn model\n\n\n  \nCNN classifier for MNIST.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained CNN.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain the CNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images to include in a training batch.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-samples model\n\n\n  \nSample MNIST images.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nprepare\n\n    \nGenerate a set of sample MNIST images.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ncount\n\n          \nNumber of images to generate.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-softmax model\n\n\n  \nSoftmax regression classifier for MNIST.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained softmax regression.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain the softmax regression.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images to include in a training batch.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \ndataset resource\n\n\n  \nYann Lecun's MNIST dataset in compressed IDX format.\n\n\n  \nSources\n\n  \n\n    \n\n      \n      \nhttp://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n\n      \n      \nhttp://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n\n      \n      \nhttp://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n\n      \n      \nhttp://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz",
            "title": "Guild AI MNIST"
        },
        {
            "location": "/models/mnist/#guild-ai-mnist",
            "text": "This package contains softmax and CNN models for training with the\nMNIST dataset. These are used primarily for introducing Guild AI and\nfor testing.  \n\n   \n     \n       Name \n       mnist \n     \n     \n       Description \n       CNN and softmax regression classifiers for MNIST digits \n     \n     \n       Version \n       0.3.1.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/mnist \n     \n     \n       Maintainer",
            "title": "Guild AI MNIST"
        },
        {
            "location": "/models/mnist/#models",
            "text": "mnist-cnn \n    \n     mnist-samples \n    \n     mnist-softmax",
            "title": "Models"
        },
        {
            "location": "/models/mnist/#resources",
            "text": "dataset",
            "title": "Resources"
        },
        {
            "location": "/models/mnist/#mnist-cnn",
            "text": "CNN classifier for MNIST. \n\n   \n     Operations",
            "title": "mnist-cnn model"
        },
        {
            "location": "/models/mnist/#mnist-cnn-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/mnist/#mnist-cnn-evaluate",
            "text": "Evaluate a trained CNN. \n\n     \n      \n       This operation does not have any flags.",
            "title": "evaluate"
        },
        {
            "location": "/models/mnist/#mnist-cnn-train",
            "text": "Train the CNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images to include in a training batch. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             10",
            "title": "train"
        },
        {
            "location": "/models/mnist/#mnist-samples",
            "text": "Sample MNIST images. \n\n   \n     Operations",
            "title": "mnist-samples model"
        },
        {
            "location": "/models/mnist/#mnist-samples-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/mnist/#mnist-samples-prepare",
            "text": "Generate a set of sample MNIST images. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           count \n           Number of images to generate. \n           \n            \n             100",
            "title": "prepare"
        },
        {
            "location": "/models/mnist/#mnist-softmax",
            "text": "Softmax regression classifier for MNIST. \n\n   \n     Operations",
            "title": "mnist-softmax model"
        },
        {
            "location": "/models/mnist/#mnist-softmax-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/mnist/#mnist-softmax-evaluate",
            "text": "Evaluate a trained softmax regression. \n\n     \n      \n       This operation does not have any flags.",
            "title": "evaluate"
        },
        {
            "location": "/models/mnist/#mnist-softmax-train",
            "text": "Train the softmax regression. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images to include in a training batch. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             10",
            "title": "train"
        },
        {
            "location": "/models/mnist/#dataset",
            "text": "Yann Lecun's MNIST dataset in compressed IDX format.",
            "title": "dataset resource"
        },
        {
            "location": "/models/mnist/#dataset-sources",
            "text": "http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz \n      \n       http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz \n      \n       http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz \n      \n       http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz",
            "title": "Sources"
        },
        {
            "location": "/models/keras/",
            "text": "Keras\n\n\nKeras\n is an independent neural networks API\nthat has been adopted as a core interface to TensorFlow. Keras\nprovides a number of excellent \nmodels\n, many of\nwhich have been adapted as Guild packages.\n\n\nKeras models are supported without modification. You can specify\ntraining epochs, batch size, and model hyper parameters without\nediting Keras scripts. Guild AI also enables TensorFlow event logging\nfor models so you can view training metrics using\n\nTensorBoard\n.\n\n\n\n\n\n\nPackages\n\n\nDeep Dream\nMNIST",
            "title": "Keras"
        },
        {
            "location": "/models/keras/#keras",
            "text": "Keras  is an independent neural networks API\nthat has been adopted as a core interface to TensorFlow. Keras\nprovides a number of excellent  models , many of\nwhich have been adapted as Guild packages.  Keras models are supported without modification. You can specify\ntraining epochs, batch size, and model hyper parameters without\nediting Keras scripts. Guild AI also enables TensorFlow event logging\nfor models so you can view training metrics using TensorBoard .",
            "title": "Keras"
        },
        {
            "location": "/models/keras/#packages",
            "text": "Deep Dream MNIST",
            "title": "Packages"
        },
        {
            "location": "/models/keras/deep-dream/",
            "text": "Keras Deep Dream\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nkeras.deep-dream\n\n    \n\n    \n\n      \nDescription\n\n      \nDeep dream generator in Keras\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/keras/deep-dream\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \ndeep-dream\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \ndeep-dream model\n\n\n  \n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ngenerate\n\n    \nGenerate a Deep Dream image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nimage\n\n          \nInput image used by Deep Dream.\n\n          \n\n            \n            \nrequired",
            "title": "Keras Deep Dream"
        },
        {
            "location": "/models/keras/deep-dream/#keras-deep-dream",
            "text": "Name \n       keras.deep-dream \n     \n     \n       Description \n       Deep dream generator in Keras \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/keras/deep-dream \n     \n     \n       Maintainer",
            "title": "Keras Deep Dream"
        },
        {
            "location": "/models/keras/deep-dream/#models",
            "text": "deep-dream",
            "title": "Models"
        },
        {
            "location": "/models/keras/deep-dream/#deep-dream",
            "text": "Operations",
            "title": "deep-dream model"
        },
        {
            "location": "/models/keras/deep-dream/#deep-dream-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/deep-dream/#deep-dream-generate",
            "text": "Generate a Deep Dream image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           image \n           Input image used by Deep Dream. \n           \n            \n             required",
            "title": "generate"
        },
        {
            "location": "/models/keras/mnist/",
            "text": "Keras MNIST\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nkeras.mnist\n\n    \n\n    \n\n      \nDescription\n\n      \nMNIST related models in Keras\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/keras/mnist\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmnist-acgan\n\n    \n    \nmnist-cnn\n\n    \n    \nmnist-denoising-autoencoder\n\n    \n    \nmnist-hierarchical-rnn\n\n    \n    \nmnist-irnn\n\n    \n    \nmnist-mlp\n\n    \n    \nmnist-net2net\n\n    \n    \nmnist-siamese\n\n    \n    \nmnist-swwae\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-acgan model\n\n\n  \nAuxiliary Classifier Generative Adversarial Network (ACGAN) for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the ACGAN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nbeta-1\n\n          \nBeta 1.\n\n          \n\n            \n            \n0.5\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nLearning rate.\n\n          \n\n            \n            \n0.0002\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_acgan.py\n\n      \n      \narXiv: 1511.06434\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-cnn model\n\n\n  \nConvolutional neural network (CNN) classifier for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the CNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n12\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_cnn.py\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-denoising-autoencoder model\n\n\n  \nDenoising autoencoder for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the autoencoder.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n30\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_denoising_autoencoder.py\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-hierarchical-rnn model\n\n\n  \nHierarchical RNN (HRNN) classifier for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the HRNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n5\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_hierarchical_rnn.py\n\n      \n      \narXiv: 1506.01057\n\n      \n      \nhttp://ieeexplore.ieee.org/document/7298714/\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-irnn model\n\n\n  \nImplementation of 'A Simple Way to Initialize Recurrent Networks of Rectified Linear Units' with MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the RNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n200\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nLearning rate.\n\n          \n\n            \n            \n1e-06\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_irnn.py\n\n      \n      \nhttp://arxiv.org/pdf/1504.00941v2.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-mlp model\n\n\n  \nMultilayer perceptron (MLP) classifier for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the MLP.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n20\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_mlp.py\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-net2net model\n\n\n  \nImplementation of 'Net2Net: Accelerating Learning via Knowledge Transfer' with MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the network.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n3\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_net2net.py\n\n      \n      \nhttp://arxiv.org/abs/1511.05641\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-siamese model\n\n\n  \nSiamese MLP classifier for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the MLP.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n20\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_siamese.py\n\n      \n      \nhttp://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-swwae model\n\n\n  \nStacked what-where autoencoder for MNIST in Keras.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the MLP.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nTraining batch size.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n5\n\n            \n          \n\n        \n\n        \n        \n\n          \npool-size\n\n          \nkernel size used for the MaxPooling2D.\n\n          \n\n            \n            \n2\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: keras-team/keras/examples/mnist_swwae.py\n\n      \n      \narXiv: 1311.2901v3\n\n      \n      \narXiv: 1506.02351v8",
            "title": "Keras MNIST"
        },
        {
            "location": "/models/keras/mnist/#keras-mnist",
            "text": "Name \n       keras.mnist \n     \n     \n       Description \n       MNIST related models in Keras \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/keras/mnist \n     \n     \n       Maintainer",
            "title": "Keras MNIST"
        },
        {
            "location": "/models/keras/mnist/#models",
            "text": "mnist-acgan \n    \n     mnist-cnn \n    \n     mnist-denoising-autoencoder \n    \n     mnist-hierarchical-rnn \n    \n     mnist-irnn \n    \n     mnist-mlp \n    \n     mnist-net2net \n    \n     mnist-siamese \n    \n     mnist-swwae",
            "title": "Models"
        },
        {
            "location": "/models/keras/mnist/#mnist-acgan",
            "text": "Auxiliary Classifier Generative Adversarial Network (ACGAN) for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-acgan model"
        },
        {
            "location": "/models/keras/mnist/#mnist-acgan-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-acgan-train",
            "text": "Train the ACGAN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           beta-1 \n           Beta 1. \n           \n            \n             0.5 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           learning-rate \n           Learning rate. \n           \n            \n             0.0002",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-acgan-references",
            "text": "GitHub: keras-team/keras/examples/mnist_acgan.py \n      \n       arXiv: 1511.06434",
            "title": "References"
        },
        {
            "location": "/models/keras/mnist/#mnist-cnn",
            "text": "Convolutional neural network (CNN) classifier for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-cnn model"
        },
        {
            "location": "/models/keras/mnist/#mnist-cnn-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-cnn-train",
            "text": "Train the CNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             12",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-cnn-references",
            "text": "GitHub: keras-team/keras/examples/mnist_cnn.py",
            "title": "References"
        },
        {
            "location": "/models/keras/mnist/#mnist-denoising-autoencoder",
            "text": "Denoising autoencoder for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-denoising-autoencoder model"
        },
        {
            "location": "/models/keras/mnist/#mnist-denoising-autoencoder-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-denoising-autoencoder-train",
            "text": "Train the autoencoder. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             30",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-denoising-autoencoder-references",
            "text": "GitHub: keras-team/keras/examples/mnist_denoising_autoencoder.py",
            "title": "References"
        },
        {
            "location": "/models/keras/mnist/#mnist-hierarchical-rnn",
            "text": "Hierarchical RNN (HRNN) classifier for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-hierarchical-rnn model"
        },
        {
            "location": "/models/keras/mnist/#mnist-hierarchical-rnn-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-hierarchical-rnn-train",
            "text": "Train the HRNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             5",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-hierarchical-rnn-references",
            "text": "GitHub: keras-team/keras/examples/mnist_hierarchical_rnn.py \n      \n       arXiv: 1506.01057 \n      \n       http://ieeexplore.ieee.org/document/7298714/",
            "title": "References"
        },
        {
            "location": "/models/keras/mnist/#mnist-irnn",
            "text": "Implementation of 'A Simple Way to Initialize Recurrent Networks of Rectified Linear Units' with MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-irnn model"
        },
        {
            "location": "/models/keras/mnist/#mnist-irnn-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-irnn-train",
            "text": "Train the RNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             200 \n            \n           \n         \n        \n         \n           learning-rate \n           Learning rate. \n           \n            \n             1e-06",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-irnn-references",
            "text": "GitHub: keras-team/keras/examples/mnist_irnn.py \n      \n       http://arxiv.org/pdf/1504.00941v2.pdf",
            "title": "References"
        },
        {
            "location": "/models/keras/mnist/#mnist-mlp",
            "text": "Multilayer perceptron (MLP) classifier for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-mlp model"
        },
        {
            "location": "/models/keras/mnist/#mnist-mlp-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-mlp-train",
            "text": "Train the MLP. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             20",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-mlp-references",
            "text": "GitHub: keras-team/keras/examples/mnist_mlp.py",
            "title": "References"
        },
        {
            "location": "/models/keras/mnist/#mnist-net2net",
            "text": "Implementation of 'Net2Net: Accelerating Learning via Knowledge Transfer' with MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-net2net model"
        },
        {
            "location": "/models/keras/mnist/#mnist-net2net-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-net2net-train",
            "text": "Train the network. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             3",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-net2net-references",
            "text": "GitHub: keras-team/keras/examples/mnist_net2net.py \n      \n       http://arxiv.org/abs/1511.05641",
            "title": "References"
        },
        {
            "location": "/models/keras/mnist/#mnist-siamese",
            "text": "Siamese MLP classifier for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-siamese model"
        },
        {
            "location": "/models/keras/mnist/#mnist-siamese-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-siamese-train",
            "text": "Train the MLP. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             20",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-siamese-references",
            "text": "GitHub: keras-team/keras/examples/mnist_siamese.py \n      \n       http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf",
            "title": "References"
        },
        {
            "location": "/models/keras/mnist/#mnist-swwae",
            "text": "Stacked what-where autoencoder for MNIST in Keras. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-swwae model"
        },
        {
            "location": "/models/keras/mnist/#mnist-swwae-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/keras/mnist/#mnist-swwae-train",
            "text": "Train the MLP. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Training batch size. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             5 \n            \n           \n         \n        \n         \n           pool-size \n           kernel size used for the MaxPooling2D. \n           \n            \n             2",
            "title": "train"
        },
        {
            "location": "/models/keras/mnist/#mnist-swwae-references",
            "text": "GitHub: keras-team/keras/examples/mnist_swwae.py \n      \n       arXiv: 1311.2901v3 \n      \n       arXiv: 1506.02351v8",
            "title": "References"
        },
        {
            "location": "/models/magenta/",
            "text": "Magenta\n\n\nMagenta\n is a project started by\nGoogle Brain to explore the role of machine learning in the creation\nof visual and musical art. Since its inception many outside Google\nhave made significant contributions to the project.\n\n\nGuild AI supports a number of Magenta models and more are being\nadded. See each of the packages below for its list of supported\nmodels.\n\n\n\n\n\n\nPackages\n\n\nImage\nMusic",
            "title": "Magenta"
        },
        {
            "location": "/models/magenta/#magenta",
            "text": "Magenta  is a project started by\nGoogle Brain to explore the role of machine learning in the creation\nof visual and musical art. Since its inception many outside Google\nhave made significant contributions to the project.  Guild AI supports a number of Magenta models and more are being\nadded. See each of the packages below for its list of supported\nmodels.",
            "title": "Magenta"
        },
        {
            "location": "/models/magenta/#packages",
            "text": "Image Music",
            "title": "Packages"
        },
        {
            "location": "/models/magenta/image/",
            "text": "Magenta Image\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nmagenta.image\n\n    \n\n    \n\n      \nDescription\n\n      \nImage generators from the Magenta project\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/magenta/image\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmagenta-arbitrary-stylize\n\n    \n    \nmagenta-image-stylize\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmagenta-arbitrary-stylize model\n\n\n  \nFast artistic style transfer using arbitrary painting styles.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ngenerate\n\n    \nGenerate a stylized using model pretrained on PNB and DRD images.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ncontent-images\n\n          \nPath to content images (include glob pattern matching images).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage-size\n\n          \nSize of images.\n\n          \n\n            \n            \n1024\n\n            \n          \n\n        \n\n        \n        \n\n          \ninterpolation-weights\n\n          \nInterpolation weights\nThis value is a list of float values inside square brackets. Each value is a weight for interpolation between the parameters of the identity transform and the style parameters of the style image.\nThe larger the weight is the strength of stylization is more. Weight of 1.0 means the normal style transfer and weight of 0.0 means identity transform.\n\n          \n\n            \n            \n[1.0]\n\n            \n          \n\n        \n\n        \n        \n\n          \nstyle-images\n\n          \nPath to style images (include glob pattern matching images).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \nmagenta\n\n    \nMagenta Python distribution.\n\n    \n\n      \n\n        \n        \nmodule:magenta\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization\n\n      \n      \narXiv: 1705.06830\n\n      \n      \narXiv: 1610.07629\n\n      \n      \narXiv: 1603.08155\n\n      \n      \narXiv: 1508.06576\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nmagenta-image-stylize model\n\n\n  \nImplementation of 'A Learned Representation for Artistic Style'.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ngenerate\n\n    \nGenerate a stylized image using a pretrained models.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nimage\n\n          \nImage to stylize.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nstyle\n\n          \nStyle to apply (monet or varied).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \nmagenta\n\n    \nMagenta Python distribution.\n\n    \n\n      \n\n        \n        \nmodule:magenta\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization\n\n      \n      \narXiv: 1610.07629",
            "title": "Magenta Image"
        },
        {
            "location": "/models/magenta/image/#magenta-image",
            "text": "Name \n       magenta.image \n     \n     \n       Description \n       Image generators from the Magenta project \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/magenta/image \n     \n     \n       Maintainer",
            "title": "Magenta Image"
        },
        {
            "location": "/models/magenta/image/#models",
            "text": "magenta-arbitrary-stylize \n    \n     magenta-image-stylize",
            "title": "Models"
        },
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize",
            "text": "Fast artistic style transfer using arbitrary painting styles. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "magenta-arbitrary-stylize model"
        },
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-generate",
            "text": "Generate a stylized using model pretrained on PNB and DRD images. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           content-images \n           Path to content images (include glob pattern matching images). \n           \n            \n             required \n            \n           \n         \n        \n         \n           image-size \n           Size of images. \n           \n            \n             1024 \n            \n           \n         \n        \n         \n           interpolation-weights \n           Interpolation weights\nThis value is a list of float values inside square brackets. Each value is a weight for interpolation between the parameters of the identity transform and the style parameters of the style image.\nThe larger the weight is the strength of stylization is more. Weight of 1.0 means the normal style transfer and weight of 0.0 means identity transform. \n           \n            \n             [1.0] \n            \n           \n         \n        \n         \n           style-images \n           Path to style images (include glob pattern matching images). \n           \n            \n             required",
            "title": "generate"
        },
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-magenta-res",
            "text": "Magenta Python distribution. \n     \n       \n        \n         module:magenta",
            "title": "magenta"
        },
        {
            "location": "/models/magenta/image/#magenta-arbitrary-stylize-references",
            "text": "https://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization \n      \n       arXiv: 1705.06830 \n      \n       arXiv: 1610.07629 \n      \n       arXiv: 1603.08155 \n      \n       arXiv: 1508.06576",
            "title": "References"
        },
        {
            "location": "/models/magenta/image/#magenta-image-stylize",
            "text": "Implementation of 'A Learned Representation for Artistic Style'. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "magenta-image-stylize model"
        },
        {
            "location": "/models/magenta/image/#magenta-image-stylize-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/magenta/image/#magenta-image-stylize-generate",
            "text": "Generate a stylized image using a pretrained models. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           image \n           Image to stylize. \n           \n            \n             required \n            \n           \n         \n        \n         \n           style \n           Style to apply (monet or varied). \n           \n            \n             required",
            "title": "generate"
        },
        {
            "location": "/models/magenta/image/#magenta-image-stylize-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/magenta/image/#magenta-image-stylize-magenta-res",
            "text": "Magenta Python distribution. \n     \n       \n        \n         module:magenta",
            "title": "magenta"
        },
        {
            "location": "/models/magenta/image/#magenta-image-stylize-references",
            "text": "https://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization \n      \n       arXiv: 1610.07629",
            "title": "References"
        },
        {
            "location": "/models/magenta/music/",
            "text": "Magenta Music\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nmagenta.music\n\n    \n\n    \n\n      \nDescription\n\n      \nMusic generators from the Magenta project\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/magenta/music\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmagenta-melody\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmagenta-melody model\n\n\n  \nApplies language modeling to melody generation using an LSTM.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ngenerate\n\n    \nCompose melodies using one of three available pretrained models.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nconfig\n\n          \nModel configuration (basic, lookback, or attention).\n\n          \n\n            \n            \nbasic_rnn\n\n            \n          \n\n        \n\n        \n        \n\n          \noutputs\n\n          \nNumber of melodies to generate.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n        \n\n          \nprimary-midi\n\n          \nMIDI file used to prime the generator.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nprimer-melody\n\n          \nMelody to prime the generator.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nsteps\n\n          \nMelody length (16 steps = 1 bar).\n\n          \n\n            \n            \n128",
            "title": "Magenta Music"
        },
        {
            "location": "/models/magenta/music/#magenta-music",
            "text": "Name \n       magenta.music \n     \n     \n       Description \n       Music generators from the Magenta project \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/magenta/music \n     \n     \n       Maintainer",
            "title": "Magenta Music"
        },
        {
            "location": "/models/magenta/music/#models",
            "text": "magenta-melody",
            "title": "Models"
        },
        {
            "location": "/models/magenta/music/#magenta-melody",
            "text": "Applies language modeling to melody generation using an LSTM. \n\n   \n     Operations",
            "title": "magenta-melody model"
        },
        {
            "location": "/models/magenta/music/#magenta-melody-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/magenta/music/#magenta-melody-generate",
            "text": "Compose melodies using one of three available pretrained models. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           config \n           Model configuration (basic, lookback, or attention). \n           \n            \n             basic_rnn \n            \n           \n         \n        \n         \n           outputs \n           Number of melodies to generate. \n           \n            \n             10 \n            \n           \n         \n        \n         \n           primary-midi \n           MIDI file used to prime the generator. \n           \n            \n           \n         \n        \n         \n           primer-melody \n           Melody to prime the generator. \n           \n            \n           \n         \n        \n         \n           steps \n           Melody length (16 steps = 1 bar). \n           \n            \n             128",
            "title": "generate"
        },
        {
            "location": "/models/tensorflow/",
            "text": "Official TensorFlow models\n\n\nThe TensorFlow project maintains a set of \nofficial models on GitHub\n. These\nare each supported by Guild AI and are available from the packages\nbelow.\n\n\n\n\n\n\nPackages\n\n\nMNIST\nResNet\nWide and Deep",
            "title": "Official TensorFlow models"
        },
        {
            "location": "/models/tensorflow/#official-tensorflow-models",
            "text": "The TensorFlow project maintains a set of  official models on GitHub . These\nare each supported by Guild AI and are available from the packages\nbelow.",
            "title": "Official TensorFlow models"
        },
        {
            "location": "/models/tensorflow/#packages",
            "text": "MNIST ResNet Wide and Deep",
            "title": "Packages"
        },
        {
            "location": "/models/tensorflow/mnist/",
            "text": "TensorFlow MNIST\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ntensorflow.mnist\n\n    \n\n    \n\n      \nDescription\n\n      \nOfficial TensorFlow CNN classifiers for MNIST\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/tensorflow/mnist\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nmnist-cnn\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nmnist-cnn model\n\n\n  \nCNN classifier for MNIST using Estimators.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the CNN.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images to process in a batch.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n40\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/models/tree/v.1.6.0/official/mnist",
            "title": "TensorFlow MNIST"
        },
        {
            "location": "/models/tensorflow/mnist/#tensorflow-mnist",
            "text": "Name \n       tensorflow.mnist \n     \n     \n       Description \n       Official TensorFlow CNN classifiers for MNIST \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/tensorflow/mnist \n     \n     \n       Maintainer",
            "title": "TensorFlow MNIST"
        },
        {
            "location": "/models/tensorflow/mnist/#models",
            "text": "mnist-cnn",
            "title": "Models"
        },
        {
            "location": "/models/tensorflow/mnist/#mnist-cnn",
            "text": "CNN classifier for MNIST using Estimators. \n\n   \n     Operations \n    \n    \n     References",
            "title": "mnist-cnn model"
        },
        {
            "location": "/models/tensorflow/mnist/#mnist-cnn-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/tensorflow/mnist/#mnist-cnn-train",
            "text": "Train the CNN. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images to process in a batch. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             40",
            "title": "train"
        },
        {
            "location": "/models/tensorflow/mnist/#mnist-cnn-references",
            "text": "https://github.com/tensorflow/models/tree/v.1.6.0/official/mnist",
            "title": "References"
        },
        {
            "location": "/models/tensorflow/resnet/",
            "text": "TensorFlow ResNet\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ntensorflow.resnet\n\n    \n\n    \n\n      \nDescription\n\n      \nOfficial TensorFlow ResNet classifier\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/tensorflow/resnet\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nresnet-32-cifar10\n\n    \n    \nresnet-imagenet\n\n    \n    \n\n  \n\n  \n\n  \n  \n\n    \n\n      \nResources\n\n    \n    \nresnet-lib\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nresnet-32-cifar10 model\n\n\n  \nResNet-32 classifier for CIFAR-10.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the ResNet model on CIFAR-10 data.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images per batch.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n250\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs-per-eval\n\n          \nNumber of epochs to run in between evaluations.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ndataset\n\n    \nCIFAR10 dataset.\n\n    \n\n      \n\n        \n        \nhttps://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/models/tree/v.1.6.0/official/resnet\n\n      \n      \nhttps://arxiv.org/pdf/1512.03385.pdf\n\n      \n      \nhttps://arxiv.org/pdf/1603.05027.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nresnet-imagenet model\n\n\n  \nResNet classifier for ImageNet\nNote, this model requires that the ImageNet dataset be available and prepared as TFRrecords. Refer to https://git.io/vFQNF for instructions on preparing the dataset.\n\n\n  \n\n    \nOperations\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \ntrain\n\n    \nTrain the ResNet model on ImageNet data.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of images per batch.\n\n          \n\n            \n            \n128\n\n            \n          \n\n        \n\n        \n        \n\n          \ndata-dir\n\n          \nImageNet data location.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of epochs to train.\n\n          \n\n            \n            \n250\n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs-per-eval\n\n          \nNumber of epochs to run in between evaluations.\n\n          \n\n            \n            \n10\n\n            \n          \n\n        \n\n        \n        \n\n          \nresnet-size\n\n          \nSize of the ResNet model to use (18, 34, 50, 101, 152, or 200).\n\n          \n\n            \n            \n50\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/models/tree/v.1.6.0/official/resnet\n\n      \n      \nhttps://arxiv.org/pdf/1512.03385.pdf\n\n      \n      \nhttps://arxiv.org/pdf/1603.05027.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nresnet-lib resource\n\n\n  \nPython library for tensorflow.resnet.\n\n\n  \nSources\n\n  \n\n    \n\n      \n      \nhttps://github.com/tensorflow/models/archive/v.1.6.0.zip",
            "title": "TensorFlow ResNet"
        },
        {
            "location": "/models/tensorflow/resnet/#tensorflow-resnet",
            "text": "Name \n       tensorflow.resnet \n     \n     \n       Description \n       Official TensorFlow ResNet classifier \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/tensorflow/resnet \n     \n     \n       Maintainer",
            "title": "TensorFlow ResNet"
        },
        {
            "location": "/models/tensorflow/resnet/#models",
            "text": "resnet-32-cifar10 \n    \n     resnet-imagenet",
            "title": "Models"
        },
        {
            "location": "/models/tensorflow/resnet/#resources",
            "text": "resnet-lib",
            "title": "Resources"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10",
            "text": "ResNet-32 classifier for CIFAR-10. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "resnet-32-cifar10 model"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-train",
            "text": "Train the ResNet model on CIFAR-10 data. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images per batch. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             250 \n            \n           \n         \n        \n         \n           epochs-per-eval \n           Number of epochs to run in between evaluations. \n           \n            \n             10",
            "title": "train"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-dataset-res",
            "text": "CIFAR10 dataset. \n     \n       \n        \n         https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz",
            "title": "dataset"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-32-cifar10-references",
            "text": "https://github.com/tensorflow/models/tree/v.1.6.0/official/resnet \n      \n       https://arxiv.org/pdf/1512.03385.pdf \n      \n       https://arxiv.org/pdf/1603.05027.pdf",
            "title": "References"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-imagenet",
            "text": "ResNet classifier for ImageNet\nNote, this model requires that the ImageNet dataset be available and prepared as TFRrecords. Refer to https://git.io/vFQNF for instructions on preparing the dataset. \n\n   \n     Operations \n    \n    \n     References",
            "title": "resnet-imagenet model"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-imagenet-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-imagenet-train",
            "text": "Train the ResNet model on ImageNet data. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of images per batch. \n           \n            \n             128 \n            \n           \n         \n        \n         \n           data-dir \n           ImageNet data location. \n           \n            \n             required \n            \n           \n         \n        \n         \n           epochs \n           Number of epochs to train. \n           \n            \n             250 \n            \n           \n         \n        \n         \n           epochs-per-eval \n           Number of epochs to run in between evaluations. \n           \n            \n             10 \n            \n           \n         \n        \n         \n           resnet-size \n           Size of the ResNet model to use (18, 34, 50, 101, 152, or 200). \n           \n            \n             50",
            "title": "train"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-imagenet-references",
            "text": "https://github.com/tensorflow/models/tree/v.1.6.0/official/resnet \n      \n       https://arxiv.org/pdf/1512.03385.pdf \n      \n       https://arxiv.org/pdf/1603.05027.pdf",
            "title": "References"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-lib",
            "text": "Python library for tensorflow.resnet.",
            "title": "resnet-lib resource"
        },
        {
            "location": "/models/tensorflow/resnet/#resnet-lib-sources",
            "text": "https://github.com/tensorflow/models/archive/v.1.6.0.zip",
            "title": "Sources"
        },
        {
            "location": "/models/tensorflow/wide-deep/",
            "text": "TensorFlow Wide and Deep\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \ntensorflow.wide-deep\n\n    \n\n    \n\n      \nDescription\n\n      \nWide and deep income predictor for Census Income data\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/tensorflow/wide-deep\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nwide-deep-census\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nwide-deep-census model\n\n\n  \nWide and deep income predictor for Census Income data.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nprepare\n\n    \nDownload and prepare the Census Income data.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a wide and deep model on Census Income data.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of examples per batch.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs\n\n          \nNumber of training epochs.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nepochs-per-eval\n\n          \nNumber of training epochs to run between evaluations.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \nmodel-type\n\n          \nModel type (wide, deep, wide_deep).",
            "title": "TensorFlow Wide and Deep"
        },
        {
            "location": "/models/tensorflow/wide-deep/#tensorflow-wide-and-deep",
            "text": "Name \n       tensorflow.wide-deep \n     \n     \n       Description \n       Wide and deep income predictor for Census Income data \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/tensorflow/wide-deep \n     \n     \n       Maintainer",
            "title": "TensorFlow Wide and Deep"
        },
        {
            "location": "/models/tensorflow/wide-deep/#models",
            "text": "wide-deep-census",
            "title": "Models"
        },
        {
            "location": "/models/tensorflow/wide-deep/#wide-deep-census",
            "text": "Wide and deep income predictor for Census Income data. \n\n   \n     Operations",
            "title": "wide-deep-census model"
        },
        {
            "location": "/models/tensorflow/wide-deep/#wide-deep-census-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/tensorflow/wide-deep/#wide-deep-census-prepare",
            "text": "Download and prepare the Census Income data. \n\n     \n      \n       This operation does not have any flags.",
            "title": "prepare"
        },
        {
            "location": "/models/tensorflow/wide-deep/#wide-deep-census-train",
            "text": "Train a wide and deep model on Census Income data. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of examples per batch. \n           \n            \n           \n         \n        \n         \n           epochs \n           Number of training epochs. \n           \n            \n           \n         \n        \n         \n           epochs-per-eval \n           Number of training epochs to run between evaluations. \n           \n            \n           \n         \n        \n         \n           model-type \n           Model type (wide, deep, wide_deep).",
            "title": "train"
        },
        {
            "location": "/models/slim/",
            "text": "TF Slim\n\n\nTensorFlow Slim is a high level API to TensorFlow. There are a number\nof excellent models in the \nTensorFlow model zoo\n, many of which have\nbeen adapted as Guild packages.\n\n\n\n\n\n\nPackages\n\n\nInception\nResNet\nVGG",
            "title": "TF Slim"
        },
        {
            "location": "/models/slim/#tf-slim",
            "text": "TensorFlow Slim is a high level API to TensorFlow. There are a number\nof excellent models in the  TensorFlow model zoo , many of which have\nbeen adapted as Guild packages.",
            "title": "TF Slim"
        },
        {
            "location": "/models/slim/#packages",
            "text": "Inception ResNet VGG",
            "title": "Packages"
        },
        {
            "location": "/models/slim/inception/",
            "text": "TF Slim Inception\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nslim.inception\n\n    \n\n    \n\n      \nDescription\n\n      \nTF-Slim Inception models (v1, v2, v3, v4, and Inception ResNet v2)\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/slim/inception\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nslim-inception-resnet-v2\n\n    \n    \nslim-inception-v1\n\n    \n    \nslim-inception-v2\n\n    \n    \nslim-inception-v3\n\n    \n    \nslim-inception-v4\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-resnet-v2 model\n\n\n  \nInception ResNet v2 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception ResNet v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception ResNet v2 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception ResNet v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception ResNet v2 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception ResNet v2 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception ResNet v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception ResNet v2 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception ResNet v2 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception ResNet v2 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception ResNet v2 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_resnet_v2.py\n\n      \n      \nhttp://arxiv.org/abs/1602.07261\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-v1 model\n\n\n  \nInception v1 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception v1 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception v1 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception v1 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception v1 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception v1 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception v1 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception v1 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception v1 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception v1 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception v1 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_v1.py\n\n      \n      \nhttp://arxiv.org/pdf/1409.4842v1.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-v2 model\n\n\n  \nInception v2 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception v2 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception v2 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception v2 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception v2 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception v2 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_v2_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception v2 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception v2 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception v2 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_v2.py\n\n      \n      \nhttp://arxiv.org/abs/1502.03167\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-v3 model\n\n\n  \nInception v3 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception v3 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception v3 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception v3 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception v3 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception v3 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception v3 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception v3 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception v3 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception v3 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception v3 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_v3.py\n\n      \n      \nhttp://arxiv.org/abs/1512.00567\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-inception-v4 model\n\n\n  \nInception v4 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained Inception v4 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a Inception v4 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a Inception v4 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a Inception v4 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and Inception v4 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a Inception v4 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained Inception v4 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported Inception v4 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen Inception v4 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained Inception v4 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/inception_v4.py\n\n      \n      \nhttp://arxiv.org/abs/1602.07261",
            "title": "TF Slim Inception"
        },
        {
            "location": "/models/slim/inception/#tf-slim-inception",
            "text": "Name \n       slim.inception \n     \n     \n       Description \n       TF-Slim Inception models (v1, v2, v3, v4, and Inception ResNet v2) \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/slim/inception \n     \n     \n       Maintainer",
            "title": "TF Slim Inception"
        },
        {
            "location": "/models/slim/inception/#models",
            "text": "slim-inception-resnet-v2 \n    \n     slim-inception-v1 \n    \n     slim-inception-v2 \n    \n     slim-inception-v3 \n    \n     slim-inception-v4",
            "title": "Models"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2",
            "text": "Inception ResNet v2 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-inception-resnet-v2 model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-evaluate",
            "text": "Evaluate a trained Inception ResNet v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-export",
            "text": "Generate a Inception ResNet v2 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-finetune",
            "text": "Fine tune a Inception ResNet v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-freeze",
            "text": "Generate a Inception ResNet v2 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-predict",
            "text": "Use TensorFlow label_image and Inception ResNet v2 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-train",
            "text": "Train a Inception ResNet v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-checkpoint-res",
            "text": "Pretrained Inception ResNet v2 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-export-res",
            "text": "Exported Inception ResNet v2 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-frozen-model-res",
            "text": "Frozen Inception ResNet v2 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-trained-model-res",
            "text": "Trained Inception ResNet v2 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-resnet-v2-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_resnet_v2.py \n      \n       http://arxiv.org/abs/1602.07261",
            "title": "References"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1",
            "text": "Inception v1 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-inception-v1 model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-evaluate",
            "text": "Evaluate a trained Inception v1 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-export",
            "text": "Generate a Inception v1 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-finetune",
            "text": "Fine tune a Inception v1 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-freeze",
            "text": "Generate a Inception v1 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-predict",
            "text": "Use TensorFlow label_image and Inception v1 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-train",
            "text": "Train a Inception v1 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-checkpoint-res",
            "text": "Pretrained Inception v1 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-export-res",
            "text": "Exported Inception v1 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-frozen-model-res",
            "text": "Frozen Inception v1 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-trained-model-res",
            "text": "Trained Inception v1 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v1-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_v1.py \n      \n       http://arxiv.org/pdf/1409.4842v1.pdf",
            "title": "References"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2",
            "text": "Inception v2 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-inception-v2 model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-evaluate",
            "text": "Evaluate a trained Inception v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-export",
            "text": "Generate a Inception v2 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-finetune",
            "text": "Fine tune a Inception v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-freeze",
            "text": "Generate a Inception v2 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-predict",
            "text": "Use TensorFlow label_image and Inception v2 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-train",
            "text": "Train a Inception v2 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-checkpoint-res",
            "text": "Pretrained Inception v2 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_v2_2016_08_28.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-export-res",
            "text": "Exported Inception v2 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-frozen-model-res",
            "text": "Frozen Inception v2 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-trained-model-res",
            "text": "Trained Inception v2 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v2-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_v2.py \n      \n       http://arxiv.org/abs/1502.03167",
            "title": "References"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3",
            "text": "Inception v3 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-inception-v3 model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-evaluate",
            "text": "Evaluate a trained Inception v3 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-export",
            "text": "Generate a Inception v3 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-finetune",
            "text": "Fine tune a Inception v3 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-freeze",
            "text": "Generate a Inception v3 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-predict",
            "text": "Use TensorFlow label_image and Inception v3 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-train",
            "text": "Train a Inception v3 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-checkpoint-res",
            "text": "Pretrained Inception v3 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-export-res",
            "text": "Exported Inception v3 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-frozen-model-res",
            "text": "Frozen Inception v3 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-trained-model-res",
            "text": "Trained Inception v3 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v3-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_v3.py \n      \n       http://arxiv.org/abs/1512.00567",
            "title": "References"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4",
            "text": "Inception v4 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-inception-v4 model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-evaluate",
            "text": "Evaluate a trained Inception v4 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-export",
            "text": "Generate a Inception v4 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-finetune",
            "text": "Fine tune a Inception v4 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-freeze",
            "text": "Generate a Inception v4 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-predict",
            "text": "Use TensorFlow label_image and Inception v4 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-train",
            "text": "Train a Inception v4 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-checkpoint-res",
            "text": "Pretrained Inception v4 model. \n     \n       \n        \n         http://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-export-res",
            "text": "Exported Inception v4 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-frozen-model-res",
            "text": "Frozen Inception v4 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-trained-model-res",
            "text": "Trained Inception v4 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/inception/#slim-inception-v4-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/inception_v4.py \n      \n       http://arxiv.org/abs/1602.07261",
            "title": "References"
        },
        {
            "location": "/models/slim/resnet/",
            "text": "TF Slim ResNet\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nslim.resnet\n\n    \n\n    \n\n      \nDescription\n\n      \nTF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2)\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/slim/resnet\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nslim-resnet-101\n\n    \n    \nslim-resnet-152\n\n    \n    \nslim-resnet-200\n\n    \n    \nslim-resnet-50\n\n    \n    \nslim-resnet-v2-101\n\n    \n    \nslim-resnet-v2-152\n\n    \n    \nslim-resnet-v2-200\n\n    \n    \nslim-resnet-v2-50\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-101 model\n\n\n  \nResNet-101 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-101 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-101 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-101 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-101 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-101 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-101 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-101 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v1.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-152 model\n\n\n  \nResNet-152 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-152 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-152 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-152 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-152 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-152 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-152 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-152 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v1.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-200 model\n\n\n  \nResNet-200 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-200 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-200 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-200 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-200 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-200 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-200 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-200 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-200 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v1.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-50 model\n\n\n  \nResNet-50 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-50 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-50 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-50 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-50 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-50 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-50 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-50 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v1.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-v2-101 model\n\n\n  \nResNet-v2-101 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-v2-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-v2-101 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-v2-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-v2-101 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-v2-101 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-v2-101 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-v2-101 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v2_101_2017_04_14.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-v2-101 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-v2-101 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-v2-101 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v2.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-v2-152 model\n\n\n  \nResNet-v2-152 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-v2-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-v2-152 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-v2-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-v2-152 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-v2-152 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-v2-152 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-v2-152 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v2_152_2017_04_14.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-v2-152 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-v2-152 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-v2-152 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v2.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-v2-200 model\n\n\n  \nResNet-v2-200 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-v2-200 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-v2-200 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-v2-200 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-v2-200 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-v2-200 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-v2-200 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-v2-200 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-v2-200 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v2.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-resnet-v2-50 model\n\n\n  \nResNet-v2-50 classifier for TF-Slim.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained ResNet-v2-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a ResNet-v2-50 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a ResNet-v2-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a ResNet-v2-50 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and ResNet-v2-50 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a ResNet-v2-50 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained ResNet-v2-50 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/resnet_v2_50_2017_04_14.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported ResNet-v2-50 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen ResNet-v2-50 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained ResNet-v2-50 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/resnet_v2.py\n\n      \n      \narXiv: 1512.03385\n\n      \n      \narXiv: 1603.05027",
            "title": "TF Slim ResNet"
        },
        {
            "location": "/models/slim/resnet/#tf-slim-resnet",
            "text": "Name \n       slim.resnet \n     \n     \n       Description \n       TF-Slim ResNet models (50, 101, 152, and 200 layer models for ResNet v1 and v2) \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/slim/resnet \n     \n     \n       Maintainer",
            "title": "TF Slim ResNet"
        },
        {
            "location": "/models/slim/resnet/#models",
            "text": "slim-resnet-101 \n    \n     slim-resnet-152 \n    \n     slim-resnet-200 \n    \n     slim-resnet-50 \n    \n     slim-resnet-v2-101 \n    \n     slim-resnet-v2-152 \n    \n     slim-resnet-v2-200 \n    \n     slim-resnet-v2-50",
            "title": "Models"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101",
            "text": "ResNet-101 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-resnet-101 model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-evaluate",
            "text": "Evaluate a trained ResNet-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-export",
            "text": "Generate a ResNet-101 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-finetune",
            "text": "Fine tune a ResNet-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-freeze",
            "text": "Generate a ResNet-101 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-predict",
            "text": "Use TensorFlow label_image and ResNet-101 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-train",
            "text": "Train a ResNet-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-checkpoint-res",
            "text": "Pretrained ResNet-101 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-export-res",
            "text": "Exported ResNet-101 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-frozen-model-res",
            "text": "Frozen ResNet-101 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-trained-model-res",
            "text": "Trained ResNet-101 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-101-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v1.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027",
            "title": "References"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152",
            "text": "ResNet-152 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-resnet-152 model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-evaluate",
            "text": "Evaluate a trained ResNet-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-export",
            "text": "Generate a ResNet-152 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-finetune",
            "text": "Fine tune a ResNet-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-freeze",
            "text": "Generate a ResNet-152 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-predict",
            "text": "Use TensorFlow label_image and ResNet-152 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-train",
            "text": "Train a ResNet-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-checkpoint-res",
            "text": "Pretrained ResNet-152 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-export-res",
            "text": "Exported ResNet-152 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-frozen-model-res",
            "text": "Frozen ResNet-152 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-trained-model-res",
            "text": "Trained ResNet-152 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-152-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v1.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027",
            "title": "References"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200",
            "text": "ResNet-200 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-resnet-200 model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-evaluate",
            "text": "Evaluate a trained ResNet-200 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-export",
            "text": "Generate a ResNet-200 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-freeze",
            "text": "Generate a ResNet-200 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-predict",
            "text": "Use TensorFlow label_image and ResNet-200 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-train",
            "text": "Train a ResNet-200 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-export-res",
            "text": "Exported ResNet-200 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-frozen-model-res",
            "text": "Frozen ResNet-200 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-trained-model-res",
            "text": "Trained ResNet-200 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-200-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v1.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027",
            "title": "References"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50",
            "text": "ResNet-50 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-resnet-50 model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-evaluate",
            "text": "Evaluate a trained ResNet-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-export",
            "text": "Generate a ResNet-50 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-finetune",
            "text": "Fine tune a ResNet-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-freeze",
            "text": "Generate a ResNet-50 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-predict",
            "text": "Use TensorFlow label_image and ResNet-50 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-train",
            "text": "Train a ResNet-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-checkpoint-res",
            "text": "Pretrained ResNet-50 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-export-res",
            "text": "Exported ResNet-50 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-frozen-model-res",
            "text": "Frozen ResNet-50 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-trained-model-res",
            "text": "Trained ResNet-50 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-50-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v1.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027",
            "title": "References"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101",
            "text": "ResNet-v2-101 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-resnet-v2-101 model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-evaluate",
            "text": "Evaluate a trained ResNet-v2-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-export",
            "text": "Generate a ResNet-v2-101 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-finetune",
            "text": "Fine tune a ResNet-v2-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-freeze",
            "text": "Generate a ResNet-v2-101 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-predict",
            "text": "Use TensorFlow label_image and ResNet-v2-101 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-train",
            "text": "Train a ResNet-v2-101 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-checkpoint-res",
            "text": "Pretrained ResNet-v2-101 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v2_101_2017_04_14.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-export-res",
            "text": "Exported ResNet-v2-101 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-frozen-model-res",
            "text": "Frozen ResNet-v2-101 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-trained-model-res",
            "text": "Trained ResNet-v2-101 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-101-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v2.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027",
            "title": "References"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152",
            "text": "ResNet-v2-152 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-resnet-v2-152 model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-evaluate",
            "text": "Evaluate a trained ResNet-v2-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-export",
            "text": "Generate a ResNet-v2-152 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-finetune",
            "text": "Fine tune a ResNet-v2-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-freeze",
            "text": "Generate a ResNet-v2-152 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-predict",
            "text": "Use TensorFlow label_image and ResNet-v2-152 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-train",
            "text": "Train a ResNet-v2-152 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-checkpoint-res",
            "text": "Pretrained ResNet-v2-152 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v2_152_2017_04_14.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-export-res",
            "text": "Exported ResNet-v2-152 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-frozen-model-res",
            "text": "Frozen ResNet-v2-152 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-trained-model-res",
            "text": "Trained ResNet-v2-152 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-152-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v2.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027",
            "title": "References"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200",
            "text": "ResNet-v2-200 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-resnet-v2-200 model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-evaluate",
            "text": "Evaluate a trained ResNet-v2-200 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-export",
            "text": "Generate a ResNet-v2-200 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-freeze",
            "text": "Generate a ResNet-v2-200 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-predict",
            "text": "Use TensorFlow label_image and ResNet-v2-200 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-train",
            "text": "Train a ResNet-v2-200 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-export-res",
            "text": "Exported ResNet-v2-200 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-frozen-model-res",
            "text": "Frozen ResNet-v2-200 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-trained-model-res",
            "text": "Trained ResNet-v2-200 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-200-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v2.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027",
            "title": "References"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50",
            "text": "ResNet-v2-50 classifier for TF-Slim. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-resnet-v2-50 model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-evaluate",
            "text": "Evaluate a trained ResNet-v2-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-export",
            "text": "Generate a ResNet-v2-50 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-finetune",
            "text": "Fine tune a ResNet-v2-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-freeze",
            "text": "Generate a ResNet-v2-50 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-predict",
            "text": "Use TensorFlow label_image and ResNet-v2-50 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-train",
            "text": "Train a ResNet-v2-50 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-checkpoint-res",
            "text": "Pretrained ResNet-v2-50 model. \n     \n       \n        \n         http://download.tensorflow.org/models/resnet_v2_50_2017_04_14.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-export-res",
            "text": "Exported ResNet-v2-50 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-frozen-model-res",
            "text": "Frozen ResNet-v2-50 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-trained-model-res",
            "text": "Trained ResNet-v2-50 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/resnet/#slim-resnet-v2-50-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/resnet_v2.py \n      \n       arXiv: 1512.03385 \n      \n       arXiv: 1603.05027",
            "title": "References"
        },
        {
            "location": "/models/slim/vgg/",
            "text": "TF Slim VGG\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nslim.vgg\n\n    \n\n    \n\n      \nDescription\n\n      \nOxford VGG models (11-Layers verion A, 16-Layers version D, 19-Layers version E)\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/slim/vgg\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \nslim-vgg-11\n\n    \n    \nslim-vgg-16\n\n    \n    \nslim-vgg-19\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-vgg-11 model\n\n\n  \nOxford VGG 11-Layers version A model.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained VGG-11 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a VGG-11 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a VGG-11 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and VGG-11 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a VGG-11 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported VGG-11 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen VGG-11 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained VGG-11 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/vgg.py\n\n      \n      \nhttp://arxiv.org/pdf/1409.1556.pdf\n\n      \n      \nhttp://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-vgg-16 model\n\n\n  \nOxford VGG 16-Layers version D model.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained VGG-16 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a VGG-16 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a VGG-16 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a VGG-16 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and VGG-16 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a VGG-16 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained VGG-16 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported VGG-16 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen VGG-16 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained VGG-16 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/vgg.py\n\n      \n      \nhttp://arxiv.org/pdf/1409.1556.pdf\n\n      \n      \nhttp://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n\n      \n    \n\n  \n\n  \n\n  \n\n  \n\n  \nslim-vgg-19 model\n\n\n  \nOxford VGG 19-Layers version E model.\n\n\n  \n\n    \nOperations\n\n    \n    \nResources\n\n    \n    \n    \nReferences\n\n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nevaluate\n\n    \nEvaluate a trained VGG-19 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-batches\n\n          \nMaximum number of batches to evaluate (default is all).\n\n          \n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nexport\n\n    \nGenerate a VGG-19 graph def.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfinetune\n\n    \nFine tune a VGG-19 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \nfreeze\n\n    \nGenerate a VGG-19 graph def with checkpoint weights.\n\n\n    \n\n      \n      \nThis operation does not have any flags.\n\n      \n    \n\n\n    \n\n    \npredict\n\n    \nUse TensorFlow label_image and VGG-19 to classify an image.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset name to use for labels and image transformation.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nimage\n\n          \nPath to the input image.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-mean\n\n          \nImage mean to apply to the image.\n\n          \n\n            \n            \n0.0\n\n            \n          \n\n        \n\n        \n        \n\n          \ninput-std\n\n          \nImage std deviation to apply to the image.\n\n          \n\n            \n            \n1.0\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n\n    \ntrain\n\n    \nTrain a VGG-19 model.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \nbatch-size\n\n          \nNumber of samples in each batch.\n\n          \n\n            \n            \n32\n\n            \n          \n\n        \n\n        \n        \n\n          \ncheckpoint\n\n          \nRun ID or path to checkpoint to resume training from.\n\n          \n\n            \n          \n\n        \n\n        \n        \n\n          \ndataset\n\n          \nDataset to train with (cifar10, mnist, flowers, custom).\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate\n\n          \nInitial learning rate.\n\n          \n\n            \n            \n0.01\n\n            \n          \n\n        \n\n        \n        \n\n          \nlearning-rate-decay-type\n\n          \nHow the learning rate is decayed.\n\n          \n\n            \n            \nexponential\n\n            \n          \n\n        \n\n        \n        \n\n          \nlog-every-n-steps\n\n          \nSteps between status updates.\n\n          \n\n            \n            \n100\n\n            \n          \n\n        \n\n        \n        \n\n          \nmax-steps\n\n          \nMaximum number of training steps.\n\n          \n\n            \n            \n1000\n\n            \n          \n\n        \n\n        \n        \n\n          \noptimizer\n\n          \nTraining optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop).\n\n          \n\n            \n            \nrmsprop\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-model-secs\n\n          \nSeconds between model saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nsave-summaries-secs\n\n          \nSeconds between summary saves.\n\n          \n\n            \n            \n60\n\n            \n          \n\n        \n\n        \n        \n\n          \nweight-decay\n\n          \nWeight decay on the model weights.\n\n          \n\n            \n            \n4e-05\n\n            \n          \n\n        \n\n        \n      \n\n      \n    \n\n\n    \n  \n\n\n  \n  \nResources\n\n  \n\n    \n    \ncheckpoint\n\n    \nPretrained VGG-19 model.\n\n    \n\n      \n\n        \n        \nhttp://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz\n\n        \n      \n\n    \n\n    \n    \ncifar10\n\n    \nPrepared CIFAR-10 dataset (slim-cifar10:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-cifar10:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ncustom\n\n    \nPrepared custom dataset (slim-custom:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-custom-images:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nexport\n\n    \nExported VGG-19 graph def from export operation.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nexport\n operation\n\n        \n      \n\n    \n\n    \n    \nflowers\n\n    \nPrepared Flowers dataset (slim-flowers:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-flowers:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \nfrozen-model\n\n    \nFrozen VGG-19 graph with weights.\n\n    \n\n      \n\n        \n        \n['graph.pb']\n from \nfreeze\n operation\n\n        \n      \n\n    \n\n    \n    \nmnist\n\n    \nPrepared MNIST dataset (slim-mnist:prepare operation).\n\n    \n\n      \n\n        \n        \n['data']\n from \nslim-mnist:prepare\n operation\n\n        \n      \n\n    \n\n    \n    \ntrained-model\n\n    \nTrained VGG-19 model (train operation).\n\n    \n\n      \n\n        \n        \n['checkpoint|model\\\\.ckpt.*']\n from \ntrain\n or \nfinetune\n operations\n\n        \n      \n\n    \n\n    \n  \n\n  \n\n  \n  \nReferences\n\n  \n\n    \n\n      \n      \nGitHub: tensorflow/models/research/slim/nets/vgg.py\n\n      \n      \nhttp://arxiv.org/pdf/1409.1556.pdf\n\n      \n      \nhttp://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf",
            "title": "TF Slim VGG"
        },
        {
            "location": "/models/slim/vgg/#tf-slim-vgg",
            "text": "Name \n       slim.vgg \n     \n     \n       Description \n       Oxford VGG models (11-Layers verion A, 16-Layers version D, 19-Layers version E) \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/slim/vgg \n     \n     \n       Maintainer",
            "title": "TF Slim VGG"
        },
        {
            "location": "/models/slim/vgg/#models",
            "text": "slim-vgg-11 \n    \n     slim-vgg-16 \n    \n     slim-vgg-19",
            "title": "Models"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11",
            "text": "Oxford VGG 11-Layers version A model. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-vgg-11 model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-evaluate",
            "text": "Evaluate a trained VGG-11 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-export",
            "text": "Generate a VGG-11 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-freeze",
            "text": "Generate a VGG-11 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-predict",
            "text": "Use TensorFlow label_image and VGG-11 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-train",
            "text": "Train a VGG-11 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-export-res",
            "text": "Exported VGG-11 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-frozen-model-res",
            "text": "Frozen VGG-11 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-trained-model-res",
            "text": "Trained VGG-11 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-11-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/vgg.py \n      \n       http://arxiv.org/pdf/1409.1556.pdf \n      \n       http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf",
            "title": "References"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16",
            "text": "Oxford VGG 16-Layers version D model. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-vgg-16 model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-evaluate",
            "text": "Evaluate a trained VGG-16 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-export",
            "text": "Generate a VGG-16 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-finetune",
            "text": "Fine tune a VGG-16 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-freeze",
            "text": "Generate a VGG-16 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-predict",
            "text": "Use TensorFlow label_image and VGG-16 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-train",
            "text": "Train a VGG-16 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-checkpoint-res",
            "text": "Pretrained VGG-16 model. \n     \n       \n        \n         http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-export-res",
            "text": "Exported VGG-16 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-frozen-model-res",
            "text": "Frozen VGG-16 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-trained-model-res",
            "text": "Trained VGG-16 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-16-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/vgg.py \n      \n       http://arxiv.org/pdf/1409.1556.pdf \n      \n       http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf",
            "title": "References"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19",
            "text": "Oxford VGG 19-Layers version E model. \n\n   \n     Operations \n    \n     Resources \n    \n    \n     References",
            "title": "slim-vgg-19 model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-evaluate",
            "text": "Evaluate a trained VGG-19 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           max-batches \n           Maximum number of batches to evaluate (default is all).",
            "title": "evaluate"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-export",
            "text": "Generate a VGG-19 graph def. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required",
            "title": "export"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-finetune",
            "text": "Fine tune a VGG-19 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "finetune"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-freeze",
            "text": "Generate a VGG-19 graph def with checkpoint weights. \n\n     \n      \n       This operation does not have any flags.",
            "title": "freeze"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-predict",
            "text": "Use TensorFlow label_image and VGG-19 to classify an image. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           dataset \n           Dataset name to use for labels and image transformation. \n           \n            \n             required \n            \n           \n         \n        \n         \n           image \n           Path to the input image. \n           \n            \n             required \n            \n           \n         \n        \n         \n           input-mean \n           Image mean to apply to the image. \n           \n            \n             0.0 \n            \n           \n         \n        \n         \n           input-std \n           Image std deviation to apply to the image. \n           \n            \n             1.0",
            "title": "predict"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-train",
            "text": "Train a VGG-19 model. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           batch-size \n           Number of samples in each batch. \n           \n            \n             32 \n            \n           \n         \n        \n         \n           checkpoint \n           Run ID or path to checkpoint to resume training from. \n           \n            \n           \n         \n        \n         \n           dataset \n           Dataset to train with (cifar10, mnist, flowers, custom). \n           \n            \n             required \n            \n           \n         \n        \n         \n           learning-rate \n           Initial learning rate. \n           \n            \n             0.01 \n            \n           \n         \n        \n         \n           learning-rate-decay-type \n           How the learning rate is decayed. \n           \n            \n             exponential \n            \n           \n         \n        \n         \n           log-every-n-steps \n           Steps between status updates. \n           \n            \n             100 \n            \n           \n         \n        \n         \n           max-steps \n           Maximum number of training steps. \n           \n            \n             1000 \n            \n           \n         \n        \n         \n           optimizer \n           Training optimizer (adadelta, adagrad, adam, ftrl, momentum, sgd, rmsprop). \n           \n            \n             rmsprop \n            \n           \n         \n        \n         \n           save-model-secs \n           Seconds between model saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           save-summaries-secs \n           Seconds between summary saves. \n           \n            \n             60 \n            \n           \n         \n        \n         \n           weight-decay \n           Weight decay on the model weights. \n           \n            \n             4e-05",
            "title": "train"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-checkpoint-res",
            "text": "Pretrained VGG-19 model. \n     \n       \n        \n         http://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz",
            "title": "checkpoint"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-cifar10-res",
            "text": "Prepared CIFAR-10 dataset (slim-cifar10:prepare operation). \n     \n       \n        \n         ['data']  from  slim-cifar10:prepare  operation",
            "title": "cifar10"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-custom-res",
            "text": "Prepared custom dataset (slim-custom:prepare operation). \n     \n       \n        \n         ['data']  from  slim-custom-images:prepare  operation",
            "title": "custom"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-export-res",
            "text": "Exported VGG-19 graph def from export operation. \n     \n       \n        \n         ['graph.pb']  from  export  operation",
            "title": "export"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-flowers-res",
            "text": "Prepared Flowers dataset (slim-flowers:prepare operation). \n     \n       \n        \n         ['data']  from  slim-flowers:prepare  operation",
            "title": "flowers"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-frozen-model-res",
            "text": "Frozen VGG-19 graph with weights. \n     \n       \n        \n         ['graph.pb']  from  freeze  operation",
            "title": "frozen-model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-mnist-res",
            "text": "Prepared MNIST dataset (slim-mnist:prepare operation). \n     \n       \n        \n         ['data']  from  slim-mnist:prepare  operation",
            "title": "mnist"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-trained-model-res",
            "text": "Trained VGG-19 model (train operation). \n     \n       \n        \n         ['checkpoint|model\\\\.ckpt.*']  from  train  or  finetune  operations",
            "title": "trained-model"
        },
        {
            "location": "/models/slim/vgg/#slim-vgg-19-references",
            "text": "GitHub: tensorflow/models/research/slim/nets/vgg.py \n      \n       http://arxiv.org/pdf/1409.1556.pdf \n      \n       http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf",
            "title": "References"
        },
        {
            "location": "/models/wikipedia/datasets/",
            "text": "Wikipedia Datasets\n\n\n\n\n  \n\n    \n\n      \nName\n\n      \nwikipedia.datasets\n\n    \n\n    \n\n      \nDescription\n\n      \nSupport for downloading images from Wikipedia Commons\n\n    \n\n    \n\n      \nVersion\n\n      \n0.3.1\n\n    \n\n    \n\n      \nSource\n\n      \nhttps://github.com/guildai/index/tree/master/wikipedia/commons\n\n    \n\n    \n\n      \nMaintainer\n\n      \n\n    \n\n  \n\n\n  \n  \n\n    \n\n      \nModels\n\n    \n    \ncommons-images\n\n    \n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \ncommons-images model\n\n\n  \nSupport for downloading images from Wikipedia Commons.\n\n\n  \n\n    \nOperations\n\n    \n    \n  \n\n\n  \nOperations\n\n\n  \n\n    \n\n    \nprepare\n\n    \nDownload images from Wikipedia Commons\nSpecify the category to download. See https://commons.wikimedia.org/wiki/Category:Images for a list of categories.\n\n\n    \n\n      \n      \n\n        \n\n          \nFlag\n\n          \nDescription\n\n          \nDefault\n\n        \n\n        \n        \n\n          \ncategory\n\n          \nCategory of images to download.\n\n          \n\n            \n            \nrequired\n\n            \n          \n\n        \n\n        \n        \n\n          \nwidth\n\n          \nImage width.\n\n          \n\n            \n            \n100",
            "title": "Wikipedia Datasets"
        },
        {
            "location": "/models/wikipedia/datasets/#wikipedia-datasets",
            "text": "Name \n       wikipedia.datasets \n     \n     \n       Description \n       Support for downloading images from Wikipedia Commons \n     \n     \n       Version \n       0.3.1 \n     \n     \n       Source \n       https://github.com/guildai/index/tree/master/wikipedia/commons \n     \n     \n       Maintainer",
            "title": "Wikipedia Datasets"
        },
        {
            "location": "/models/wikipedia/datasets/#models",
            "text": "commons-images",
            "title": "Models"
        },
        {
            "location": "/models/wikipedia/datasets/#commons-images",
            "text": "Support for downloading images from Wikipedia Commons. \n\n   \n     Operations",
            "title": "commons-images model"
        },
        {
            "location": "/models/wikipedia/datasets/#commons-images-operations",
            "text": "",
            "title": "Operations"
        },
        {
            "location": "/models/wikipedia/datasets/#commons-images-prepare",
            "text": "Download images from Wikipedia Commons\nSpecify the category to download. See https://commons.wikimedia.org/wiki/Category:Images for a list of categories. \n\n     \n      \n       \n         \n           Flag \n           Description \n           Default \n         \n        \n         \n           category \n           Category of images to download. \n           \n            \n             required \n            \n           \n         \n        \n         \n           width \n           Image width. \n           \n            \n             100",
            "title": "prepare"
        },
        {
            "location": "/support/",
            "text": "Support\n\n\nTo resolve an issue you\u2019re having with Guild AI, search this\ndocumentation or review the links below \u2014 there\u2019s a good chance your\ntopic has been covered here.\n\n\nIf you\u2019re still facing issues, \nopen an issue on GitHub\n and we\u2019ll help!\n\n\n\n\n\n\nLinks\n\n\nDocumentation\nTroubleshooting\nFAQ",
            "title": "Support"
        },
        {
            "location": "/support/#support",
            "text": "To resolve an issue you\u2019re having with Guild AI, search this\ndocumentation or review the links below \u2014 there\u2019s a good chance your\ntopic has been covered here.  If you\u2019re still facing issues,  open an issue on GitHub  and we\u2019ll help!",
            "title": "Support"
        },
        {
            "location": "/support/#links",
            "text": "Documentation Troubleshooting FAQ",
            "title": "Links"
        },
        {
            "location": "/faq/",
            "text": "FAQ\n\n\n\n\nManage runs\n\n\nHow do I quickly delete failed runs?\n\n\n\n\n\n\nResources\n\n\nIf a source is referenced multiple times, does Guild download each occurrence?\n\n\n\n\n\n\nRuntime characteristics\n\n\nHow much overhead does Guild incur when running an operation?\n\n\n\n\n\n\nTroubleshooting\n\n\nHow do I know which library version I\u2019m using?\n\n\n\n\n\n\n\n\nManage runs\n\n\nHow do I quickly delete failed runs?\n\n\nGuild saves every run whether it succeeds or not. This lets you\ntroubleshoot issues But over time failed runs can accumulate and\nyou\u2019ll want to delete them.\n\n\nUse this command to delete failed runs:\n\n\nguild runs delete --error\n\n\n\n\nUse this command to delete terminated runs (i.e. runs that were\nstopped by the user by typing \nCTRL-c\n):\n\n\nguild runs delete --terminated\n\n\n\n\nYou can delete both failed and terminated runs by using both\n\n--error\n and \n--terminated\n options at the same time, or using\nthis short form:\n\n\nguild runs delete -ET\n\n\n\n\nGuild will let you confirm the list of runs before deleting them.\n\n\nYou can later restore a deleted run using (cmd:runs-restore)[runs\nrestore].\n\n\nResources\n\n\nIf a source is referenced multiple times, does Guild download each occurrence?\n\n\nNo, Guild will only download the source once. There is no performance\npenalty for referencing a resource source multiple times.\n\n\nRuntime characteristics\n\n\nHow much overhead does Guild incur when running an operation?\n\n\nGuild runs operations in a separate OS process to ensure that the\noperation is isolated. As of Guild 0.3.0, the additional overhead\nincurred when running an operation is as follows:\n\n\n\n\n\n\nAdditional time: typically less than 100 milliseconds but may be\n  more on slower systems or loaded systems\n\n\n\n\n\n\nAdditional resident RAM: less than 40 MB\n\n\n\n\n\n\nTroubleshooting\n\n\nHow do I know which library version I\u2019m using?\n\n\nThe \ncheck\n command shows software library versions:\n\n\n\n\nGuild AI\n\n\nPython\n\n\nTensorFlow\n\n\nCUDA and cuDNN\n\n\n\n\nTo show this information, run:\n\n\nguild check\n\n\n\n\nTo show more information, use the \n--verbose\n option:\n\n\nguild check --verbose",
            "title": "FAQ"
        },
        {
            "location": "/faq/#faq",
            "text": "Manage runs  How do I quickly delete failed runs?    Resources  If a source is referenced multiple times, does Guild download each occurrence?    Runtime characteristics  How much overhead does Guild incur when running an operation?    Troubleshooting  How do I know which library version I\u2019m using?",
            "title": "FAQ"
        },
        {
            "location": "/faq/#manage-runs",
            "text": "",
            "title": "Manage runs"
        },
        {
            "location": "/faq/#how-do-i-quickly-delete-failed-runs",
            "text": "Guild saves every run whether it succeeds or not. This lets you\ntroubleshoot issues But over time failed runs can accumulate and\nyou\u2019ll want to delete them.  Use this command to delete failed runs:  guild runs delete --error  Use this command to delete terminated runs (i.e. runs that were\nstopped by the user by typing  CTRL-c ):  guild runs delete --terminated  You can delete both failed and terminated runs by using both --error  and  --terminated  options at the same time, or using\nthis short form:  guild runs delete -ET  Guild will let you confirm the list of runs before deleting them.  You can later restore a deleted run using (cmd:runs-restore)[runs\nrestore].",
            "title": "How do I quickly delete failed runs?"
        },
        {
            "location": "/faq/#resources",
            "text": "",
            "title": "Resources"
        },
        {
            "location": "/faq/#if-a-source-is-referenced-multiple-times-does-guild-download-each-occurrence",
            "text": "No, Guild will only download the source once. There is no performance\npenalty for referencing a resource source multiple times.",
            "title": "If a source is referenced multiple times, does Guild download each occurrence?"
        },
        {
            "location": "/faq/#runtime-characteristics",
            "text": "",
            "title": "Runtime characteristics"
        },
        {
            "location": "/faq/#how-much-overhead-does-guild-incur-when-running-an-operation",
            "text": "Guild runs operations in a separate OS process to ensure that the\noperation is isolated. As of Guild 0.3.0, the additional overhead\nincurred when running an operation is as follows:    Additional time: typically less than 100 milliseconds but may be\n  more on slower systems or loaded systems    Additional resident RAM: less than 40 MB",
            "title": "How much overhead does Guild incur when running an operation?"
        },
        {
            "location": "/faq/#troubleshooting",
            "text": "",
            "title": "Troubleshooting"
        },
        {
            "location": "/faq/#how-do-i-know-which-library-version-im-using",
            "text": "The  check  command shows software library versions:   Guild AI  Python  TensorFlow  CUDA and cuDNN   To show this information, run:  guild check  To show more information, use the  --verbose  option:  guild check --verbose",
            "title": "How do I know which library version I&rsquo;m using?"
        },
        {
            "location": "/about/",
            "text": "Guild AI\n\n\nLinks\n\n\nGitHub project\n\n\nContributing\n\n\nGuild AI does not currently have a contributors policy but we are\nhappy to work with you to add features that you need and resolve\nissues.\n\n\nBefore spending time on a pull request, please\n\nopen an issue on GitHub\n to let us know what problems you\u2019d like to\nsolve and we\u2019ll help identify next-steps with you!",
            "title": "About"
        },
        {
            "location": "/about/#guild-ai",
            "text": "",
            "title": "Guild AI"
        },
        {
            "location": "/about/#links",
            "text": "GitHub project",
            "title": "Links"
        },
        {
            "location": "/about/#contributing",
            "text": "Guild AI does not currently have a contributors policy but we are\nhappy to work with you to add features that you need and resolve\nissues.  Before spending time on a pull request, please open an issue on GitHub  to let us know what problems you\u2019d like to\nsolve and we\u2019ll help identify next-steps with you!",
            "title": "Contributing"
        }
    ]
}