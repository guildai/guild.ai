tags: tutorial, popular, intro

# Training and prediction with Cloud ML

[TOC]

In this tutorial, we'll train a model with Google's Cloud Machine
Learning Engine (Cloud ML) and use the trained model to make
prediction.

This tutorial mirrors the steps [Cloud ML Engine - Getting Started
->](https://cloud.google.com/ml-engine/docs/getting-started-training-prediction) and trains

## Requirements

This tutorial assumes the following:

- Guild AI is [installed and verified](/install)
- [Google Cloud SDK ->](https://cloud.google.com/sdk/docs/) is installed
- Your [virtual environment is activated](alias:virtualenv-activate)
  (if applicable)
- You have a working Internet connection

While not required, we recommend using a dedicated virtual environment
for this tutorial. To setup your environment, see
[](alias:tut-env-setup).

## Install the census package

We'll be training a model in the `cloudml.census` package. From a
command line, run:

``` command
guild install cloudml.census
```

You can list the model operations available from the package by
running:

``` command
guild ops cloudml.census
```

``` output
cloudml.census/census-dnn:cloudml-train  Train the classifier on Cloud ML
cloudml.census/census-dnn:train          Train the model locally
```

## Train the model locally

Let's first train the model locally. This will confirm that we can get
a reasonable result locally before training in Cloud ML.

Train the model using 1000 steps by running:

``` command
guild train census train-steps=1000
```

Before proceeding, Guild lets you review the flags used in the
run. Press `ENTER` to start training.

The training should finish on most systems within a minute or two.

After the run has finished, list it by running:

``` command
guild runs
```

## View training results

The training run generates a number of files. We can view them by
running:

``` command
guild runs info --files
```

The [runs info](cmd:runs-info) command can be used to view information
on any run. In this case, it shows information about the latest
run. The ``--files`` option tells Guild to include file information in
the output.

Let's take a look at the files:

`census-data/`
: Link to census data used for training and test

`checkpoint`
: Latest TensorFlow checkpoint marker

`eval_census-eval/events.out.tfevents.*.omaha`
: TensorFlow event log generated during evaluation

`events.out.tfevents.*.omaha`
: TensorFlow event log generated during training

`export/census/*/saved_model.pb`
: TensorFlow SavedModel

`export/census/*/variables/`
: TensorFlow variables

`graph.pbtxt`
: TensorFlow GraphDef

`model.ckpt-1.*`
: TensorFlow checkpoint at training step 1

`model.ckpt-1000.*`
: TensorFlow checkpoint at training step 1000

`trainer`
: Link to training scripts

We can also use Guild View to view this information. In a
[](alias:separate-console), run:

``` command
guild view
```

This opens a browser window showing you the list of runs for the
census classifier. You can see the list of files for a run by clicking
the **FILES** tab.

You can additionally view the TensorFlow logs by clicking ![View in
TensorBoard](/assets/img/view-in-tensorboard.png) in the upper left of
Guild View.

You can keep these windows open as they will automatically update as
you generate more runs.

Note the model **accuracy** in TensorBoard, which can be viewed under
the **SCALARS** tab. The value for our first run should be
approximately 80%.

## Label your run

Over time you'll generate a lot of runs and it's helpful to label them
for easy identification. Guild provides [runs label](cmd:runs-label),
which associates a run with a simple string.

Let's label our run to indicate that it was local and used 1,000
training steps:

``` command
guild runs label local-1000
```

Guild will confirm that you want to label the latest run. Press
`ENTER` to apply the label.

You can view the new label when you list runs:

``` command
guild runs
```

## Train the model in Cloud ML

Now that we've trained the model locally, let's train it in Cloud ML.

When training in Cloud ML, you need to create a Cloud Storage
bucket. This will contain the runs generated by Guild.

For instructions, see [Set up your Cloud Storage bucket
->](https://cloud.google.com/ml-engine/docs/getting-started-training-prediction#set_up_your_cloud_storage_bucket)
in the Cloud ML Getting Started guide.

We'll use the name of your Cloud Storage bucket several times in this
tutorial so let's create a variable for it:

``` command
BUCKET=<name-of-your-cloud-storage-bucket>
```

Verify that you can use `gsutil` to list the contents of your bucket:

``` command
guild gsutil ls $BUCKET
```

If you've successfully setup your bucket and have read access to it,
the command will list the contents of your bucket.

Train the census classifier in Cloud ML by running:

``` command
guild run census:cloudml-train bucket-name=$BUCKET train-steps=1000
```

!!! note
    In general, operations are run using the [](cmd:run)
    command. Earlier we used the [](cmd:train) command as an
    alternative way of running ``guild run train``. The `train`
    command is an [alias](term:operation-alias) for the `train`
    operation and can't be used for other operations. Because
    `cloudml-train` doesn't have an alias, we must use the `run`
    command.

The `cloudml-train` operation is identical to the `train` operation,
but it is run remotely on Cloud ML rather than locally. You need to
specify the `bucket-name` flag to let Guild know where to upload the
training files to.

The remote operation will take longer to run because Cloud ML must
first provision a job and wait for TensorFlow to start. This can take
several minutes or more. You can view the run status as it progresses
in the command line console. You can also view run status and log
updates from Guild View and TensorBoard.

Guild will automatically synchronize the run status with the remote
job. This includes generated files such as TensorFlow event logs and
saved models.

If Guild becomes disconnected from a remote job --- e.g. you terminate
the command or lose network connectivity --- the job will still run in
Cloud ML. You can synchornize the run status using the [](cmd:sync)
command. If you'd like to reconnect to a running job, use the
``--watch`` option with the command:

``` command
guild sync --watch
```

Once TensorFlow is started in Cloud ML, the model should train in
roughly the same time (within a few minutes).

When the run has finished, let's label it so we can easily identify it
later:

``` command
guild runs label cloud-1000
```

This tells us the model was trained in the cloud using 1000 steps.

## Compare runs

Each time you train a new model, you'll want to compare it to previous
runs. Guild's [](cmd:compare) command can be used to quickly sort and
compare training accuracies and loss.

Compare your two runs by running:

``` command
guild compare
```

!!! note
    You don't need to wait for a run to complete to compare
    it. If a run is still running, you can press ``r`` in Guild
    Compare to refresh the view with the latest accuracies and loss.

For a complete list of commands supported by Guild Compare, press
``?`` while it's running.

To exit Guild Compare, press ``q``.

If you'd like to export the comparison data in [CSV format
->](https://en.wikipedia.org/wiki/Comma-separated_values) run:

``` command
guild compare --csv
```

Next, let's compare run results in TensorBoard. If you still have
TensorBoard open from earlier, it will automatically refresh to
display the current training results. If you need to restart Guild
View, run:

``` command
guild view
```

Click **VIEW IN TENSORBOARD** to open TensorBoard.

You can alternatively open TensorBoard directly using the
[](cmd:tensorboard) command:

``` command
guild tensorboard
```

The accuracies from the two runs should be similar, provided you used
the same value for `train-steps` in both runs.

## Improve model accuracy

Let's try to improve the accuracy of the model with more training.

We can train the model again in Cloud ML but this time with more
training steps. Let's also use the ``--label`` option to label the run
when we start it:

``` command
guild run cloudml-train bucket-name=$BUCKET train-steps=10000 --label cloud-10000
```

We've increased our training steps from 1,000 to 10,000! While this is
a 10x increase, the training time in Cloud ML should still be under
ten minutes.

You can monitor the training progress using Guild View, Guild Compare
and TensorBoard.

As the training proceeds, note the increase in accuracy. 1,000
training steps should yield an accuracy of about 80%. 10,000 steps
should yield an accuracy around 84%.

Use Guild Compare to confirm:

``` command
guild compare
```

You should see that the run labeled `cloud-10000` (our latest run) has
a higher accuracy than the other runs, which used a lower `train-step`
value.

## Scale up

Let's take advantage of Cloud ML's ability to scale! We can train our
model using distributed TensorFlow and multiple workers.

We can use the `scale-tier` flag to specify a different set of
resources for running our operation.

See [Scale Tier
->](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#scaletier)
for more information on Cloud ML's supported environments.

By default, our Cloud ML operations use the `BASIC` scale tier, which
is suitable for smaller models and experimentation. Let's train again
but this time with the `STANDARD_1` scale tier. We'll also increase
our training steps to take advantage of the scaled environment.

``` command
guild run cloudml-train \
  bucket-name=$BUCKET \
  scale-tier=STANDARD_1 \
  train-steps=10000 \
  --label scaled-10000
```

This operation should take less time than our previous run of 10,000
steps (label `cloud-10000`). You can check the result using Guild Compare:

``` command
guild compare
```
