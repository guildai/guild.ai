tags: start

# Optimize a Model

[TOC]

## Overview

In the [previous section](/start/index.md), you use Guild to run
`train.py` --- a sample training script. In this section, you run the
same script with different hyperparameter values to find lower values
of `loss`. This process is known as [hyperparameter optimization
->](term:https://en.wikipedia.org/wiki/Hyperparameter_optimization),
or hyperparameter *tuning*.

Guild supports hyperparameter optimization using different search
methods:

- Manual search
- [Grid Search ->](term:grid-search)
- [Random search ->](term:random-search)
- [Bayesian optimization ->](term:bayesian-optimization)

For review, here's the loss function used in `train.py`:

``` python
loss = (np.sin(5 * x) * (1 - np.tanh(x ** 2)) + np.random.randn() * noise)
```

The relationship between `x` and `loss` is plotted below. Optimal
values for `x` are around -0.3.

![](/assets/img/bayesian-optimization.png)

^ Plot of `x` on the horizontal axis to `loss` on the vertical axis
  [^hparam-plot]

[^hparam-plot]: Image credit: [*Bayesian optimization with skopt*
    ->](https://scikit-optimize.github.io/auto_examples/bayesian-optimization.html)

In a real scenario, we don't know the optimal hyperparameter values
--- we need to search for them.

## Manual Search

When experimenting with hyperparameters, it's often useful to start
with values based on prior experience or intuition.

Guild lets you run multiple trials in a batch by specifying them as a
list in the form ``[VAL1,VAL2,...VALN]``.

Run three trials of `train.py` using different values for `x`:

``` command
guild run train.py x=[-1,0,1]
```

``` output
You are about to run train.py as a batch (3 trials)
  noise: 0.1
  x: [-1, 0, 1]
Continue? (Y/n)
```

Press `Enter` to start the batch.

Guild runs `train.py` three times, once for each specified value of
`x`.

Show the runs:

``` command
guild runs
```

``` output
[1:1933bdcb]  train.py   2020-01-14 09:38:15  completed  noise=0.1 x=1
[2:83dc048d]  train.py   2020-01-14 09:38:14  completed  noise=0.1 x=0
[3:468bb240]  train.py   2020-01-14 09:38:14  completed  noise=0.1 x=-1
[4:bfcff413]  train.py+  2020-01-14 09:38:13  completed
[5:68f4da74]  train.py   2020-01-14 08:42:54  completed  noise=0.1 x=0.1
```

Runs 1 through 3 are the *trial* runs generated by the command. Run 4
is a [batch run](term:batch). Batch runs are responsible for
generating trials and are denoted using ``+`` in their name.

### Compare Runs

To compare `loss` across runs, use the [compare](cmd:compare) command:

``` command
guild compare --min loss
```

Guild starts an interactive application that lets you browse
experiment results. Runs with lower `loss` appear at the top. Use your
arrow keys to navigate. Press `1` to sort by the current column in
ascending order or `2` to sort in descending order. Press `?` for a
list of supported commands.

![](/assets/img/compare-start.png)

^ Compare experiment results --- press `?` for a list of commands, `q`
  to exit

Exit *Guild Compare* by pressing `q`.

When you specify lists for more than one flag, Guild runs trials for
each flag value combination (the cartesian product of flag values).

The following command generates *four* runs --- one for for each
unique combination of flag values:

``` command
guild run train.py x=[-0.5,0.5] noise=[0.1,0.2]
```

``` output
You are about to run train.py as a batch (max 20 trials, minimize loss)
  noise: [0.1, 0.2]
  x: [-0.5, 0.5]
Continue? (Y/n)
```

Press `Enter` to start the batch.

Show the top-3 best runs:

``` command
guild compare --table --min loss --top 3
```

The `--table` option tells Guild to show results without running in
interactive mode. The `--top` option tells Guild to show only the
top-N runs based on the sort order. In this case `--min loss` sorts by
loss in ascending order.

Note that runs where `x` is `-0.5` have the lowest `loss`. This is
consistent with our expectation from the plot above.

### View Runs in TensorBoard

View runs in [TensorBoard](ref:tensorboard) with the
[tensorboard](cmd:tensorboard) command:

``` command
guild tensorboard
```

Guild starts TensorBoard and opens a new tab in your
browser. TensorBoard board is used to view run results, including
scalars, images, embeddings, and hyperparameters.

Select the **HPARAMS** tab and then select the **PARALLEL COORDINATES
VIEW** subtab.

![](/assets/img/tb-hparams.png)

^ Compare runs using **Parallel Coordinates View**

The *Parallel Coordinates View* highlights runs that perform better
along various axes. Click-and-drag along the `loss` axis to highlight
runs with the lowest values.

!!! highlight
    Guild automatically generates *HParam* summaries from
    Guild runs to simply the process of comparing runs in TensorBoard.

To continue with this guide using the same terminal, stop TensorBoard
by typing `Ctrl-C` in the terminal. Alternatively, open a new command
terminal, leaving TensorBoard running in the background and continue
the steps below.

## Grid Search

[Grid search](term:grid-search) is a systematic search across a subset
of hyperparameter space. Guild supports a special [flag sequence
syntax](term:flag-sequence-function) for specifying value
ranges. Ranges are specified using the format
``FUNCTION[START:END:STEP_OR_COUNT]`` where `FUNCTION` is the type of
sequence and `START` and `END` mark the start and end of the sequence
respectively. `STEP_OR_COUNT` is the range step or value count,
depending on the function used.

Use the [`linspace`](/flags.md#linspace) function to run four trials
where `x` is evenly spaced between `-2.0` and `2.0`:

``` command
guild run train.py x=linspace[-0.6:0.6:4]
```

``` output
You are about to run train.py as a batch (max 20 trials, minimize loss)
  noise: 0.1
  x: [-0.6, -0.2, 0.2, 0.6]
Continue? (Y/n)
```

Guild expands the function `linspace[-0.6:0.6:4]` to the sequence of
values `[-0.6, -0.2, 0.2, 0.6]`. The value `4` indicates how many
values appear in the list.

Press `Enter` to start the batch.

Guild runs trials for each value of `x` generated by the `linspace`
function.

!!! tip
    Use sequence functions for multiple flags to expand a grid
    search to the catesian product of each set of values.

## Random Search

Guild supports [random search](term:random-search) over a both uniform
and log-uniform distributions.

Search space is specified by [special flag
functions](/flags.md#search-space-functions), which include
[`uniform`](/flags.md#uniform) and
[`loguniform`](/flags.md#loguniform). The `uniform` function name may
be omitted.

To search over a uniformly distributed range of values, specify a flag
value in the format `[MIN:MAX]`. By default, Guild runs 20 trials
using randomly chosen values within the specified range. Use
`--max-trials` to specify the number of trials to run.

Start a random search over `x` with 5 trials:

``` command
guild run train.py x=[-2.0:2.0] --max-trials 5
```

``` output
You are about to run train.py with random search (max 5 trials, minimize loss)
  noise: 0.1
  x: [-2.0:2.0]
Continue? (Y/n)
```

Press `Enter` to start the batch.

Guild runs `train.py` five times using randomly sampled values for `x`
from `-2.0` to `2.0`.

## Bayesian Optimization

*Bayesian optimization* methods use light-weight probabilistic models
to suggest hyperparameter values believed to optimize an *objective*
based on previous results.

By default, Guild attempts to minimize `loss`, which is logged by our
sample `train.py` script. If the script used a different objective,
you would specify it using the `--mininize` or `--maximize` options
for [run](cmd:run).

Run ten trials using the [`gp`](/reference/optimizers.md#gp)
optimizer, which uses Bayesian optimization with *gaussian processes*:

``` command
guild run train.py x=[-2.0:2.0] --optimizer gp --max-trials 10
```

``` output
You are about to run train.py with 'skopt:gp' optimizer (max 10 trials, minimize loss)
  noise: 0.1
  x: [-2.0:2.0]
Continue? (Y/n)
```

Press `Enter` to start the batch.

The `gp` optimizer seeds the batch with three random starts. After the
third random start, the optimizer uses previous results to suggest
candidates for `x` to minimize `loss`.

### Restart Batch to Continue Optimization

Guild optimizers use previous results *from trials of the same
batch*. If you want to continue a search for better hyperparameters
using Bayesian optimization, you must restart the batch run.

To restart a run, you need the target run ID.

To get the run ID of the `gp` batch, run:

``` command
guild runs info --operation train.py+gp
```

``` output
id: <run ID used for restart below>
operation: train.py+gp
from: guildai
status: completed
...
```

Note the ID for the batch run, which is shown on the first line.

Restart the batch to generate another ten trials, replacing ``<batch
run ID>`` below with the `train.py+gp` run ID from above:

``` command
guild run -Fo random-starts=0 --restart <batch run ID>
```

``` output
You are about to start ... (train.py) with 'skopt:gp' optimizer (max 10 trials)
  noise: 0.1
  x: [-2.0:2.0]
Continue? (Y/n)
```

Press `Enter` to restart the batch.

Guild generates another ten trials for the batch. Guild uses the
previous trials generated earlier as inputs to the optimization.

The option ``-Fo random-starts=0`` prevents the optimizer from using
additional random starts.

### Evaluate Bayesian Optimization Results

Run [Guild Compare](ref:guild-compare) on the last 20 runs to evaluate
the results of the Bayesian optimization:

``` command
guild compare 1:20
```

By default, Guild shows runs ordered by start time starting with the
latest run first. Note the values for `x` that the `gp` optimizer
suggested. If the optimizer is effective in finding values for `x`
that minimize `loss`, you should see more values around `-0.3` as the
runs progress.

Press `q` to exit Guild Compare.

You can also use TensorBoard to evaluate optimization results.

Start TensorBoard to view the last 20 runs:

``` command
guild tensorboard 1:20
```

Guild starts TensorBoard and opens a new browser tab.

Click **HPARAMS** and then click **SCATTER PLOT MATRIX VIEW**.

TensorBoard displays scatter plots of hypermaraters and metrics.

In the left side panel, *deselect* the following hyperparameters:

- `noise`
- `sourcecode`

In the same panel, *deselect* the following metrics:

- `time`
- Xxx Yss

TensorBoard displays a plot of `loss` against `x`. Each point on the
plot represents a trial. Over 20 trials, the Bayesian optimizer should
spend more time exporing values for `x` around `-0.3`. This will
appear as a cluster of trials along the bottom of the plot.

![](/assets/img/tb-hparams-scatter.png)

^ Plot `loss` against `x` to evaluate the Bayesian optimization
  results

Return to the command terminal and press `Ctrl-C` to stop TensorBoard.

## Save Run Comparison to CSV

Use Guild to save run results to a CSV:

``` command
guild compare --csv results.csv
```

Use

!!! highlight
    Use Guild to replace manual experiment tracking and
    spreadsheets.

## Summary

In this section, you use various techniques to run `train.py`.

!!! highlights
    - Run experiments with manually assigned hyperparameters.
    - Automate larger runs using *grid* and *random* search.
    - Optimize model performance using Bayesian methods.

In this section, you run a grid search and performed Bayesian
optimization to find optimal values of `x`.

In the next section, you learn about more about runs.
